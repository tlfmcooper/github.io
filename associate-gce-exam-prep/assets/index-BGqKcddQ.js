(function(){const Z=document.createElement("link").relList;if(Z&&Z.supports&&Z.supports("modulepreload"))return;for(const K of document.querySelectorAll('link[rel="modulepreload"]'))m(K);new MutationObserver(K=>{for(const L of K)if(L.type==="childList")for(const D of L.addedNodes)D.tagName==="LINK"&&D.rel==="modulepreload"&&m(D)}).observe(document,{childList:!0,subtree:!0});function te(K){const L={};return K.integrity&&(L.integrity=K.integrity),K.referrerPolicy&&(L.referrerPolicy=K.referrerPolicy),K.crossOrigin==="use-credentials"?L.credentials="include":K.crossOrigin==="anonymous"?L.credentials="omit":L.credentials="same-origin",L}function m(K){if(K.ep)return;K.ep=!0;const L=te(K);fetch(K.href,L)}})();var xl={exports:{}},Hn={};/**
 * @license React
 * react-jsx-runtime.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Tp;function $h(){if(Tp)return Hn;Tp=1;var M=Symbol.for("react.transitional.element"),Z=Symbol.for("react.fragment");function te(m,K,L){var D=null;if(L!==void 0&&(D=""+L),K.key!==void 0&&(D=""+K.key),"key"in K){L={};for(var R in K)R!=="key"&&(L[R]=K[R])}else L=K;return K=L.ref,{$$typeof:M,type:m,key:D,ref:K!==void 0?K:null,props:L}}return Hn.Fragment=Z,Hn.jsx=te,Hn.jsxs=te,Hn}var Mp;function em(){return Mp||(Mp=1,xl.exports=$h()),xl.exports}var c=em(),kl={exports:{}},X={};/**
 * @license React
 * react.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Pp;function tm(){if(Pp)return X;Pp=1;var M=Symbol.for("react.transitional.element"),Z=Symbol.for("react.portal"),te=Symbol.for("react.fragment"),m=Symbol.for("react.strict_mode"),K=Symbol.for("react.profiler"),L=Symbol.for("react.consumer"),D=Symbol.for("react.context"),R=Symbol.for("react.forward_ref"),I=Symbol.for("react.suspense"),k=Symbol.for("react.memo"),re=Symbol.for("react.lazy"),N=Symbol.for("react.activity"),ke=Symbol.iterator;function st(d){return d===null||typeof d!="object"?null:(d=ke&&d[ke]||d["@@iterator"],typeof d=="function"?d:null)}var Ze={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},ze=Object.assign,Se={};function De(d,S,T){this.props=d,this.context=S,this.refs=Se,this.updater=T||Ze}De.prototype.isReactComponent={},De.prototype.setState=function(d,S){if(typeof d!="object"&&typeof d!="function"&&d!=null)throw Error("takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,d,S,"setState")},De.prototype.forceUpdate=function(d){this.updater.enqueueForceUpdate(this,d,"forceUpdate")};function Fe(){}Fe.prototype=De.prototype;function Ie(d,S,T){this.props=d,this.context=S,this.refs=Se,this.updater=T||Ze}var Qe=Ie.prototype=new Fe;Qe.constructor=Ie,ze(Qe,De.prototype),Qe.isPureReactComponent=!0;var ct=Array.isArray;function nt(){}var J={H:null,A:null,T:null,S:null},Pe=Object.prototype.hasOwnProperty;function Je(d,S,T){var P=T.ref;return{$$typeof:M,type:d,key:S,ref:P!==void 0?P:null,props:T}}function Ne(d,S){return Je(d.type,S,d.props)}function Ve(d){return typeof d=="object"&&d!==null&&d.$$typeof===M}function be(d){var S={"=":"=0",":":"=2"};return"$"+d.replace(/[=:]/g,function(T){return S[T]})}var ut=/\/+/g;function Ke(d,S){return typeof d=="object"&&d!==null&&d.key!=null?be(""+d.key):S.toString(36)}function Ae(d){switch(d.status){case"fulfilled":return d.value;case"rejected":throw d.reason;default:switch(typeof d.status=="string"?d.then(nt,nt):(d.status="pending",d.then(function(S){d.status==="pending"&&(d.status="fulfilled",d.value=S)},function(S){d.status==="pending"&&(d.status="rejected",d.reason=S)})),d.status){case"fulfilled":return d.value;case"rejected":throw d.reason}}throw d}function b(d,S,T,P,H){var F=typeof d;(F==="undefined"||F==="boolean")&&(d=null);var le=!1;if(d===null)le=!0;else switch(F){case"bigint":case"string":case"number":le=!0;break;case"object":switch(d.$$typeof){case M:case Z:le=!0;break;case re:return le=d._init,b(le(d._payload),S,T,P,H)}}if(le)return H=H(d),le=P===""?"."+Ke(d,0):P,ct(H)?(T="",le!=null&&(T=le.replace(ut,"$&/")+"/"),b(H,S,T,"",function(_){return _})):H!=null&&(Ve(H)&&(H=Ne(H,T+(H.key==null||d&&d.key===H.key?"":(""+H.key).replace(ut,"$&/")+"/")+le)),S.push(H)),1;le=0;var W=P===""?".":P+":";if(ct(d))for(var Ee=0;Ee<d.length;Ee++)P=d[Ee],F=W+Ke(P,Ee),le+=b(P,S,T,F,H);else if(Ee=st(d),typeof Ee=="function")for(d=Ee.call(d),Ee=0;!(P=d.next()).done;)P=P.value,F=W+Ke(P,Ee++),le+=b(P,S,T,F,H);else if(F==="object"){if(typeof d.then=="function")return b(Ae(d),S,T,P,H);throw S=String(d),Error("Objects are not valid as a React child (found: "+(S==="[object Object]"?"object with keys {"+Object.keys(d).join(", ")+"}":S)+"). If you meant to render a collection of children, use an array instead.")}return le}function E(d,S,T){if(d==null)return d;var P=[],H=0;return b(d,P,"","",function(F){return S.call(T,F,H++)}),P}function V(d){if(d._status===-1){var S=d._result;S=S(),S.then(function(T){(d._status===0||d._status===-1)&&(d._status=1,d._result=T)},function(T){(d._status===0||d._status===-1)&&(d._status=2,d._result=T)}),d._status===-1&&(d._status=0,d._result=S)}if(d._status===1)return d._result.default;throw d._result}var ue=typeof reportError=="function"?reportError:function(d){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var S=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof d=="object"&&d!==null&&typeof d.message=="string"?String(d.message):String(d),error:d});if(!window.dispatchEvent(S))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",d);return}console.error(d)},me={map:E,forEach:function(d,S,T){E(d,function(){S.apply(this,arguments)},T)},count:function(d){var S=0;return E(d,function(){S++}),S},toArray:function(d){return E(d,function(S){return S})||[]},only:function(d){if(!Ve(d))throw Error("React.Children.only expected to receive a single React element child.");return d}};return X.Activity=N,X.Children=me,X.Component=De,X.Fragment=te,X.Profiler=K,X.PureComponent=Ie,X.StrictMode=m,X.Suspense=I,X.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=J,X.__COMPILER_RUNTIME={__proto__:null,c:function(d){return J.H.useMemoCache(d)}},X.cache=function(d){return function(){return d.apply(null,arguments)}},X.cacheSignal=function(){return null},X.cloneElement=function(d,S,T){if(d==null)throw Error("The argument must be a React element, but you passed "+d+".");var P=ze({},d.props),H=d.key;if(S!=null)for(F in S.key!==void 0&&(H=""+S.key),S)!Pe.call(S,F)||F==="key"||F==="__self"||F==="__source"||F==="ref"&&S.ref===void 0||(P[F]=S[F]);var F=arguments.length-2;if(F===1)P.children=T;else if(1<F){for(var le=Array(F),W=0;W<F;W++)le[W]=arguments[W+2];P.children=le}return Je(d.type,H,P)},X.createContext=function(d){return d={$$typeof:D,_currentValue:d,_currentValue2:d,_threadCount:0,Provider:null,Consumer:null},d.Provider=d,d.Consumer={$$typeof:L,_context:d},d},X.createElement=function(d,S,T){var P,H={},F=null;if(S!=null)for(P in S.key!==void 0&&(F=""+S.key),S)Pe.call(S,P)&&P!=="key"&&P!=="__self"&&P!=="__source"&&(H[P]=S[P]);var le=arguments.length-2;if(le===1)H.children=T;else if(1<le){for(var W=Array(le),Ee=0;Ee<le;Ee++)W[Ee]=arguments[Ee+2];H.children=W}if(d&&d.defaultProps)for(P in le=d.defaultProps,le)H[P]===void 0&&(H[P]=le[P]);return Je(d,F,H)},X.createRef=function(){return{current:null}},X.forwardRef=function(d){return{$$typeof:R,render:d}},X.isValidElement=Ve,X.lazy=function(d){return{$$typeof:re,_payload:{_status:-1,_result:d},_init:V}},X.memo=function(d,S){return{$$typeof:k,type:d,compare:S===void 0?null:S}},X.startTransition=function(d){var S=J.T,T={};J.T=T;try{var P=d(),H=J.S;H!==null&&H(T,P),typeof P=="object"&&P!==null&&typeof P.then=="function"&&P.then(nt,ue)}catch(F){ue(F)}finally{S!==null&&T.types!==null&&(S.types=T.types),J.T=S}},X.unstable_useCacheRefresh=function(){return J.H.useCacheRefresh()},X.use=function(d){return J.H.use(d)},X.useActionState=function(d,S,T){return J.H.useActionState(d,S,T)},X.useCallback=function(d,S){return J.H.useCallback(d,S)},X.useContext=function(d){return J.H.useContext(d)},X.useDebugValue=function(){},X.useDeferredValue=function(d,S){return J.H.useDeferredValue(d,S)},X.useEffect=function(d,S){return J.H.useEffect(d,S)},X.useEffectEvent=function(d){return J.H.useEffectEvent(d)},X.useId=function(){return J.H.useId()},X.useImperativeHandle=function(d,S,T){return J.H.useImperativeHandle(d,S,T)},X.useInsertionEffect=function(d,S){return J.H.useInsertionEffect(d,S)},X.useLayoutEffect=function(d,S){return J.H.useLayoutEffect(d,S)},X.useMemo=function(d,S){return J.H.useMemo(d,S)},X.useOptimistic=function(d,S){return J.H.useOptimistic(d,S)},X.useReducer=function(d,S,T){return J.H.useReducer(d,S,T)},X.useRef=function(d){return J.H.useRef(d)},X.useState=function(d){return J.H.useState(d)},X.useSyncExternalStore=function(d,S,T){return J.H.useSyncExternalStore(d,S,T)},X.useTransition=function(){return J.H.useTransition()},X.version="19.2.0",X}var qp;function ql(){return qp||(qp=1,kl.exports=tm()),kl.exports}var he=ql(),Al={exports:{}},Fn={},El={exports:{}},Tl={};/**
 * @license React
 * scheduler.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Ip;function om(){return Ip||(Ip=1,(function(M){function Z(b,E){var V=b.length;b.push(E);e:for(;0<V;){var ue=V-1>>>1,me=b[ue];if(0<K(me,E))b[ue]=E,b[V]=me,V=ue;else break e}}function te(b){return b.length===0?null:b[0]}function m(b){if(b.length===0)return null;var E=b[0],V=b.pop();if(V!==E){b[0]=V;e:for(var ue=0,me=b.length,d=me>>>1;ue<d;){var S=2*(ue+1)-1,T=b[S],P=S+1,H=b[P];if(0>K(T,V))P<me&&0>K(H,T)?(b[ue]=H,b[P]=V,ue=P):(b[ue]=T,b[S]=V,ue=S);else if(P<me&&0>K(H,V))b[ue]=H,b[P]=V,ue=P;else break e}}return E}function K(b,E){var V=b.sortIndex-E.sortIndex;return V!==0?V:b.id-E.id}if(M.unstable_now=void 0,typeof performance=="object"&&typeof performance.now=="function"){var L=performance;M.unstable_now=function(){return L.now()}}else{var D=Date,R=D.now();M.unstable_now=function(){return D.now()-R}}var I=[],k=[],re=1,N=null,ke=3,st=!1,Ze=!1,ze=!1,Se=!1,De=typeof setTimeout=="function"?setTimeout:null,Fe=typeof clearTimeout=="function"?clearTimeout:null,Ie=typeof setImmediate<"u"?setImmediate:null;function Qe(b){for(var E=te(k);E!==null;){if(E.callback===null)m(k);else if(E.startTime<=b)m(k),E.sortIndex=E.expirationTime,Z(I,E);else break;E=te(k)}}function ct(b){if(ze=!1,Qe(b),!Ze)if(te(I)!==null)Ze=!0,nt||(nt=!0,be());else{var E=te(k);E!==null&&Ae(ct,E.startTime-b)}}var nt=!1,J=-1,Pe=5,Je=-1;function Ne(){return Se?!0:!(M.unstable_now()-Je<Pe)}function Ve(){if(Se=!1,nt){var b=M.unstable_now();Je=b;var E=!0;try{e:{Ze=!1,ze&&(ze=!1,Fe(J),J=-1),st=!0;var V=ke;try{t:{for(Qe(b),N=te(I);N!==null&&!(N.expirationTime>b&&Ne());){var ue=N.callback;if(typeof ue=="function"){N.callback=null,ke=N.priorityLevel;var me=ue(N.expirationTime<=b);if(b=M.unstable_now(),typeof me=="function"){N.callback=me,Qe(b),E=!0;break t}N===te(I)&&m(I),Qe(b)}else m(I);N=te(I)}if(N!==null)E=!0;else{var d=te(k);d!==null&&Ae(ct,d.startTime-b),E=!1}}break e}finally{N=null,ke=V,st=!1}E=void 0}}finally{E?be():nt=!1}}}var be;if(typeof Ie=="function")be=function(){Ie(Ve)};else if(typeof MessageChannel<"u"){var ut=new MessageChannel,Ke=ut.port2;ut.port1.onmessage=Ve,be=function(){Ke.postMessage(null)}}else be=function(){De(Ve,0)};function Ae(b,E){J=De(function(){b(M.unstable_now())},E)}M.unstable_IdlePriority=5,M.unstable_ImmediatePriority=1,M.unstable_LowPriority=4,M.unstable_NormalPriority=3,M.unstable_Profiling=null,M.unstable_UserBlockingPriority=2,M.unstable_cancelCallback=function(b){b.callback=null},M.unstable_forceFrameRate=function(b){0>b||125<b?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):Pe=0<b?Math.floor(1e3/b):5},M.unstable_getCurrentPriorityLevel=function(){return ke},M.unstable_next=function(b){switch(ke){case 1:case 2:case 3:var E=3;break;default:E=ke}var V=ke;ke=E;try{return b()}finally{ke=V}},M.unstable_requestPaint=function(){Se=!0},M.unstable_runWithPriority=function(b,E){switch(b){case 1:case 2:case 3:case 4:case 5:break;default:b=3}var V=ke;ke=b;try{return E()}finally{ke=V}},M.unstable_scheduleCallback=function(b,E,V){var ue=M.unstable_now();switch(typeof V=="object"&&V!==null?(V=V.delay,V=typeof V=="number"&&0<V?ue+V:ue):V=ue,b){case 1:var me=-1;break;case 2:me=250;break;case 5:me=1073741823;break;case 4:me=1e4;break;default:me=5e3}return me=V+me,b={id:re++,callback:E,priorityLevel:b,startTime:V,expirationTime:me,sortIndex:-1},V>ue?(b.sortIndex=V,Z(k,b),te(I)===null&&b===te(k)&&(ze?(Fe(J),J=-1):ze=!0,Ae(ct,V-ue))):(b.sortIndex=me,Z(I,b),Ze||st||(Ze=!0,nt||(nt=!0,be()))),b},M.unstable_shouldYield=Ne,M.unstable_wrapCallback=function(b){var E=ke;return function(){var V=ke;ke=E;try{return b.apply(this,arguments)}finally{ke=V}}}})(Tl)),Tl}var zp;function im(){return zp||(zp=1,El.exports=om()),El.exports}var Ml={exports:{}},at={};/**
 * @license React
 * react-dom.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Dp;function nm(){if(Dp)return at;Dp=1;var M=ql();function Z(I){var k="https://react.dev/errors/"+I;if(1<arguments.length){k+="?args[]="+encodeURIComponent(arguments[1]);for(var re=2;re<arguments.length;re++)k+="&args[]="+encodeURIComponent(arguments[re])}return"Minified React error #"+I+"; visit "+k+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function te(){}var m={d:{f:te,r:function(){throw Error(Z(522))},D:te,C:te,L:te,m:te,X:te,S:te,M:te},p:0,findDOMNode:null},K=Symbol.for("react.portal");function L(I,k,re){var N=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:K,key:N==null?null:""+N,children:I,containerInfo:k,implementation:re}}var D=M.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE;function R(I,k){if(I==="font")return"";if(typeof k=="string")return k==="use-credentials"?k:""}return at.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=m,at.createPortal=function(I,k){var re=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!k||k.nodeType!==1&&k.nodeType!==9&&k.nodeType!==11)throw Error(Z(299));return L(I,k,null,re)},at.flushSync=function(I){var k=D.T,re=m.p;try{if(D.T=null,m.p=2,I)return I()}finally{D.T=k,m.p=re,m.d.f()}},at.preconnect=function(I,k){typeof I=="string"&&(k?(k=k.crossOrigin,k=typeof k=="string"?k==="use-credentials"?k:"":void 0):k=null,m.d.C(I,k))},at.prefetchDNS=function(I){typeof I=="string"&&m.d.D(I)},at.preinit=function(I,k){if(typeof I=="string"&&k&&typeof k.as=="string"){var re=k.as,N=R(re,k.crossOrigin),ke=typeof k.integrity=="string"?k.integrity:void 0,st=typeof k.fetchPriority=="string"?k.fetchPriority:void 0;re==="style"?m.d.S(I,typeof k.precedence=="string"?k.precedence:void 0,{crossOrigin:N,integrity:ke,fetchPriority:st}):re==="script"&&m.d.X(I,{crossOrigin:N,integrity:ke,fetchPriority:st,nonce:typeof k.nonce=="string"?k.nonce:void 0})}},at.preinitModule=function(I,k){if(typeof I=="string")if(typeof k=="object"&&k!==null){if(k.as==null||k.as==="script"){var re=R(k.as,k.crossOrigin);m.d.M(I,{crossOrigin:re,integrity:typeof k.integrity=="string"?k.integrity:void 0,nonce:typeof k.nonce=="string"?k.nonce:void 0})}}else k==null&&m.d.M(I)},at.preload=function(I,k){if(typeof I=="string"&&typeof k=="object"&&k!==null&&typeof k.as=="string"){var re=k.as,N=R(re,k.crossOrigin);m.d.L(I,re,{crossOrigin:N,integrity:typeof k.integrity=="string"?k.integrity:void 0,nonce:typeof k.nonce=="string"?k.nonce:void 0,type:typeof k.type=="string"?k.type:void 0,fetchPriority:typeof k.fetchPriority=="string"?k.fetchPriority:void 0,referrerPolicy:typeof k.referrerPolicy=="string"?k.referrerPolicy:void 0,imageSrcSet:typeof k.imageSrcSet=="string"?k.imageSrcSet:void 0,imageSizes:typeof k.imageSizes=="string"?k.imageSizes:void 0,media:typeof k.media=="string"?k.media:void 0})}},at.preloadModule=function(I,k){if(typeof I=="string")if(k){var re=R(k.as,k.crossOrigin);m.d.m(I,{as:typeof k.as=="string"&&k.as!=="script"?k.as:void 0,crossOrigin:re,integrity:typeof k.integrity=="string"?k.integrity:void 0})}else m.d.m(I)},at.requestFormReset=function(I){m.d.r(I)},at.unstable_batchedUpdates=function(I,k){return I(k)},at.useFormState=function(I,k,re){return D.H.useFormState(I,k,re)},at.useFormStatus=function(){return D.H.useHostTransitionStatus()},at.version="19.2.0",at}var Gp;function am(){if(Gp)return Ml.exports;Gp=1;function M(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(M)}catch(Z){console.error(Z)}}return M(),Ml.exports=nm(),Ml.exports}/**
 * @license React
 * react-dom-client.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var jp;function sm(){if(jp)return Fn;jp=1;var M=im(),Z=ql(),te=am();function m(e){var t="https://react.dev/errors/"+e;if(1<arguments.length){t+="?args[]="+encodeURIComponent(arguments[1]);for(var o=2;o<arguments.length;o++)t+="&args[]="+encodeURIComponent(arguments[o])}return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function K(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function L(e){var t=e,o=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,(t.flags&4098)!==0&&(o=t.return),e=t.return;while(e)}return t.tag===3?o:null}function D(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function R(e){if(e.tag===31){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function I(e){if(L(e)!==e)throw Error(m(188))}function k(e){var t=e.alternate;if(!t){if(t=L(e),t===null)throw Error(m(188));return t!==e?null:e}for(var o=e,i=t;;){var n=o.return;if(n===null)break;var a=n.alternate;if(a===null){if(i=n.return,i!==null){o=i;continue}break}if(n.child===a.child){for(a=n.child;a;){if(a===o)return I(n),e;if(a===i)return I(n),t;a=a.sibling}throw Error(m(188))}if(o.return!==i.return)o=n,i=a;else{for(var s=!1,r=n.child;r;){if(r===o){s=!0,o=n,i=a;break}if(r===i){s=!0,i=n,o=a;break}r=r.sibling}if(!s){for(r=a.child;r;){if(r===o){s=!0,o=a,i=n;break}if(r===i){s=!0,i=a,o=n;break}r=r.sibling}if(!s)throw Error(m(189))}}if(o.alternate!==i)throw Error(m(190))}if(o.tag!==3)throw Error(m(188));return o.stateNode.current===o?e:t}function re(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e;for(e=e.child;e!==null;){if(t=re(e),t!==null)return t;e=e.sibling}return null}var N=Object.assign,ke=Symbol.for("react.element"),st=Symbol.for("react.transitional.element"),Ze=Symbol.for("react.portal"),ze=Symbol.for("react.fragment"),Se=Symbol.for("react.strict_mode"),De=Symbol.for("react.profiler"),Fe=Symbol.for("react.consumer"),Ie=Symbol.for("react.context"),Qe=Symbol.for("react.forward_ref"),ct=Symbol.for("react.suspense"),nt=Symbol.for("react.suspense_list"),J=Symbol.for("react.memo"),Pe=Symbol.for("react.lazy"),Je=Symbol.for("react.activity"),Ne=Symbol.for("react.memo_cache_sentinel"),Ve=Symbol.iterator;function be(e){return e===null||typeof e!="object"?null:(e=Ve&&e[Ve]||e["@@iterator"],typeof e=="function"?e:null)}var ut=Symbol.for("react.client.reference");function Ke(e){if(e==null)return null;if(typeof e=="function")return e.$$typeof===ut?null:e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case ze:return"Fragment";case De:return"Profiler";case Se:return"StrictMode";case ct:return"Suspense";case nt:return"SuspenseList";case Je:return"Activity"}if(typeof e=="object")switch(e.$$typeof){case Ze:return"Portal";case Ie:return e.displayName||"Context";case Fe:return(e._context.displayName||"Context")+".Consumer";case Qe:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case J:return t=e.displayName||null,t!==null?t:Ke(e.type)||"Memo";case Pe:t=e._payload,e=e._init;try{return Ke(e(t))}catch{}}return null}var Ae=Array.isArray,b=Z.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,E=te.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,V={pending:!1,data:null,method:null,action:null},ue=[],me=-1;function d(e){return{current:e}}function S(e){0>me||(e.current=ue[me],ue[me]=null,me--)}function T(e,t){me++,ue[me]=e.current,e.current=t}var P=d(null),H=d(null),F=d(null),le=d(null);function W(e,t){switch(T(F,t),T(H,e),T(P,null),t.nodeType){case 9:case 11:e=(e=t.documentElement)&&(e=e.namespaceURI)?Xd(e):0;break;default:if(e=t.tagName,t=t.namespaceURI)t=Xd(t),e=Zd(t,e);else switch(e){case"svg":e=1;break;case"math":e=2;break;default:e=0}}S(P),T(P,e)}function Ee(){S(P),S(H),S(F)}function _(e){e.memoizedState!==null&&T(le,e);var t=P.current,o=Zd(t,e.type);t!==o&&(T(H,e),T(P,o))}function Bt(e){H.current===e&&(S(P),S(H)),le.current===e&&(S(le),Rn._currentValue=V)}var oi,Fi;function bt(e){if(oi===void 0)try{throw Error()}catch(o){var t=o.stack.trim().match(/\n( *(at )?)/);oi=t&&t[1]||"",Fi=-1<o.stack.indexOf(`
    at`)?" (<anonymous>)":-1<o.stack.indexOf("@")?"@unknown:0:0":""}return`
`+oi+e+Fi}var Ki=!1;function ii(e,t){if(!e||Ki)return"";Ki=!0;var o=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{var i={DetermineComponentFrameRoot:function(){try{if(t){var C=function(){throw Error()};if(Object.defineProperty(C.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(C,[])}catch(y){var f=y}Reflect.construct(e,[],C)}else{try{C.call()}catch(y){f=y}e.call(C.prototype)}}else{try{throw Error()}catch(y){f=y}(C=e())&&typeof C.catch=="function"&&C.catch(function(){})}}catch(y){if(y&&f&&typeof y.stack=="string")return[y.stack,f.stack]}return[null,null]}};i.DetermineComponentFrameRoot.displayName="DetermineComponentFrameRoot";var n=Object.getOwnPropertyDescriptor(i.DetermineComponentFrameRoot,"name");n&&n.configurable&&Object.defineProperty(i.DetermineComponentFrameRoot,"name",{value:"DetermineComponentFrameRoot"});var a=i.DetermineComponentFrameRoot(),s=a[0],r=a[1];if(s&&r){var l=s.split(`
`),h=r.split(`
`);for(n=i=0;i<l.length&&!l[i].includes("DetermineComponentFrameRoot");)i++;for(;n<h.length&&!h[n].includes("DetermineComponentFrameRoot");)n++;if(i===l.length||n===h.length)for(i=l.length-1,n=h.length-1;1<=i&&0<=n&&l[i]!==h[n];)n--;for(;1<=i&&0<=n;i--,n--)if(l[i]!==h[n]){if(i!==1||n!==1)do if(i--,n--,0>n||l[i]!==h[n]){var v=`
`+l[i].replace(" at new "," at ");return e.displayName&&v.includes("<anonymous>")&&(v=v.replace("<anonymous>",e.displayName)),v}while(1<=i&&0<=n);break}}}finally{Ki=!1,Error.prepareStackTrace=o}return(o=e?e.displayName||e.name:"")?bt(o):""}function _i(e,t){switch(e.tag){case 26:case 27:case 5:return bt(e.type);case 16:return bt("Lazy");case 13:return e.child!==t&&t!==null?bt("Suspense Fallback"):bt("Suspense");case 19:return bt("SuspenseList");case 0:case 15:return ii(e.type,!1);case 11:return ii(e.type.render,!1);case 1:return ii(e.type,!0);case 31:return bt("Activity");default:return""}}function Xi(e){try{var t="",o=null;do t+=_i(e,o),o=e,e=e.return;while(e);return t}catch(i){return`
Error generating stack: `+i.message+`
`+i.stack}}var ni=Object.prototype.hasOwnProperty,ai=M.unstable_scheduleCallback,Zi=M.unstable_cancelCallback,Ji=M.unstable_shouldYield,cs=M.unstable_requestPaint,rt=M.unstable_now,Kn=M.unstable_getCurrentPriorityLevel,_n=M.unstable_ImmediatePriority,$i=M.unstable_UserBlockingPriority,x=M.unstable_NormalPriority,A=M.unstable_LowPriority,j=M.unstable_IdlePriority,U=M.log,O=M.unstable_setDisableYieldValue,q=null,Q=null;function oe(e){if(typeof U=="function"&&O(e),Q&&typeof Q.setStrictMode=="function")try{Q.setStrictMode(q,e)}catch{}}var Ye=Math.clz32?Math.clz32:us,Yo=Math.log,si=Math.LN2;function us(e){return e>>>=0,e===0?32:31-(Yo(e)/si|0)|0}var uo=256,po=262144,Lo=4194304;function Tt(e){var t=e&42;if(t!==0)return t;switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:return 64;case 128:return 128;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:return e&261888;case 262144:case 524288:case 1048576:case 2097152:return e&3932160;case 4194304:case 8388608:case 16777216:case 33554432:return e&62914560;case 67108864:return 67108864;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 0;default:return e}}function Xn(e,t,o){var i=e.pendingLanes;if(i===0)return 0;var n=0,a=e.suspendedLanes,s=e.pingedLanes;e=e.warmLanes;var r=i&134217727;return r!==0?(i=r&~a,i!==0?n=Tt(i):(s&=r,s!==0?n=Tt(s):o||(o=r&~e,o!==0&&(n=Tt(o))))):(r=i&~a,r!==0?n=Tt(r):s!==0?n=Tt(s):o||(o=i&~e,o!==0&&(n=Tt(o)))),n===0?0:t!==0&&t!==n&&(t&a)===0&&(a=n&-n,o=t&-t,a>=o||a===32&&(o&4194048)!==0)?t:n}function en(e,t){return(e.pendingLanes&~(e.suspendedLanes&~e.pingedLanes)&t)===0}function Np(e,t){switch(e){case 1:case 2:case 4:case 8:case 64:return t+250;case 16:case 32:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:return-1;case 67108864:case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function Il(){var e=Lo;return Lo<<=1,(Lo&62914560)===0&&(Lo=4194304),e}function ds(e){for(var t=[],o=0;31>o;o++)t.push(e);return t}function tn(e,t){e.pendingLanes|=t,t!==268435456&&(e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0)}function Bp(e,t,o,i,n,a){var s=e.pendingLanes;e.pendingLanes=o,e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0,e.expiredLanes&=o,e.entangledLanes&=o,e.errorRecoveryDisabledLanes&=o,e.shellSuspendCounter=0;var r=e.entanglements,l=e.expirationTimes,h=e.hiddenUpdates;for(o=s&~o;0<o;){var v=31-Ye(o),C=1<<v;r[v]=0,l[v]=-1;var f=h[v];if(f!==null)for(h[v]=null,v=0;v<f.length;v++){var y=f[v];y!==null&&(y.lane&=-536870913)}o&=~C}i!==0&&zl(e,i,0),a!==0&&n===0&&e.tag!==0&&(e.suspendedLanes|=a&~(s&~t))}function zl(e,t,o){e.pendingLanes|=t,e.suspendedLanes&=~t;var i=31-Ye(t);e.entangledLanes|=t,e.entanglements[i]=e.entanglements[i]|1073741824|o&261930}function Dl(e,t){var o=e.entangledLanes|=t;for(e=e.entanglements;o;){var i=31-Ye(o),n=1<<i;n&t|e[i]&t&&(e[i]|=t),o&=~n}}function Gl(e,t){var o=t&-t;return o=(o&42)!==0?1:ps(o),(o&(e.suspendedLanes|t))!==0?0:o}function ps(e){switch(e){case 2:e=1;break;case 8:e=4;break;case 32:e=16;break;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:e=128;break;case 268435456:e=134217728;break;default:e=0}return e}function gs(e){return e&=-e,2<e?8<e?(e&134217727)!==0?32:268435456:8:2}function jl(){var e=E.p;return e!==0?e:(e=window.event,e===void 0?32:wp(e.type))}function Vl(e,t){var o=E.p;try{return E.p=e,t()}finally{E.p=o}}var go=Math.random().toString(36).slice(2),$e="__reactFiber$"+go,dt="__reactProps$"+go,ri="__reactContainer$"+go,hs="__reactEvents$"+go,Rp="__reactListeners$"+go,Op="__reactHandles$"+go,Yl="__reactResources$"+go,on="__reactMarker$"+go;function ms(e){delete e[$e],delete e[dt],delete e[hs],delete e[Rp],delete e[Op]}function li(e){var t=e[$e];if(t)return t;for(var o=e.parentNode;o;){if(t=o[ri]||o[$e]){if(o=t.alternate,t.child!==null||o!==null&&o.child!==null)for(e=np(e);e!==null;){if(o=e[$e])return o;e=np(e)}return t}e=o,o=e.parentNode}return null}function ci(e){if(e=e[$e]||e[ri]){var t=e.tag;if(t===5||t===6||t===13||t===31||t===26||t===27||t===3)return e}return null}function nn(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e.stateNode;throw Error(m(33))}function ui(e){var t=e[Yl];return t||(t=e[Yl]={hoistableStyles:new Map,hoistableScripts:new Map}),t}function _e(e){e[on]=!0}var Ll=new Set,Ul={};function Uo(e,t){di(e,t),di(e+"Capture",t)}function di(e,t){for(Ul[e]=t,e=0;e<t.length;e++)Ll.add(t[e])}var Wp=RegExp("^[:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD][:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\-.0-9\\u00B7\\u0300-\\u036F\\u203F-\\u2040]*$"),Nl={},Bl={};function Qp(e){return ni.call(Bl,e)?!0:ni.call(Nl,e)?!1:Wp.test(e)?Bl[e]=!0:(Nl[e]=!0,!1)}function Zn(e,t,o){if(Qp(t))if(o===null)e.removeAttribute(t);else{switch(typeof o){case"undefined":case"function":case"symbol":e.removeAttribute(t);return;case"boolean":var i=t.toLowerCase().slice(0,5);if(i!=="data-"&&i!=="aria-"){e.removeAttribute(t);return}}e.setAttribute(t,""+o)}}function Jn(e,t,o){if(o===null)e.removeAttribute(t);else{switch(typeof o){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(t);return}e.setAttribute(t,""+o)}}function Ht(e,t,o,i){if(i===null)e.removeAttribute(o);else{switch(typeof i){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(o);return}e.setAttributeNS(t,o,""+i)}}function Mt(e){switch(typeof e){case"bigint":case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function Rl(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function Hp(e,t,o){var i=Object.getOwnPropertyDescriptor(e.constructor.prototype,t);if(!e.hasOwnProperty(t)&&typeof i<"u"&&typeof i.get=="function"&&typeof i.set=="function"){var n=i.get,a=i.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return n.call(this)},set:function(s){o=""+s,a.call(this,s)}}),Object.defineProperty(e,t,{enumerable:i.enumerable}),{getValue:function(){return o},setValue:function(s){o=""+s},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function fs(e){if(!e._valueTracker){var t=Rl(e)?"checked":"value";e._valueTracker=Hp(e,t,""+e[t])}}function Ol(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var o=t.getValue(),i="";return e&&(i=Rl(e)?e.checked?"true":"false":e.value),e=i,e!==o?(t.setValue(e),!0):!1}function $n(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}var Fp=/[\n"\\]/g;function Pt(e){return e.replace(Fp,function(t){return"\\"+t.charCodeAt(0).toString(16)+" "})}function ys(e,t,o,i,n,a,s,r){e.name="",s!=null&&typeof s!="function"&&typeof s!="symbol"&&typeof s!="boolean"?e.type=s:e.removeAttribute("type"),t!=null?s==="number"?(t===0&&e.value===""||e.value!=t)&&(e.value=""+Mt(t)):e.value!==""+Mt(t)&&(e.value=""+Mt(t)):s!=="submit"&&s!=="reset"||e.removeAttribute("value"),t!=null?bs(e,s,Mt(t)):o!=null?bs(e,s,Mt(o)):i!=null&&e.removeAttribute("value"),n==null&&a!=null&&(e.defaultChecked=!!a),n!=null&&(e.checked=n&&typeof n!="function"&&typeof n!="symbol"),r!=null&&typeof r!="function"&&typeof r!="symbol"&&typeof r!="boolean"?e.name=""+Mt(r):e.removeAttribute("name")}function Wl(e,t,o,i,n,a,s,r){if(a!=null&&typeof a!="function"&&typeof a!="symbol"&&typeof a!="boolean"&&(e.type=a),t!=null||o!=null){if(!(a!=="submit"&&a!=="reset"||t!=null)){fs(e);return}o=o!=null?""+Mt(o):"",t=t!=null?""+Mt(t):o,r||t===e.value||(e.value=t),e.defaultValue=t}i=i??n,i=typeof i!="function"&&typeof i!="symbol"&&!!i,e.checked=r?e.checked:!!i,e.defaultChecked=!!i,s!=null&&typeof s!="function"&&typeof s!="symbol"&&typeof s!="boolean"&&(e.name=s),fs(e)}function bs(e,t,o){t==="number"&&$n(e.ownerDocument)===e||e.defaultValue===""+o||(e.defaultValue=""+o)}function pi(e,t,o,i){if(e=e.options,t){t={};for(var n=0;n<o.length;n++)t["$"+o[n]]=!0;for(o=0;o<e.length;o++)n=t.hasOwnProperty("$"+e[o].value),e[o].selected!==n&&(e[o].selected=n),n&&i&&(e[o].defaultSelected=!0)}else{for(o=""+Mt(o),t=null,n=0;n<e.length;n++){if(e[n].value===o){e[n].selected=!0,i&&(e[n].defaultSelected=!0);return}t!==null||e[n].disabled||(t=e[n])}t!==null&&(t.selected=!0)}}function Ql(e,t,o){if(t!=null&&(t=""+Mt(t),t!==e.value&&(e.value=t),o==null)){e.defaultValue!==t&&(e.defaultValue=t);return}e.defaultValue=o!=null?""+Mt(o):""}function Hl(e,t,o,i){if(t==null){if(i!=null){if(o!=null)throw Error(m(92));if(Ae(i)){if(1<i.length)throw Error(m(93));i=i[0]}o=i}o==null&&(o=""),t=o}o=Mt(t),e.defaultValue=o,i=e.textContent,i===o&&i!==""&&i!==null&&(e.value=i),fs(e)}function gi(e,t){if(t){var o=e.firstChild;if(o&&o===e.lastChild&&o.nodeType===3){o.nodeValue=t;return}}e.textContent=t}var Kp=new Set("animationIterationCount aspectRatio borderImageOutset borderImageSlice borderImageWidth boxFlex boxFlexGroup boxOrdinalGroup columnCount columns flex flexGrow flexPositive flexShrink flexNegative flexOrder gridArea gridRow gridRowEnd gridRowSpan gridRowStart gridColumn gridColumnEnd gridColumnSpan gridColumnStart fontWeight lineClamp lineHeight opacity order orphans scale tabSize widows zIndex zoom fillOpacity floodOpacity stopOpacity strokeDasharray strokeDashoffset strokeMiterlimit strokeOpacity strokeWidth MozAnimationIterationCount MozBoxFlex MozBoxFlexGroup MozLineClamp msAnimationIterationCount msFlex msZoom msFlexGrow msFlexNegative msFlexOrder msFlexPositive msFlexShrink msGridColumn msGridColumnSpan msGridRow msGridRowSpan WebkitAnimationIterationCount WebkitBoxFlex WebKitBoxFlexGroup WebkitBoxOrdinalGroup WebkitColumnCount WebkitColumns WebkitFlex WebkitFlexGrow WebkitFlexPositive WebkitFlexShrink WebkitLineClamp".split(" "));function Fl(e,t,o){var i=t.indexOf("--")===0;o==null||typeof o=="boolean"||o===""?i?e.setProperty(t,""):t==="float"?e.cssFloat="":e[t]="":i?e.setProperty(t,o):typeof o!="number"||o===0||Kp.has(t)?t==="float"?e.cssFloat=o:e[t]=(""+o).trim():e[t]=o+"px"}function Kl(e,t,o){if(t!=null&&typeof t!="object")throw Error(m(62));if(e=e.style,o!=null){for(var i in o)!o.hasOwnProperty(i)||t!=null&&t.hasOwnProperty(i)||(i.indexOf("--")===0?e.setProperty(i,""):i==="float"?e.cssFloat="":e[i]="");for(var n in t)i=t[n],t.hasOwnProperty(n)&&o[n]!==i&&Fl(e,n,i)}else for(var a in t)t.hasOwnProperty(a)&&Fl(e,a,t[a])}function vs(e){if(e.indexOf("-")===-1)return!1;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var _p=new Map([["acceptCharset","accept-charset"],["htmlFor","for"],["httpEquiv","http-equiv"],["crossOrigin","crossorigin"],["accentHeight","accent-height"],["alignmentBaseline","alignment-baseline"],["arabicForm","arabic-form"],["baselineShift","baseline-shift"],["capHeight","cap-height"],["clipPath","clip-path"],["clipRule","clip-rule"],["colorInterpolation","color-interpolation"],["colorInterpolationFilters","color-interpolation-filters"],["colorProfile","color-profile"],["colorRendering","color-rendering"],["dominantBaseline","dominant-baseline"],["enableBackground","enable-background"],["fillOpacity","fill-opacity"],["fillRule","fill-rule"],["floodColor","flood-color"],["floodOpacity","flood-opacity"],["fontFamily","font-family"],["fontSize","font-size"],["fontSizeAdjust","font-size-adjust"],["fontStretch","font-stretch"],["fontStyle","font-style"],["fontVariant","font-variant"],["fontWeight","font-weight"],["glyphName","glyph-name"],["glyphOrientationHorizontal","glyph-orientation-horizontal"],["glyphOrientationVertical","glyph-orientation-vertical"],["horizAdvX","horiz-adv-x"],["horizOriginX","horiz-origin-x"],["imageRendering","image-rendering"],["letterSpacing","letter-spacing"],["lightingColor","lighting-color"],["markerEnd","marker-end"],["markerMid","marker-mid"],["markerStart","marker-start"],["overlinePosition","overline-position"],["overlineThickness","overline-thickness"],["paintOrder","paint-order"],["panose-1","panose-1"],["pointerEvents","pointer-events"],["renderingIntent","rendering-intent"],["shapeRendering","shape-rendering"],["stopColor","stop-color"],["stopOpacity","stop-opacity"],["strikethroughPosition","strikethrough-position"],["strikethroughThickness","strikethrough-thickness"],["strokeDasharray","stroke-dasharray"],["strokeDashoffset","stroke-dashoffset"],["strokeLinecap","stroke-linecap"],["strokeLinejoin","stroke-linejoin"],["strokeMiterlimit","stroke-miterlimit"],["strokeOpacity","stroke-opacity"],["strokeWidth","stroke-width"],["textAnchor","text-anchor"],["textDecoration","text-decoration"],["textRendering","text-rendering"],["transformOrigin","transform-origin"],["underlinePosition","underline-position"],["underlineThickness","underline-thickness"],["unicodeBidi","unicode-bidi"],["unicodeRange","unicode-range"],["unitsPerEm","units-per-em"],["vAlphabetic","v-alphabetic"],["vHanging","v-hanging"],["vIdeographic","v-ideographic"],["vMathematical","v-mathematical"],["vectorEffect","vector-effect"],["vertAdvY","vert-adv-y"],["vertOriginX","vert-origin-x"],["vertOriginY","vert-origin-y"],["wordSpacing","word-spacing"],["writingMode","writing-mode"],["xmlnsXlink","xmlns:xlink"],["xHeight","x-height"]]),Xp=/^[\u0000-\u001F ]*j[\r\n\t]*a[\r\n\t]*v[\r\n\t]*a[\r\n\t]*s[\r\n\t]*c[\r\n\t]*r[\r\n\t]*i[\r\n\t]*p[\r\n\t]*t[\r\n\t]*:/i;function ea(e){return Xp.test(""+e)?"javascript:throw new Error('React has blocked a javascript: URL as a security precaution.')":e}function Ft(){}var ws=null;function Cs(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var hi=null,mi=null;function _l(e){var t=ci(e);if(t&&(e=t.stateNode)){var o=e[dt]||null;e:switch(e=t.stateNode,t.type){case"input":if(ys(e,o.value,o.defaultValue,o.defaultValue,o.checked,o.defaultChecked,o.type,o.name),t=o.name,o.type==="radio"&&t!=null){for(o=e;o.parentNode;)o=o.parentNode;for(o=o.querySelectorAll('input[name="'+Pt(""+t)+'"][type="radio"]'),t=0;t<o.length;t++){var i=o[t];if(i!==e&&i.form===e.form){var n=i[dt]||null;if(!n)throw Error(m(90));ys(i,n.value,n.defaultValue,n.defaultValue,n.checked,n.defaultChecked,n.type,n.name)}}for(t=0;t<o.length;t++)i=o[t],i.form===e.form&&Ol(i)}break e;case"textarea":Ql(e,o.value,o.defaultValue);break e;case"select":t=o.value,t!=null&&pi(e,!!o.multiple,t,!1)}}}var Ss=!1;function Xl(e,t,o){if(Ss)return e(t,o);Ss=!0;try{var i=e(t);return i}finally{if(Ss=!1,(hi!==null||mi!==null)&&(Ba(),hi&&(t=hi,e=mi,mi=hi=null,_l(t),e)))for(t=0;t<e.length;t++)_l(e[t])}}function an(e,t){var o=e.stateNode;if(o===null)return null;var i=o[dt]||null;if(i===null)return null;o=i[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(i=!i.disabled)||(e=e.type,i=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!i;break e;default:e=!1}if(e)return null;if(o&&typeof o!="function")throw Error(m(231,t,typeof o));return o}var Kt=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),xs=!1;if(Kt)try{var sn={};Object.defineProperty(sn,"passive",{get:function(){xs=!0}}),window.addEventListener("test",sn,sn),window.removeEventListener("test",sn,sn)}catch{xs=!1}var ho=null,ks=null,ta=null;function Zl(){if(ta)return ta;var e,t=ks,o=t.length,i,n="value"in ho?ho.value:ho.textContent,a=n.length;for(e=0;e<o&&t[e]===n[e];e++);var s=o-e;for(i=1;i<=s&&t[o-i]===n[a-i];i++);return ta=n.slice(e,1<i?1-i:void 0)}function oa(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function ia(){return!0}function Jl(){return!1}function pt(e){function t(o,i,n,a,s){this._reactName=o,this._targetInst=n,this.type=i,this.nativeEvent=a,this.target=s,this.currentTarget=null;for(var r in e)e.hasOwnProperty(r)&&(o=e[r],this[r]=o?o(a):a[r]);return this.isDefaultPrevented=(a.defaultPrevented!=null?a.defaultPrevented:a.returnValue===!1)?ia:Jl,this.isPropagationStopped=Jl,this}return N(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var o=this.nativeEvent;o&&(o.preventDefault?o.preventDefault():typeof o.returnValue!="unknown"&&(o.returnValue=!1),this.isDefaultPrevented=ia)},stopPropagation:function(){var o=this.nativeEvent;o&&(o.stopPropagation?o.stopPropagation():typeof o.cancelBubble!="unknown"&&(o.cancelBubble=!0),this.isPropagationStopped=ia)},persist:function(){},isPersistent:ia}),t}var No={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},na=pt(No),rn=N({},No,{view:0,detail:0}),Zp=pt(rn),As,Es,ln,aa=N({},rn,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:Ms,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==ln&&(ln&&e.type==="mousemove"?(As=e.screenX-ln.screenX,Es=e.screenY-ln.screenY):Es=As=0,ln=e),As)},movementY:function(e){return"movementY"in e?e.movementY:Es}}),$l=pt(aa),Jp=N({},aa,{dataTransfer:0}),$p=pt(Jp),eg=N({},rn,{relatedTarget:0}),Ts=pt(eg),tg=N({},No,{animationName:0,elapsedTime:0,pseudoElement:0}),og=pt(tg),ig=N({},No,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),ng=pt(ig),ag=N({},No,{data:0}),ec=pt(ag),sg={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},rg={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},lg={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function cg(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=lg[e])?!!t[e]:!1}function Ms(){return cg}var ug=N({},rn,{key:function(e){if(e.key){var t=sg[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=oa(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?rg[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:Ms,charCode:function(e){return e.type==="keypress"?oa(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?oa(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),dg=pt(ug),pg=N({},aa,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),tc=pt(pg),gg=N({},rn,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:Ms}),hg=pt(gg),mg=N({},No,{propertyName:0,elapsedTime:0,pseudoElement:0}),fg=pt(mg),yg=N({},aa,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),bg=pt(yg),vg=N({},No,{newState:0,oldState:0}),wg=pt(vg),Cg=[9,13,27,32],Ps=Kt&&"CompositionEvent"in window,cn=null;Kt&&"documentMode"in document&&(cn=document.documentMode);var Sg=Kt&&"TextEvent"in window&&!cn,oc=Kt&&(!Ps||cn&&8<cn&&11>=cn),ic=" ",nc=!1;function ac(e,t){switch(e){case"keyup":return Cg.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function sc(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var fi=!1;function xg(e,t){switch(e){case"compositionend":return sc(t);case"keypress":return t.which!==32?null:(nc=!0,ic);case"textInput":return e=t.data,e===ic&&nc?null:e;default:return null}}function kg(e,t){if(fi)return e==="compositionend"||!Ps&&ac(e,t)?(e=Zl(),ta=ks=ho=null,fi=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return oc&&t.locale!=="ko"?null:t.data;default:return null}}var Ag={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function rc(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!Ag[e.type]:t==="textarea"}function lc(e,t,o,i){hi?mi?mi.push(i):mi=[i]:hi=i,t=Ka(t,"onChange"),0<t.length&&(o=new na("onChange","change",null,o,i),e.push({event:o,listeners:t}))}var un=null,dn=null;function Eg(e){Wd(e,0)}function sa(e){var t=nn(e);if(Ol(t))return e}function cc(e,t){if(e==="change")return t}var uc=!1;if(Kt){var qs;if(Kt){var Is="oninput"in document;if(!Is){var dc=document.createElement("div");dc.setAttribute("oninput","return;"),Is=typeof dc.oninput=="function"}qs=Is}else qs=!1;uc=qs&&(!document.documentMode||9<document.documentMode)}function pc(){un&&(un.detachEvent("onpropertychange",gc),dn=un=null)}function gc(e){if(e.propertyName==="value"&&sa(dn)){var t=[];lc(t,dn,e,Cs(e)),Xl(Eg,t)}}function Tg(e,t,o){e==="focusin"?(pc(),un=t,dn=o,un.attachEvent("onpropertychange",gc)):e==="focusout"&&pc()}function Mg(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return sa(dn)}function Pg(e,t){if(e==="click")return sa(t)}function qg(e,t){if(e==="input"||e==="change")return sa(t)}function Ig(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var vt=typeof Object.is=="function"?Object.is:Ig;function pn(e,t){if(vt(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var o=Object.keys(e),i=Object.keys(t);if(o.length!==i.length)return!1;for(i=0;i<o.length;i++){var n=o[i];if(!ni.call(t,n)||!vt(e[n],t[n]))return!1}return!0}function hc(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function mc(e,t){var o=hc(e);e=0;for(var i;o;){if(o.nodeType===3){if(i=e+o.textContent.length,e<=t&&i>=t)return{node:o,offset:t-e};e=i}e:{for(;o;){if(o.nextSibling){o=o.nextSibling;break e}o=o.parentNode}o=void 0}o=hc(o)}}function fc(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?fc(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function yc(e){e=e!=null&&e.ownerDocument!=null&&e.ownerDocument.defaultView!=null?e.ownerDocument.defaultView:window;for(var t=$n(e.document);t instanceof e.HTMLIFrameElement;){try{var o=typeof t.contentWindow.location.href=="string"}catch{o=!1}if(o)e=t.contentWindow;else break;t=$n(e.document)}return t}function zs(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}var zg=Kt&&"documentMode"in document&&11>=document.documentMode,yi=null,Ds=null,gn=null,Gs=!1;function bc(e,t,o){var i=o.window===o?o.document:o.nodeType===9?o:o.ownerDocument;Gs||yi==null||yi!==$n(i)||(i=yi,"selectionStart"in i&&zs(i)?i={start:i.selectionStart,end:i.selectionEnd}:(i=(i.ownerDocument&&i.ownerDocument.defaultView||window).getSelection(),i={anchorNode:i.anchorNode,anchorOffset:i.anchorOffset,focusNode:i.focusNode,focusOffset:i.focusOffset}),gn&&pn(gn,i)||(gn=i,i=Ka(Ds,"onSelect"),0<i.length&&(t=new na("onSelect","select",null,t,o),e.push({event:t,listeners:i}),t.target=yi)))}function Bo(e,t){var o={};return o[e.toLowerCase()]=t.toLowerCase(),o["Webkit"+e]="webkit"+t,o["Moz"+e]="moz"+t,o}var bi={animationend:Bo("Animation","AnimationEnd"),animationiteration:Bo("Animation","AnimationIteration"),animationstart:Bo("Animation","AnimationStart"),transitionrun:Bo("Transition","TransitionRun"),transitionstart:Bo("Transition","TransitionStart"),transitioncancel:Bo("Transition","TransitionCancel"),transitionend:Bo("Transition","TransitionEnd")},js={},vc={};Kt&&(vc=document.createElement("div").style,"AnimationEvent"in window||(delete bi.animationend.animation,delete bi.animationiteration.animation,delete bi.animationstart.animation),"TransitionEvent"in window||delete bi.transitionend.transition);function Ro(e){if(js[e])return js[e];if(!bi[e])return e;var t=bi[e],o;for(o in t)if(t.hasOwnProperty(o)&&o in vc)return js[e]=t[o];return e}var wc=Ro("animationend"),Cc=Ro("animationiteration"),Sc=Ro("animationstart"),Dg=Ro("transitionrun"),Gg=Ro("transitionstart"),jg=Ro("transitioncancel"),xc=Ro("transitionend"),kc=new Map,Vs="abort auxClick beforeToggle cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");Vs.push("scrollEnd");function Lt(e,t){kc.set(e,t),Uo(t,[e])}var ra=typeof reportError=="function"?reportError:function(e){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var t=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof e=="object"&&e!==null&&typeof e.message=="string"?String(e.message):String(e),error:e});if(!window.dispatchEvent(t))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",e);return}console.error(e)},qt=[],vi=0,Ys=0;function la(){for(var e=vi,t=Ys=vi=0;t<e;){var o=qt[t];qt[t++]=null;var i=qt[t];qt[t++]=null;var n=qt[t];qt[t++]=null;var a=qt[t];if(qt[t++]=null,i!==null&&n!==null){var s=i.pending;s===null?n.next=n:(n.next=s.next,s.next=n),i.pending=n}a!==0&&Ac(o,n,a)}}function ca(e,t,o,i){qt[vi++]=e,qt[vi++]=t,qt[vi++]=o,qt[vi++]=i,Ys|=i,e.lanes|=i,e=e.alternate,e!==null&&(e.lanes|=i)}function Ls(e,t,o,i){return ca(e,t,o,i),ua(e)}function Oo(e,t){return ca(e,null,null,t),ua(e)}function Ac(e,t,o){e.lanes|=o;var i=e.alternate;i!==null&&(i.lanes|=o);for(var n=!1,a=e.return;a!==null;)a.childLanes|=o,i=a.alternate,i!==null&&(i.childLanes|=o),a.tag===22&&(e=a.stateNode,e===null||e._visibility&1||(n=!0)),e=a,a=a.return;return e.tag===3?(a=e.stateNode,n&&t!==null&&(n=31-Ye(o),e=a.hiddenUpdates,i=e[n],i===null?e[n]=[t]:i.push(t),t.lane=o|536870912),a):null}function ua(e){if(50<jn)throw jn=0,Fr=null,Error(m(185));for(var t=e.return;t!==null;)e=t,t=e.return;return e.tag===3?e.stateNode:null}var wi={};function Vg(e,t,o,i){this.tag=e,this.key=o,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.refCleanup=this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=i,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function wt(e,t,o,i){return new Vg(e,t,o,i)}function Us(e){return e=e.prototype,!(!e||!e.isReactComponent)}function _t(e,t){var o=e.alternate;return o===null?(o=wt(e.tag,t,e.key,e.mode),o.elementType=e.elementType,o.type=e.type,o.stateNode=e.stateNode,o.alternate=e,e.alternate=o):(o.pendingProps=t,o.type=e.type,o.flags=0,o.subtreeFlags=0,o.deletions=null),o.flags=e.flags&65011712,o.childLanes=e.childLanes,o.lanes=e.lanes,o.child=e.child,o.memoizedProps=e.memoizedProps,o.memoizedState=e.memoizedState,o.updateQueue=e.updateQueue,t=e.dependencies,o.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},o.sibling=e.sibling,o.index=e.index,o.ref=e.ref,o.refCleanup=e.refCleanup,o}function Ec(e,t){e.flags&=65011714;var o=e.alternate;return o===null?(e.childLanes=0,e.lanes=t,e.child=null,e.subtreeFlags=0,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.stateNode=null):(e.childLanes=o.childLanes,e.lanes=o.lanes,e.child=o.child,e.subtreeFlags=0,e.deletions=null,e.memoizedProps=o.memoizedProps,e.memoizedState=o.memoizedState,e.updateQueue=o.updateQueue,e.type=o.type,t=o.dependencies,e.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext}),e}function da(e,t,o,i,n,a){var s=0;if(i=e,typeof e=="function")Us(e)&&(s=1);else if(typeof e=="string")s=Bh(e,o,P.current)?26:e==="html"||e==="head"||e==="body"?27:5;else e:switch(e){case Je:return e=wt(31,o,t,n),e.elementType=Je,e.lanes=a,e;case ze:return Wo(o.children,n,a,t);case Se:s=8,n|=24;break;case De:return e=wt(12,o,t,n|2),e.elementType=De,e.lanes=a,e;case ct:return e=wt(13,o,t,n),e.elementType=ct,e.lanes=a,e;case nt:return e=wt(19,o,t,n),e.elementType=nt,e.lanes=a,e;default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case Ie:s=10;break e;case Fe:s=9;break e;case Qe:s=11;break e;case J:s=14;break e;case Pe:s=16,i=null;break e}s=29,o=Error(m(130,e===null?"null":typeof e,"")),i=null}return t=wt(s,o,t,n),t.elementType=e,t.type=i,t.lanes=a,t}function Wo(e,t,o,i){return e=wt(7,e,i,t),e.lanes=o,e}function Ns(e,t,o){return e=wt(6,e,null,t),e.lanes=o,e}function Tc(e){var t=wt(18,null,null,0);return t.stateNode=e,t}function Bs(e,t,o){return t=wt(4,e.children!==null?e.children:[],e.key,t),t.lanes=o,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}var Mc=new WeakMap;function It(e,t){if(typeof e=="object"&&e!==null){var o=Mc.get(e);return o!==void 0?o:(t={value:e,source:t,stack:Xi(t)},Mc.set(e,t),t)}return{value:e,source:t,stack:Xi(t)}}var Ci=[],Si=0,pa=null,hn=0,zt=[],Dt=0,mo=null,Rt=1,Ot="";function Xt(e,t){Ci[Si++]=hn,Ci[Si++]=pa,pa=e,hn=t}function Pc(e,t,o){zt[Dt++]=Rt,zt[Dt++]=Ot,zt[Dt++]=mo,mo=e;var i=Rt;e=Ot;var n=32-Ye(i)-1;i&=~(1<<n),o+=1;var a=32-Ye(t)+n;if(30<a){var s=n-n%5;a=(i&(1<<s)-1).toString(32),i>>=s,n-=s,Rt=1<<32-Ye(t)+n|o<<n|i,Ot=a+e}else Rt=1<<a|o<<n|i,Ot=e}function Rs(e){e.return!==null&&(Xt(e,1),Pc(e,1,0))}function Os(e){for(;e===pa;)pa=Ci[--Si],Ci[Si]=null,hn=Ci[--Si],Ci[Si]=null;for(;e===mo;)mo=zt[--Dt],zt[Dt]=null,Ot=zt[--Dt],zt[Dt]=null,Rt=zt[--Dt],zt[Dt]=null}function qc(e,t){zt[Dt++]=Rt,zt[Dt++]=Ot,zt[Dt++]=mo,Rt=t.id,Ot=t.overflow,mo=e}var et=null,Te=null,ce=!1,fo=null,Gt=!1,Ws=Error(m(519));function yo(e){var t=Error(m(418,1<arguments.length&&arguments[1]!==void 0&&arguments[1]?"text":"HTML",""));throw mn(It(t,e)),Ws}function Ic(e){var t=e.stateNode,o=e.type,i=e.memoizedProps;switch(t[$e]=e,t[dt]=i,o){case"dialog":ne("cancel",t),ne("close",t);break;case"iframe":case"object":case"embed":ne("load",t);break;case"video":case"audio":for(o=0;o<Yn.length;o++)ne(Yn[o],t);break;case"source":ne("error",t);break;case"img":case"image":case"link":ne("error",t),ne("load",t);break;case"details":ne("toggle",t);break;case"input":ne("invalid",t),Wl(t,i.value,i.defaultValue,i.checked,i.defaultChecked,i.type,i.name,!0);break;case"select":ne("invalid",t);break;case"textarea":ne("invalid",t),Hl(t,i.value,i.defaultValue,i.children)}o=i.children,typeof o!="string"&&typeof o!="number"&&typeof o!="bigint"||t.textContent===""+o||i.suppressHydrationWarning===!0||Kd(t.textContent,o)?(i.popover!=null&&(ne("beforetoggle",t),ne("toggle",t)),i.onScroll!=null&&ne("scroll",t),i.onScrollEnd!=null&&ne("scrollend",t),i.onClick!=null&&(t.onclick=Ft),t=!0):t=!1,t||yo(e,!0)}function zc(e){for(et=e.return;et;)switch(et.tag){case 5:case 31:case 13:Gt=!1;return;case 27:case 3:Gt=!0;return;default:et=et.return}}function xi(e){if(e!==et)return!1;if(!ce)return zc(e),ce=!0,!1;var t=e.tag,o;if((o=t!==3&&t!==27)&&((o=t===5)&&(o=e.type,o=!(o!=="form"&&o!=="button")||ll(e.type,e.memoizedProps)),o=!o),o&&Te&&yo(e),zc(e),t===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(m(317));Te=ip(e)}else if(t===31){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(m(317));Te=ip(e)}else t===27?(t=Te,Io(e.type)?(e=gl,gl=null,Te=e):Te=t):Te=et?Vt(e.stateNode.nextSibling):null;return!0}function Qo(){Te=et=null,ce=!1}function Qs(){var e=fo;return e!==null&&(ft===null?ft=e:ft.push.apply(ft,e),fo=null),e}function mn(e){fo===null?fo=[e]:fo.push(e)}var Hs=d(null),Ho=null,Zt=null;function bo(e,t,o){T(Hs,t._currentValue),t._currentValue=o}function Jt(e){e._currentValue=Hs.current,S(Hs)}function Fs(e,t,o){for(;e!==null;){var i=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,i!==null&&(i.childLanes|=t)):i!==null&&(i.childLanes&t)!==t&&(i.childLanes|=t),e===o)break;e=e.return}}function Ks(e,t,o,i){var n=e.child;for(n!==null&&(n.return=e);n!==null;){var a=n.dependencies;if(a!==null){var s=n.child;a=a.firstContext;e:for(;a!==null;){var r=a;a=n;for(var l=0;l<t.length;l++)if(r.context===t[l]){a.lanes|=o,r=a.alternate,r!==null&&(r.lanes|=o),Fs(a.return,o,e),i||(s=null);break e}a=r.next}}else if(n.tag===18){if(s=n.return,s===null)throw Error(m(341));s.lanes|=o,a=s.alternate,a!==null&&(a.lanes|=o),Fs(s,o,e),s=null}else s=n.child;if(s!==null)s.return=n;else for(s=n;s!==null;){if(s===e){s=null;break}if(n=s.sibling,n!==null){n.return=s.return,s=n;break}s=s.return}n=s}}function ki(e,t,o,i){e=null;for(var n=t,a=!1;n!==null;){if(!a){if((n.flags&524288)!==0)a=!0;else if((n.flags&262144)!==0)break}if(n.tag===10){var s=n.alternate;if(s===null)throw Error(m(387));if(s=s.memoizedProps,s!==null){var r=n.type;vt(n.pendingProps.value,s.value)||(e!==null?e.push(r):e=[r])}}else if(n===le.current){if(s=n.alternate,s===null)throw Error(m(387));s.memoizedState.memoizedState!==n.memoizedState.memoizedState&&(e!==null?e.push(Rn):e=[Rn])}n=n.return}e!==null&&Ks(t,e,o,i),t.flags|=262144}function ga(e){for(e=e.firstContext;e!==null;){if(!vt(e.context._currentValue,e.memoizedValue))return!0;e=e.next}return!1}function Fo(e){Ho=e,Zt=null,e=e.dependencies,e!==null&&(e.firstContext=null)}function tt(e){return Dc(Ho,e)}function ha(e,t){return Ho===null&&Fo(e),Dc(e,t)}function Dc(e,t){var o=t._currentValue;if(t={context:t,memoizedValue:o,next:null},Zt===null){if(e===null)throw Error(m(308));Zt=t,e.dependencies={lanes:0,firstContext:t},e.flags|=524288}else Zt=Zt.next=t;return o}var Yg=typeof AbortController<"u"?AbortController:function(){var e=[],t=this.signal={aborted:!1,addEventListener:function(o,i){e.push(i)}};this.abort=function(){t.aborted=!0,e.forEach(function(o){return o()})}},Lg=M.unstable_scheduleCallback,Ug=M.unstable_NormalPriority,Be={$$typeof:Ie,Consumer:null,Provider:null,_currentValue:null,_currentValue2:null,_threadCount:0};function _s(){return{controller:new Yg,data:new Map,refCount:0}}function fn(e){e.refCount--,e.refCount===0&&Lg(Ug,function(){e.controller.abort()})}var yn=null,Xs=0,Ai=0,Ei=null;function Ng(e,t){if(yn===null){var o=yn=[];Xs=0,Ai=$r(),Ei={status:"pending",value:void 0,then:function(i){o.push(i)}}}return Xs++,t.then(Gc,Gc),t}function Gc(){if(--Xs===0&&yn!==null){Ei!==null&&(Ei.status="fulfilled");var e=yn;yn=null,Ai=0,Ei=null;for(var t=0;t<e.length;t++)(0,e[t])()}}function Bg(e,t){var o=[],i={status:"pending",value:null,reason:null,then:function(n){o.push(n)}};return e.then(function(){i.status="fulfilled",i.value=t;for(var n=0;n<o.length;n++)(0,o[n])(t)},function(n){for(i.status="rejected",i.reason=n,n=0;n<o.length;n++)(0,o[n])(void 0)}),i}var jc=b.S;b.S=function(e,t){bd=rt(),typeof t=="object"&&t!==null&&typeof t.then=="function"&&Ng(e,t),jc!==null&&jc(e,t)};var Ko=d(null);function Zs(){var e=Ko.current;return e!==null?e:xe.pooledCache}function ma(e,t){t===null?T(Ko,Ko.current):T(Ko,t.pool)}function Vc(){var e=Zs();return e===null?null:{parent:Be._currentValue,pool:e}}var Ti=Error(m(460)),Js=Error(m(474)),fa=Error(m(542)),ya={then:function(){}};function Yc(e){return e=e.status,e==="fulfilled"||e==="rejected"}function Lc(e,t,o){switch(o=e[o],o===void 0?e.push(t):o!==t&&(t.then(Ft,Ft),t=o),t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Nc(e),e;default:if(typeof t.status=="string")t.then(Ft,Ft);else{if(e=xe,e!==null&&100<e.shellSuspendCounter)throw Error(m(482));e=t,e.status="pending",e.then(function(i){if(t.status==="pending"){var n=t;n.status="fulfilled",n.value=i}},function(i){if(t.status==="pending"){var n=t;n.status="rejected",n.reason=i}})}switch(t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Nc(e),e}throw Xo=t,Ti}}function _o(e){try{var t=e._init;return t(e._payload)}catch(o){throw o!==null&&typeof o=="object"&&typeof o.then=="function"?(Xo=o,Ti):o}}var Xo=null;function Uc(){if(Xo===null)throw Error(m(459));var e=Xo;return Xo=null,e}function Nc(e){if(e===Ti||e===fa)throw Error(m(483))}var Mi=null,bn=0;function ba(e){var t=bn;return bn+=1,Mi===null&&(Mi=[]),Lc(Mi,e,t)}function vn(e,t){t=t.props.ref,e.ref=t!==void 0?t:null}function va(e,t){throw t.$$typeof===ke?Error(m(525)):(e=Object.prototype.toString.call(t),Error(m(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e)))}function Bc(e){function t(p,u){if(e){var g=p.deletions;g===null?(p.deletions=[u],p.flags|=16):g.push(u)}}function o(p,u){if(!e)return null;for(;u!==null;)t(p,u),u=u.sibling;return null}function i(p){for(var u=new Map;p!==null;)p.key!==null?u.set(p.key,p):u.set(p.index,p),p=p.sibling;return u}function n(p,u){return p=_t(p,u),p.index=0,p.sibling=null,p}function a(p,u,g){return p.index=g,e?(g=p.alternate,g!==null?(g=g.index,g<u?(p.flags|=67108866,u):g):(p.flags|=67108866,u)):(p.flags|=1048576,u)}function s(p){return e&&p.alternate===null&&(p.flags|=67108866),p}function r(p,u,g,w){return u===null||u.tag!==6?(u=Ns(g,p.mode,w),u.return=p,u):(u=n(u,g),u.return=p,u)}function l(p,u,g,w){var Y=g.type;return Y===ze?v(p,u,g.props.children,w,g.key):u!==null&&(u.elementType===Y||typeof Y=="object"&&Y!==null&&Y.$$typeof===Pe&&_o(Y)===u.type)?(u=n(u,g.props),vn(u,g),u.return=p,u):(u=da(g.type,g.key,g.props,null,p.mode,w),vn(u,g),u.return=p,u)}function h(p,u,g,w){return u===null||u.tag!==4||u.stateNode.containerInfo!==g.containerInfo||u.stateNode.implementation!==g.implementation?(u=Bs(g,p.mode,w),u.return=p,u):(u=n(u,g.children||[]),u.return=p,u)}function v(p,u,g,w,Y){return u===null||u.tag!==7?(u=Wo(g,p.mode,w,Y),u.return=p,u):(u=n(u,g),u.return=p,u)}function C(p,u,g){if(typeof u=="string"&&u!==""||typeof u=="number"||typeof u=="bigint")return u=Ns(""+u,p.mode,g),u.return=p,u;if(typeof u=="object"&&u!==null){switch(u.$$typeof){case st:return g=da(u.type,u.key,u.props,null,p.mode,g),vn(g,u),g.return=p,g;case Ze:return u=Bs(u,p.mode,g),u.return=p,u;case Pe:return u=_o(u),C(p,u,g)}if(Ae(u)||be(u))return u=Wo(u,p.mode,g,null),u.return=p,u;if(typeof u.then=="function")return C(p,ba(u),g);if(u.$$typeof===Ie)return C(p,ha(p,u),g);va(p,u)}return null}function f(p,u,g,w){var Y=u!==null?u.key:null;if(typeof g=="string"&&g!==""||typeof g=="number"||typeof g=="bigint")return Y!==null?null:r(p,u,""+g,w);if(typeof g=="object"&&g!==null){switch(g.$$typeof){case st:return g.key===Y?l(p,u,g,w):null;case Ze:return g.key===Y?h(p,u,g,w):null;case Pe:return g=_o(g),f(p,u,g,w)}if(Ae(g)||be(g))return Y!==null?null:v(p,u,g,w,null);if(typeof g.then=="function")return f(p,u,ba(g),w);if(g.$$typeof===Ie)return f(p,u,ha(p,g),w);va(p,g)}return null}function y(p,u,g,w,Y){if(typeof w=="string"&&w!==""||typeof w=="number"||typeof w=="bigint")return p=p.get(g)||null,r(u,p,""+w,Y);if(typeof w=="object"&&w!==null){switch(w.$$typeof){case st:return p=p.get(w.key===null?g:w.key)||null,l(u,p,w,Y);case Ze:return p=p.get(w.key===null?g:w.key)||null,h(u,p,w,Y);case Pe:return w=_o(w),y(p,u,g,w,Y)}if(Ae(w)||be(w))return p=p.get(g)||null,v(u,p,w,Y,null);if(typeof w.then=="function")return y(p,u,g,ba(w),Y);if(w.$$typeof===Ie)return y(p,u,g,ha(u,w),Y);va(u,w)}return null}function z(p,u,g,w){for(var Y=null,de=null,G=u,ee=u=0,se=null;G!==null&&ee<g.length;ee++){G.index>ee?(se=G,G=null):se=G.sibling;var pe=f(p,G,g[ee],w);if(pe===null){G===null&&(G=se);break}e&&G&&pe.alternate===null&&t(p,G),u=a(pe,u,ee),de===null?Y=pe:de.sibling=pe,de=pe,G=se}if(ee===g.length)return o(p,G),ce&&Xt(p,ee),Y;if(G===null){for(;ee<g.length;ee++)G=C(p,g[ee],w),G!==null&&(u=a(G,u,ee),de===null?Y=G:de.sibling=G,de=G);return ce&&Xt(p,ee),Y}for(G=i(G);ee<g.length;ee++)se=y(G,p,ee,g[ee],w),se!==null&&(e&&se.alternate!==null&&G.delete(se.key===null?ee:se.key),u=a(se,u,ee),de===null?Y=se:de.sibling=se,de=se);return e&&G.forEach(function(Vo){return t(p,Vo)}),ce&&Xt(p,ee),Y}function B(p,u,g,w){if(g==null)throw Error(m(151));for(var Y=null,de=null,G=u,ee=u=0,se=null,pe=g.next();G!==null&&!pe.done;ee++,pe=g.next()){G.index>ee?(se=G,G=null):se=G.sibling;var Vo=f(p,G,pe.value,w);if(Vo===null){G===null&&(G=se);break}e&&G&&Vo.alternate===null&&t(p,G),u=a(Vo,u,ee),de===null?Y=Vo:de.sibling=Vo,de=Vo,G=se}if(pe.done)return o(p,G),ce&&Xt(p,ee),Y;if(G===null){for(;!pe.done;ee++,pe=g.next())pe=C(p,pe.value,w),pe!==null&&(u=a(pe,u,ee),de===null?Y=pe:de.sibling=pe,de=pe);return ce&&Xt(p,ee),Y}for(G=i(G);!pe.done;ee++,pe=g.next())pe=y(G,p,ee,pe.value,w),pe!==null&&(e&&pe.alternate!==null&&G.delete(pe.key===null?ee:pe.key),u=a(pe,u,ee),de===null?Y=pe:de.sibling=pe,de=pe);return e&&G.forEach(function(Jh){return t(p,Jh)}),ce&&Xt(p,ee),Y}function Ce(p,u,g,w){if(typeof g=="object"&&g!==null&&g.type===ze&&g.key===null&&(g=g.props.children),typeof g=="object"&&g!==null){switch(g.$$typeof){case st:e:{for(var Y=g.key;u!==null;){if(u.key===Y){if(Y=g.type,Y===ze){if(u.tag===7){o(p,u.sibling),w=n(u,g.props.children),w.return=p,p=w;break e}}else if(u.elementType===Y||typeof Y=="object"&&Y!==null&&Y.$$typeof===Pe&&_o(Y)===u.type){o(p,u.sibling),w=n(u,g.props),vn(w,g),w.return=p,p=w;break e}o(p,u);break}else t(p,u);u=u.sibling}g.type===ze?(w=Wo(g.props.children,p.mode,w,g.key),w.return=p,p=w):(w=da(g.type,g.key,g.props,null,p.mode,w),vn(w,g),w.return=p,p=w)}return s(p);case Ze:e:{for(Y=g.key;u!==null;){if(u.key===Y)if(u.tag===4&&u.stateNode.containerInfo===g.containerInfo&&u.stateNode.implementation===g.implementation){o(p,u.sibling),w=n(u,g.children||[]),w.return=p,p=w;break e}else{o(p,u);break}else t(p,u);u=u.sibling}w=Bs(g,p.mode,w),w.return=p,p=w}return s(p);case Pe:return g=_o(g),Ce(p,u,g,w)}if(Ae(g))return z(p,u,g,w);if(be(g)){if(Y=be(g),typeof Y!="function")throw Error(m(150));return g=Y.call(g),B(p,u,g,w)}if(typeof g.then=="function")return Ce(p,u,ba(g),w);if(g.$$typeof===Ie)return Ce(p,u,ha(p,g),w);va(p,g)}return typeof g=="string"&&g!==""||typeof g=="number"||typeof g=="bigint"?(g=""+g,u!==null&&u.tag===6?(o(p,u.sibling),w=n(u,g),w.return=p,p=w):(o(p,u),w=Ns(g,p.mode,w),w.return=p,p=w),s(p)):o(p,u)}return function(p,u,g,w){try{bn=0;var Y=Ce(p,u,g,w);return Mi=null,Y}catch(G){if(G===Ti||G===fa)throw G;var de=wt(29,G,null,p.mode);return de.lanes=w,de.return=p,de}finally{}}}var Zo=Bc(!0),Rc=Bc(!1),vo=!1;function $s(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,lanes:0,hiddenCallbacks:null},callbacks:null}}function er(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,callbacks:null})}function wo(e){return{lane:e,tag:0,payload:null,callback:null,next:null}}function Co(e,t,o){var i=e.updateQueue;if(i===null)return null;if(i=i.shared,(ge&2)!==0){var n=i.pending;return n===null?t.next=t:(t.next=n.next,n.next=t),i.pending=t,t=ua(e),Ac(e,null,o),t}return ca(e,i,t,o),ua(e)}function wn(e,t,o){if(t=t.updateQueue,t!==null&&(t=t.shared,(o&4194048)!==0)){var i=t.lanes;i&=e.pendingLanes,o|=i,t.lanes=o,Dl(e,o)}}function tr(e,t){var o=e.updateQueue,i=e.alternate;if(i!==null&&(i=i.updateQueue,o===i)){var n=null,a=null;if(o=o.firstBaseUpdate,o!==null){do{var s={lane:o.lane,tag:o.tag,payload:o.payload,callback:null,next:null};a===null?n=a=s:a=a.next=s,o=o.next}while(o!==null);a===null?n=a=t:a=a.next=t}else n=a=t;o={baseState:i.baseState,firstBaseUpdate:n,lastBaseUpdate:a,shared:i.shared,callbacks:i.callbacks},e.updateQueue=o;return}e=o.lastBaseUpdate,e===null?o.firstBaseUpdate=t:e.next=t,o.lastBaseUpdate=t}var or=!1;function Cn(){if(or){var e=Ei;if(e!==null)throw e}}function Sn(e,t,o,i){or=!1;var n=e.updateQueue;vo=!1;var a=n.firstBaseUpdate,s=n.lastBaseUpdate,r=n.shared.pending;if(r!==null){n.shared.pending=null;var l=r,h=l.next;l.next=null,s===null?a=h:s.next=h,s=l;var v=e.alternate;v!==null&&(v=v.updateQueue,r=v.lastBaseUpdate,r!==s&&(r===null?v.firstBaseUpdate=h:r.next=h,v.lastBaseUpdate=l))}if(a!==null){var C=n.baseState;s=0,v=h=l=null,r=a;do{var f=r.lane&-536870913,y=f!==r.lane;if(y?(ae&f)===f:(i&f)===f){f!==0&&f===Ai&&(or=!0),v!==null&&(v=v.next={lane:0,tag:r.tag,payload:r.payload,callback:null,next:null});e:{var z=e,B=r;f=t;var Ce=o;switch(B.tag){case 1:if(z=B.payload,typeof z=="function"){C=z.call(Ce,C,f);break e}C=z;break e;case 3:z.flags=z.flags&-65537|128;case 0:if(z=B.payload,f=typeof z=="function"?z.call(Ce,C,f):z,f==null)break e;C=N({},C,f);break e;case 2:vo=!0}}f=r.callback,f!==null&&(e.flags|=64,y&&(e.flags|=8192),y=n.callbacks,y===null?n.callbacks=[f]:y.push(f))}else y={lane:f,tag:r.tag,payload:r.payload,callback:r.callback,next:null},v===null?(h=v=y,l=C):v=v.next=y,s|=f;if(r=r.next,r===null){if(r=n.shared.pending,r===null)break;y=r,r=y.next,y.next=null,n.lastBaseUpdate=y,n.shared.pending=null}}while(!0);v===null&&(l=C),n.baseState=l,n.firstBaseUpdate=h,n.lastBaseUpdate=v,a===null&&(n.shared.lanes=0),Eo|=s,e.lanes=s,e.memoizedState=C}}function Oc(e,t){if(typeof e!="function")throw Error(m(191,e));e.call(t)}function Wc(e,t){var o=e.callbacks;if(o!==null)for(e.callbacks=null,e=0;e<o.length;e++)Oc(o[e],t)}var Pi=d(null),wa=d(0);function Qc(e,t){e=ro,T(wa,e),T(Pi,t),ro=e|t.baseLanes}function ir(){T(wa,ro),T(Pi,Pi.current)}function nr(){ro=wa.current,S(Pi),S(wa)}var Ct=d(null),jt=null;function So(e){var t=e.alternate;T(Le,Le.current&1),T(Ct,e),jt===null&&(t===null||Pi.current!==null||t.memoizedState!==null)&&(jt=e)}function ar(e){T(Le,Le.current),T(Ct,e),jt===null&&(jt=e)}function Hc(e){e.tag===22?(T(Le,Le.current),T(Ct,e),jt===null&&(jt=e)):xo()}function xo(){T(Le,Le.current),T(Ct,Ct.current)}function St(e){S(Ct),jt===e&&(jt=null),S(Le)}var Le=d(0);function Ca(e){for(var t=e;t!==null;){if(t.tag===13){var o=t.memoizedState;if(o!==null&&(o=o.dehydrated,o===null||dl(o)||pl(o)))return t}else if(t.tag===19&&(t.memoizedProps.revealOrder==="forwards"||t.memoizedProps.revealOrder==="backwards"||t.memoizedProps.revealOrder==="unstable_legacy-backwards"||t.memoizedProps.revealOrder==="together")){if((t.flags&128)!==0)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var $t=0,$=null,ve=null,Re=null,Sa=!1,qi=!1,Jo=!1,xa=0,xn=0,Ii=null,Rg=0;function Ge(){throw Error(m(321))}function sr(e,t){if(t===null)return!1;for(var o=0;o<t.length&&o<e.length;o++)if(!vt(e[o],t[o]))return!1;return!0}function rr(e,t,o,i,n,a){return $t=a,$=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,b.H=e===null||e.memoizedState===null?Pu:Sr,Jo=!1,a=o(i,n),Jo=!1,qi&&(a=Kc(t,o,i,n)),Fc(e),a}function Fc(e){b.H=En;var t=ve!==null&&ve.next!==null;if($t=0,Re=ve=$=null,Sa=!1,xn=0,Ii=null,t)throw Error(m(300));e===null||Oe||(e=e.dependencies,e!==null&&ga(e)&&(Oe=!0))}function Kc(e,t,o,i){$=e;var n=0;do{if(qi&&(Ii=null),xn=0,qi=!1,25<=n)throw Error(m(301));if(n+=1,Re=ve=null,e.updateQueue!=null){var a=e.updateQueue;a.lastEffect=null,a.events=null,a.stores=null,a.memoCache!=null&&(a.memoCache.index=0)}b.H=qu,a=t(o,i)}while(qi);return a}function Og(){var e=b.H,t=e.useState()[0];return t=typeof t.then=="function"?kn(t):t,e=e.useState()[0],(ve!==null?ve.memoizedState:null)!==e&&($.flags|=1024),t}function lr(){var e=xa!==0;return xa=0,e}function cr(e,t,o){t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~o}function ur(e){if(Sa){for(e=e.memoizedState;e!==null;){var t=e.queue;t!==null&&(t.pending=null),e=e.next}Sa=!1}$t=0,Re=ve=$=null,qi=!1,xn=xa=0,Ii=null}function lt(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Re===null?$.memoizedState=Re=e:Re=Re.next=e,Re}function Ue(){if(ve===null){var e=$.alternate;e=e!==null?e.memoizedState:null}else e=ve.next;var t=Re===null?$.memoizedState:Re.next;if(t!==null)Re=t,ve=e;else{if(e===null)throw $.alternate===null?Error(m(467)):Error(m(310));ve=e,e={memoizedState:ve.memoizedState,baseState:ve.baseState,baseQueue:ve.baseQueue,queue:ve.queue,next:null},Re===null?$.memoizedState=Re=e:Re=Re.next=e}return Re}function ka(){return{lastEffect:null,events:null,stores:null,memoCache:null}}function kn(e){var t=xn;return xn+=1,Ii===null&&(Ii=[]),e=Lc(Ii,e,t),t=$,(Re===null?t.memoizedState:Re.next)===null&&(t=t.alternate,b.H=t===null||t.memoizedState===null?Pu:Sr),e}function Aa(e){if(e!==null&&typeof e=="object"){if(typeof e.then=="function")return kn(e);if(e.$$typeof===Ie)return tt(e)}throw Error(m(438,String(e)))}function dr(e){var t=null,o=$.updateQueue;if(o!==null&&(t=o.memoCache),t==null){var i=$.alternate;i!==null&&(i=i.updateQueue,i!==null&&(i=i.memoCache,i!=null&&(t={data:i.data.map(function(n){return n.slice()}),index:0})))}if(t==null&&(t={data:[],index:0}),o===null&&(o=ka(),$.updateQueue=o),o.memoCache=t,o=t.data[t.index],o===void 0)for(o=t.data[t.index]=Array(e),i=0;i<e;i++)o[i]=Ne;return t.index++,o}function eo(e,t){return typeof t=="function"?t(e):t}function Ea(e){var t=Ue();return pr(t,ve,e)}function pr(e,t,o){var i=e.queue;if(i===null)throw Error(m(311));i.lastRenderedReducer=o;var n=e.baseQueue,a=i.pending;if(a!==null){if(n!==null){var s=n.next;n.next=a.next,a.next=s}t.baseQueue=n=a,i.pending=null}if(a=e.baseState,n===null)e.memoizedState=a;else{t=n.next;var r=s=null,l=null,h=t,v=!1;do{var C=h.lane&-536870913;if(C!==h.lane?(ae&C)===C:($t&C)===C){var f=h.revertLane;if(f===0)l!==null&&(l=l.next={lane:0,revertLane:0,gesture:null,action:h.action,hasEagerState:h.hasEagerState,eagerState:h.eagerState,next:null}),C===Ai&&(v=!0);else if(($t&f)===f){h=h.next,f===Ai&&(v=!0);continue}else C={lane:0,revertLane:h.revertLane,gesture:null,action:h.action,hasEagerState:h.hasEagerState,eagerState:h.eagerState,next:null},l===null?(r=l=C,s=a):l=l.next=C,$.lanes|=f,Eo|=f;C=h.action,Jo&&o(a,C),a=h.hasEagerState?h.eagerState:o(a,C)}else f={lane:C,revertLane:h.revertLane,gesture:h.gesture,action:h.action,hasEagerState:h.hasEagerState,eagerState:h.eagerState,next:null},l===null?(r=l=f,s=a):l=l.next=f,$.lanes|=C,Eo|=C;h=h.next}while(h!==null&&h!==t);if(l===null?s=a:l.next=r,!vt(a,e.memoizedState)&&(Oe=!0,v&&(o=Ei,o!==null)))throw o;e.memoizedState=a,e.baseState=s,e.baseQueue=l,i.lastRenderedState=a}return n===null&&(i.lanes=0),[e.memoizedState,i.dispatch]}function gr(e){var t=Ue(),o=t.queue;if(o===null)throw Error(m(311));o.lastRenderedReducer=e;var i=o.dispatch,n=o.pending,a=t.memoizedState;if(n!==null){o.pending=null;var s=n=n.next;do a=e(a,s.action),s=s.next;while(s!==n);vt(a,t.memoizedState)||(Oe=!0),t.memoizedState=a,t.baseQueue===null&&(t.baseState=a),o.lastRenderedState=a}return[a,i]}function _c(e,t,o){var i=$,n=Ue(),a=ce;if(a){if(o===void 0)throw Error(m(407));o=o()}else o=t();var s=!vt((ve||n).memoizedState,o);if(s&&(n.memoizedState=o,Oe=!0),n=n.queue,fr(Jc.bind(null,i,n,e),[e]),n.getSnapshot!==t||s||Re!==null&&Re.memoizedState.tag&1){if(i.flags|=2048,zi(9,{destroy:void 0},Zc.bind(null,i,n,o,t),null),xe===null)throw Error(m(349));a||($t&127)!==0||Xc(i,t,o)}return o}function Xc(e,t,o){e.flags|=16384,e={getSnapshot:t,value:o},t=$.updateQueue,t===null?(t=ka(),$.updateQueue=t,t.stores=[e]):(o=t.stores,o===null?t.stores=[e]:o.push(e))}function Zc(e,t,o,i){t.value=o,t.getSnapshot=i,$c(t)&&eu(e)}function Jc(e,t,o){return o(function(){$c(t)&&eu(e)})}function $c(e){var t=e.getSnapshot;e=e.value;try{var o=t();return!vt(e,o)}catch{return!0}}function eu(e){var t=Oo(e,2);t!==null&&yt(t,e,2)}function hr(e){var t=lt();if(typeof e=="function"){var o=e;if(e=o(),Jo){oe(!0);try{o()}finally{oe(!1)}}}return t.memoizedState=t.baseState=e,t.queue={pending:null,lanes:0,dispatch:null,lastRenderedReducer:eo,lastRenderedState:e},t}function tu(e,t,o,i){return e.baseState=o,pr(e,ve,typeof i=="function"?i:eo)}function Wg(e,t,o,i,n){if(Pa(e))throw Error(m(485));if(e=t.action,e!==null){var a={payload:n,action:e,next:null,isTransition:!0,status:"pending",value:null,reason:null,listeners:[],then:function(s){a.listeners.push(s)}};b.T!==null?o(!0):a.isTransition=!1,i(a),o=t.pending,o===null?(a.next=t.pending=a,ou(t,a)):(a.next=o.next,t.pending=o.next=a)}}function ou(e,t){var o=t.action,i=t.payload,n=e.state;if(t.isTransition){var a=b.T,s={};b.T=s;try{var r=o(n,i),l=b.S;l!==null&&l(s,r),iu(e,t,r)}catch(h){mr(e,t,h)}finally{a!==null&&s.types!==null&&(a.types=s.types),b.T=a}}else try{a=o(n,i),iu(e,t,a)}catch(h){mr(e,t,h)}}function iu(e,t,o){o!==null&&typeof o=="object"&&typeof o.then=="function"?o.then(function(i){nu(e,t,i)},function(i){return mr(e,t,i)}):nu(e,t,o)}function nu(e,t,o){t.status="fulfilled",t.value=o,au(t),e.state=o,t=e.pending,t!==null&&(o=t.next,o===t?e.pending=null:(o=o.next,t.next=o,ou(e,o)))}function mr(e,t,o){var i=e.pending;if(e.pending=null,i!==null){i=i.next;do t.status="rejected",t.reason=o,au(t),t=t.next;while(t!==i)}e.action=null}function au(e){e=e.listeners;for(var t=0;t<e.length;t++)(0,e[t])()}function su(e,t){return t}function ru(e,t){if(ce){var o=xe.formState;if(o!==null){e:{var i=$;if(ce){if(Te){t:{for(var n=Te,a=Gt;n.nodeType!==8;){if(!a){n=null;break t}if(n=Vt(n.nextSibling),n===null){n=null;break t}}a=n.data,n=a==="F!"||a==="F"?n:null}if(n){Te=Vt(n.nextSibling),i=n.data==="F!";break e}}yo(i)}i=!1}i&&(t=o[0])}}return o=lt(),o.memoizedState=o.baseState=t,i={pending:null,lanes:0,dispatch:null,lastRenderedReducer:su,lastRenderedState:t},o.queue=i,o=Eu.bind(null,$,i),i.dispatch=o,i=hr(!1),a=Cr.bind(null,$,!1,i.queue),i=lt(),n={state:t,dispatch:null,action:e,pending:null},i.queue=n,o=Wg.bind(null,$,n,a,o),n.dispatch=o,i.memoizedState=e,[t,o,!1]}function lu(e){var t=Ue();return cu(t,ve,e)}function cu(e,t,o){if(t=pr(e,t,su)[0],e=Ea(eo)[0],typeof t=="object"&&t!==null&&typeof t.then=="function")try{var i=kn(t)}catch(s){throw s===Ti?fa:s}else i=t;t=Ue();var n=t.queue,a=n.dispatch;return o!==t.memoizedState&&($.flags|=2048,zi(9,{destroy:void 0},Qg.bind(null,n,o),null)),[i,a,e]}function Qg(e,t){e.action=t}function uu(e){var t=Ue(),o=ve;if(o!==null)return cu(t,o,e);Ue(),t=t.memoizedState,o=Ue();var i=o.queue.dispatch;return o.memoizedState=e,[t,i,!1]}function zi(e,t,o,i){return e={tag:e,create:o,deps:i,inst:t,next:null},t=$.updateQueue,t===null&&(t=ka(),$.updateQueue=t),o=t.lastEffect,o===null?t.lastEffect=e.next=e:(i=o.next,o.next=e,e.next=i,t.lastEffect=e),e}function du(){return Ue().memoizedState}function Ta(e,t,o,i){var n=lt();$.flags|=e,n.memoizedState=zi(1|t,{destroy:void 0},o,i===void 0?null:i)}function Ma(e,t,o,i){var n=Ue();i=i===void 0?null:i;var a=n.memoizedState.inst;ve!==null&&i!==null&&sr(i,ve.memoizedState.deps)?n.memoizedState=zi(t,a,o,i):($.flags|=e,n.memoizedState=zi(1|t,a,o,i))}function pu(e,t){Ta(8390656,8,e,t)}function fr(e,t){Ma(2048,8,e,t)}function Hg(e){$.flags|=4;var t=$.updateQueue;if(t===null)t=ka(),$.updateQueue=t,t.events=[e];else{var o=t.events;o===null?t.events=[e]:o.push(e)}}function gu(e){var t=Ue().memoizedState;return Hg({ref:t,nextImpl:e}),function(){if((ge&2)!==0)throw Error(m(440));return t.impl.apply(void 0,arguments)}}function hu(e,t){return Ma(4,2,e,t)}function mu(e,t){return Ma(4,4,e,t)}function fu(e,t){if(typeof t=="function"){e=e();var o=t(e);return function(){typeof o=="function"?o():t(null)}}if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function yu(e,t,o){o=o!=null?o.concat([e]):null,Ma(4,4,fu.bind(null,t,e),o)}function yr(){}function bu(e,t){var o=Ue();t=t===void 0?null:t;var i=o.memoizedState;return t!==null&&sr(t,i[1])?i[0]:(o.memoizedState=[e,t],e)}function vu(e,t){var o=Ue();t=t===void 0?null:t;var i=o.memoizedState;if(t!==null&&sr(t,i[1]))return i[0];if(i=e(),Jo){oe(!0);try{e()}finally{oe(!1)}}return o.memoizedState=[i,t],i}function br(e,t,o){return o===void 0||($t&1073741824)!==0&&(ae&261930)===0?e.memoizedState=t:(e.memoizedState=o,e=wd(),$.lanes|=e,Eo|=e,o)}function wu(e,t,o,i){return vt(o,t)?o:Pi.current!==null?(e=br(e,o,i),vt(e,t)||(Oe=!0),e):($t&42)===0||($t&1073741824)!==0&&(ae&261930)===0?(Oe=!0,e.memoizedState=o):(e=wd(),$.lanes|=e,Eo|=e,t)}function Cu(e,t,o,i,n){var a=E.p;E.p=a!==0&&8>a?a:8;var s=b.T,r={};b.T=r,Cr(e,!1,t,o);try{var l=n(),h=b.S;if(h!==null&&h(r,l),l!==null&&typeof l=="object"&&typeof l.then=="function"){var v=Bg(l,i);An(e,t,v,At(e))}else An(e,t,i,At(e))}catch(C){An(e,t,{then:function(){},status:"rejected",reason:C},At())}finally{E.p=a,s!==null&&r.types!==null&&(s.types=r.types),b.T=s}}function Fg(){}function vr(e,t,o,i){if(e.tag!==5)throw Error(m(476));var n=Su(e).queue;Cu(e,n,t,V,o===null?Fg:function(){return xu(e),o(i)})}function Su(e){var t=e.memoizedState;if(t!==null)return t;t={memoizedState:V,baseState:V,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:eo,lastRenderedState:V},next:null};var o={};return t.next={memoizedState:o,baseState:o,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:eo,lastRenderedState:o},next:null},e.memoizedState=t,e=e.alternate,e!==null&&(e.memoizedState=t),t}function xu(e){var t=Su(e);t.next===null&&(t=e.alternate.memoizedState),An(e,t.next.queue,{},At())}function wr(){return tt(Rn)}function ku(){return Ue().memoizedState}function Au(){return Ue().memoizedState}function Kg(e){for(var t=e.return;t!==null;){switch(t.tag){case 24:case 3:var o=At();e=wo(o);var i=Co(t,e,o);i!==null&&(yt(i,t,o),wn(i,t,o)),t={cache:_s()},e.payload=t;return}t=t.return}}function _g(e,t,o){var i=At();o={lane:i,revertLane:0,gesture:null,action:o,hasEagerState:!1,eagerState:null,next:null},Pa(e)?Tu(t,o):(o=Ls(e,t,o,i),o!==null&&(yt(o,e,i),Mu(o,t,i)))}function Eu(e,t,o){var i=At();An(e,t,o,i)}function An(e,t,o,i){var n={lane:i,revertLane:0,gesture:null,action:o,hasEagerState:!1,eagerState:null,next:null};if(Pa(e))Tu(t,n);else{var a=e.alternate;if(e.lanes===0&&(a===null||a.lanes===0)&&(a=t.lastRenderedReducer,a!==null))try{var s=t.lastRenderedState,r=a(s,o);if(n.hasEagerState=!0,n.eagerState=r,vt(r,s))return ca(e,t,n,0),xe===null&&la(),!1}catch{}finally{}if(o=Ls(e,t,n,i),o!==null)return yt(o,e,i),Mu(o,t,i),!0}return!1}function Cr(e,t,o,i){if(i={lane:2,revertLane:$r(),gesture:null,action:i,hasEagerState:!1,eagerState:null,next:null},Pa(e)){if(t)throw Error(m(479))}else t=Ls(e,o,i,2),t!==null&&yt(t,e,2)}function Pa(e){var t=e.alternate;return e===$||t!==null&&t===$}function Tu(e,t){qi=Sa=!0;var o=e.pending;o===null?t.next=t:(t.next=o.next,o.next=t),e.pending=t}function Mu(e,t,o){if((o&4194048)!==0){var i=t.lanes;i&=e.pendingLanes,o|=i,t.lanes=o,Dl(e,o)}}var En={readContext:tt,use:Aa,useCallback:Ge,useContext:Ge,useEffect:Ge,useImperativeHandle:Ge,useLayoutEffect:Ge,useInsertionEffect:Ge,useMemo:Ge,useReducer:Ge,useRef:Ge,useState:Ge,useDebugValue:Ge,useDeferredValue:Ge,useTransition:Ge,useSyncExternalStore:Ge,useId:Ge,useHostTransitionStatus:Ge,useFormState:Ge,useActionState:Ge,useOptimistic:Ge,useMemoCache:Ge,useCacheRefresh:Ge};En.useEffectEvent=Ge;var Pu={readContext:tt,use:Aa,useCallback:function(e,t){return lt().memoizedState=[e,t===void 0?null:t],e},useContext:tt,useEffect:pu,useImperativeHandle:function(e,t,o){o=o!=null?o.concat([e]):null,Ta(4194308,4,fu.bind(null,t,e),o)},useLayoutEffect:function(e,t){return Ta(4194308,4,e,t)},useInsertionEffect:function(e,t){Ta(4,2,e,t)},useMemo:function(e,t){var o=lt();t=t===void 0?null:t;var i=e();if(Jo){oe(!0);try{e()}finally{oe(!1)}}return o.memoizedState=[i,t],i},useReducer:function(e,t,o){var i=lt();if(o!==void 0){var n=o(t);if(Jo){oe(!0);try{o(t)}finally{oe(!1)}}}else n=t;return i.memoizedState=i.baseState=n,e={pending:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:n},i.queue=e,e=e.dispatch=_g.bind(null,$,e),[i.memoizedState,e]},useRef:function(e){var t=lt();return e={current:e},t.memoizedState=e},useState:function(e){e=hr(e);var t=e.queue,o=Eu.bind(null,$,t);return t.dispatch=o,[e.memoizedState,o]},useDebugValue:yr,useDeferredValue:function(e,t){var o=lt();return br(o,e,t)},useTransition:function(){var e=hr(!1);return e=Cu.bind(null,$,e.queue,!0,!1),lt().memoizedState=e,[!1,e]},useSyncExternalStore:function(e,t,o){var i=$,n=lt();if(ce){if(o===void 0)throw Error(m(407));o=o()}else{if(o=t(),xe===null)throw Error(m(349));(ae&127)!==0||Xc(i,t,o)}n.memoizedState=o;var a={value:o,getSnapshot:t};return n.queue=a,pu(Jc.bind(null,i,a,e),[e]),i.flags|=2048,zi(9,{destroy:void 0},Zc.bind(null,i,a,o,t),null),o},useId:function(){var e=lt(),t=xe.identifierPrefix;if(ce){var o=Ot,i=Rt;o=(i&~(1<<32-Ye(i)-1)).toString(32)+o,t="_"+t+"R_"+o,o=xa++,0<o&&(t+="H"+o.toString(32)),t+="_"}else o=Rg++,t="_"+t+"r_"+o.toString(32)+"_";return e.memoizedState=t},useHostTransitionStatus:wr,useFormState:ru,useActionState:ru,useOptimistic:function(e){var t=lt();t.memoizedState=t.baseState=e;var o={pending:null,lanes:0,dispatch:null,lastRenderedReducer:null,lastRenderedState:null};return t.queue=o,t=Cr.bind(null,$,!0,o),o.dispatch=t,[e,t]},useMemoCache:dr,useCacheRefresh:function(){return lt().memoizedState=Kg.bind(null,$)},useEffectEvent:function(e){var t=lt(),o={impl:e};return t.memoizedState=o,function(){if((ge&2)!==0)throw Error(m(440));return o.impl.apply(void 0,arguments)}}},Sr={readContext:tt,use:Aa,useCallback:bu,useContext:tt,useEffect:fr,useImperativeHandle:yu,useInsertionEffect:hu,useLayoutEffect:mu,useMemo:vu,useReducer:Ea,useRef:du,useState:function(){return Ea(eo)},useDebugValue:yr,useDeferredValue:function(e,t){var o=Ue();return wu(o,ve.memoizedState,e,t)},useTransition:function(){var e=Ea(eo)[0],t=Ue().memoizedState;return[typeof e=="boolean"?e:kn(e),t]},useSyncExternalStore:_c,useId:ku,useHostTransitionStatus:wr,useFormState:lu,useActionState:lu,useOptimistic:function(e,t){var o=Ue();return tu(o,ve,e,t)},useMemoCache:dr,useCacheRefresh:Au};Sr.useEffectEvent=gu;var qu={readContext:tt,use:Aa,useCallback:bu,useContext:tt,useEffect:fr,useImperativeHandle:yu,useInsertionEffect:hu,useLayoutEffect:mu,useMemo:vu,useReducer:gr,useRef:du,useState:function(){return gr(eo)},useDebugValue:yr,useDeferredValue:function(e,t){var o=Ue();return ve===null?br(o,e,t):wu(o,ve.memoizedState,e,t)},useTransition:function(){var e=gr(eo)[0],t=Ue().memoizedState;return[typeof e=="boolean"?e:kn(e),t]},useSyncExternalStore:_c,useId:ku,useHostTransitionStatus:wr,useFormState:uu,useActionState:uu,useOptimistic:function(e,t){var o=Ue();return ve!==null?tu(o,ve,e,t):(o.baseState=e,[e,o.queue.dispatch])},useMemoCache:dr,useCacheRefresh:Au};qu.useEffectEvent=gu;function xr(e,t,o,i){t=e.memoizedState,o=o(i,t),o=o==null?t:N({},t,o),e.memoizedState=o,e.lanes===0&&(e.updateQueue.baseState=o)}var kr={enqueueSetState:function(e,t,o){e=e._reactInternals;var i=At(),n=wo(i);n.payload=t,o!=null&&(n.callback=o),t=Co(e,n,i),t!==null&&(yt(t,e,i),wn(t,e,i))},enqueueReplaceState:function(e,t,o){e=e._reactInternals;var i=At(),n=wo(i);n.tag=1,n.payload=t,o!=null&&(n.callback=o),t=Co(e,n,i),t!==null&&(yt(t,e,i),wn(t,e,i))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var o=At(),i=wo(o);i.tag=2,t!=null&&(i.callback=t),t=Co(e,i,o),t!==null&&(yt(t,e,o),wn(t,e,o))}};function Iu(e,t,o,i,n,a,s){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(i,a,s):t.prototype&&t.prototype.isPureReactComponent?!pn(o,i)||!pn(n,a):!0}function zu(e,t,o,i){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(o,i),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(o,i),t.state!==e&&kr.enqueueReplaceState(t,t.state,null)}function $o(e,t){var o=t;if("ref"in t){o={};for(var i in t)i!=="ref"&&(o[i]=t[i])}if(e=e.defaultProps){o===t&&(o=N({},o));for(var n in e)o[n]===void 0&&(o[n]=e[n])}return o}function Du(e){ra(e)}function Gu(e){console.error(e)}function ju(e){ra(e)}function qa(e,t){try{var o=e.onUncaughtError;o(t.value,{componentStack:t.stack})}catch(i){setTimeout(function(){throw i})}}function Vu(e,t,o){try{var i=e.onCaughtError;i(o.value,{componentStack:o.stack,errorBoundary:t.tag===1?t.stateNode:null})}catch(n){setTimeout(function(){throw n})}}function Ar(e,t,o){return o=wo(o),o.tag=3,o.payload={element:null},o.callback=function(){qa(e,t)},o}function Yu(e){return e=wo(e),e.tag=3,e}function Lu(e,t,o,i){var n=o.type.getDerivedStateFromError;if(typeof n=="function"){var a=i.value;e.payload=function(){return n(a)},e.callback=function(){Vu(t,o,i)}}var s=o.stateNode;s!==null&&typeof s.componentDidCatch=="function"&&(e.callback=function(){Vu(t,o,i),typeof n!="function"&&(To===null?To=new Set([this]):To.add(this));var r=i.stack;this.componentDidCatch(i.value,{componentStack:r!==null?r:""})})}function Xg(e,t,o,i,n){if(o.flags|=32768,i!==null&&typeof i=="object"&&typeof i.then=="function"){if(t=o.alternate,t!==null&&ki(t,o,n,!0),o=Ct.current,o!==null){switch(o.tag){case 31:case 13:return jt===null?Ra():o.alternate===null&&je===0&&(je=3),o.flags&=-257,o.flags|=65536,o.lanes=n,i===ya?o.flags|=16384:(t=o.updateQueue,t===null?o.updateQueue=new Set([i]):t.add(i),Xr(e,i,n)),!1;case 22:return o.flags|=65536,i===ya?o.flags|=16384:(t=o.updateQueue,t===null?(t={transitions:null,markerInstances:null,retryQueue:new Set([i])},o.updateQueue=t):(o=t.retryQueue,o===null?t.retryQueue=new Set([i]):o.add(i)),Xr(e,i,n)),!1}throw Error(m(435,o.tag))}return Xr(e,i,n),Ra(),!1}if(ce)return t=Ct.current,t!==null?((t.flags&65536)===0&&(t.flags|=256),t.flags|=65536,t.lanes=n,i!==Ws&&(e=Error(m(422),{cause:i}),mn(It(e,o)))):(i!==Ws&&(t=Error(m(423),{cause:i}),mn(It(t,o))),e=e.current.alternate,e.flags|=65536,n&=-n,e.lanes|=n,i=It(i,o),n=Ar(e.stateNode,i,n),tr(e,n),je!==4&&(je=2)),!1;var a=Error(m(520),{cause:i});if(a=It(a,o),Gn===null?Gn=[a]:Gn.push(a),je!==4&&(je=2),t===null)return!0;i=It(i,o),o=t;do{switch(o.tag){case 3:return o.flags|=65536,e=n&-n,o.lanes|=e,e=Ar(o.stateNode,i,e),tr(o,e),!1;case 1:if(t=o.type,a=o.stateNode,(o.flags&128)===0&&(typeof t.getDerivedStateFromError=="function"||a!==null&&typeof a.componentDidCatch=="function"&&(To===null||!To.has(a))))return o.flags|=65536,n&=-n,o.lanes|=n,n=Yu(n),Lu(n,e,o,i),tr(o,n),!1}o=o.return}while(o!==null);return!1}var Er=Error(m(461)),Oe=!1;function ot(e,t,o,i){t.child=e===null?Rc(t,null,o,i):Zo(t,e.child,o,i)}function Uu(e,t,o,i,n){o=o.render;var a=t.ref;if("ref"in i){var s={};for(var r in i)r!=="ref"&&(s[r]=i[r])}else s=i;return Fo(t),i=rr(e,t,o,s,a,n),r=lr(),e!==null&&!Oe?(cr(e,t,n),to(e,t,n)):(ce&&r&&Rs(t),t.flags|=1,ot(e,t,i,n),t.child)}function Nu(e,t,o,i,n){if(e===null){var a=o.type;return typeof a=="function"&&!Us(a)&&a.defaultProps===void 0&&o.compare===null?(t.tag=15,t.type=a,Bu(e,t,a,i,n)):(e=da(o.type,null,i,t,t.mode,n),e.ref=t.ref,e.return=t,t.child=e)}if(a=e.child,!Gr(e,n)){var s=a.memoizedProps;if(o=o.compare,o=o!==null?o:pn,o(s,i)&&e.ref===t.ref)return to(e,t,n)}return t.flags|=1,e=_t(a,i),e.ref=t.ref,e.return=t,t.child=e}function Bu(e,t,o,i,n){if(e!==null){var a=e.memoizedProps;if(pn(a,i)&&e.ref===t.ref)if(Oe=!1,t.pendingProps=i=a,Gr(e,n))(e.flags&131072)!==0&&(Oe=!0);else return t.lanes=e.lanes,to(e,t,n)}return Tr(e,t,o,i,n)}function Ru(e,t,o,i){var n=i.children,a=e!==null?e.memoizedState:null;if(e===null&&t.stateNode===null&&(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),i.mode==="hidden"){if((t.flags&128)!==0){if(a=a!==null?a.baseLanes|o:o,e!==null){for(i=t.child=e.child,n=0;i!==null;)n=n|i.lanes|i.childLanes,i=i.sibling;i=n&~a}else i=0,t.child=null;return Ou(e,t,a,o,i)}if((o&536870912)!==0)t.memoizedState={baseLanes:0,cachePool:null},e!==null&&ma(t,a!==null?a.cachePool:null),a!==null?Qc(t,a):ir(),Hc(t);else return i=t.lanes=536870912,Ou(e,t,a!==null?a.baseLanes|o:o,o,i)}else a!==null?(ma(t,a.cachePool),Qc(t,a),xo(),t.memoizedState=null):(e!==null&&ma(t,null),ir(),xo());return ot(e,t,n,o),t.child}function Tn(e,t){return e!==null&&e.tag===22||t.stateNode!==null||(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),t.sibling}function Ou(e,t,o,i,n){var a=Zs();return a=a===null?null:{parent:Be._currentValue,pool:a},t.memoizedState={baseLanes:o,cachePool:a},e!==null&&ma(t,null),ir(),Hc(t),e!==null&&ki(e,t,i,!0),t.childLanes=n,null}function Ia(e,t){return t=Da({mode:t.mode,children:t.children},e.mode),t.ref=e.ref,e.child=t,t.return=e,t}function Wu(e,t,o){return Zo(t,e.child,null,o),e=Ia(t,t.pendingProps),e.flags|=2,St(t),t.memoizedState=null,e}function Zg(e,t,o){var i=t.pendingProps,n=(t.flags&128)!==0;if(t.flags&=-129,e===null){if(ce){if(i.mode==="hidden")return e=Ia(t,i),t.lanes=536870912,Tn(null,e);if(ar(t),(e=Te)?(e=op(e,Gt),e=e!==null&&e.data==="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:mo!==null?{id:Rt,overflow:Ot}:null,retryLane:536870912,hydrationErrors:null},o=Tc(e),o.return=t,t.child=o,et=t,Te=null)):e=null,e===null)throw yo(t);return t.lanes=536870912,null}return Ia(t,i)}var a=e.memoizedState;if(a!==null){var s=a.dehydrated;if(ar(t),n)if(t.flags&256)t.flags&=-257,t=Wu(e,t,o);else if(t.memoizedState!==null)t.child=e.child,t.flags|=128,t=null;else throw Error(m(558));else if(Oe||ki(e,t,o,!1),n=(o&e.childLanes)!==0,Oe||n){if(i=xe,i!==null&&(s=Gl(i,o),s!==0&&s!==a.retryLane))throw a.retryLane=s,Oo(e,s),yt(i,e,s),Er;Ra(),t=Wu(e,t,o)}else e=a.treeContext,Te=Vt(s.nextSibling),et=t,ce=!0,fo=null,Gt=!1,e!==null&&qc(t,e),t=Ia(t,i),t.flags|=4096;return t}return e=_t(e.child,{mode:i.mode,children:i.children}),e.ref=t.ref,t.child=e,e.return=t,e}function za(e,t){var o=t.ref;if(o===null)e!==null&&e.ref!==null&&(t.flags|=4194816);else{if(typeof o!="function"&&typeof o!="object")throw Error(m(284));(e===null||e.ref!==o)&&(t.flags|=4194816)}}function Tr(e,t,o,i,n){return Fo(t),o=rr(e,t,o,i,void 0,n),i=lr(),e!==null&&!Oe?(cr(e,t,n),to(e,t,n)):(ce&&i&&Rs(t),t.flags|=1,ot(e,t,o,n),t.child)}function Qu(e,t,o,i,n,a){return Fo(t),t.updateQueue=null,o=Kc(t,i,o,n),Fc(e),i=lr(),e!==null&&!Oe?(cr(e,t,a),to(e,t,a)):(ce&&i&&Rs(t),t.flags|=1,ot(e,t,o,a),t.child)}function Hu(e,t,o,i,n){if(Fo(t),t.stateNode===null){var a=wi,s=o.contextType;typeof s=="object"&&s!==null&&(a=tt(s)),a=new o(i,a),t.memoizedState=a.state!==null&&a.state!==void 0?a.state:null,a.updater=kr,t.stateNode=a,a._reactInternals=t,a=t.stateNode,a.props=i,a.state=t.memoizedState,a.refs={},$s(t),s=o.contextType,a.context=typeof s=="object"&&s!==null?tt(s):wi,a.state=t.memoizedState,s=o.getDerivedStateFromProps,typeof s=="function"&&(xr(t,o,s,i),a.state=t.memoizedState),typeof o.getDerivedStateFromProps=="function"||typeof a.getSnapshotBeforeUpdate=="function"||typeof a.UNSAFE_componentWillMount!="function"&&typeof a.componentWillMount!="function"||(s=a.state,typeof a.componentWillMount=="function"&&a.componentWillMount(),typeof a.UNSAFE_componentWillMount=="function"&&a.UNSAFE_componentWillMount(),s!==a.state&&kr.enqueueReplaceState(a,a.state,null),Sn(t,i,a,n),Cn(),a.state=t.memoizedState),typeof a.componentDidMount=="function"&&(t.flags|=4194308),i=!0}else if(e===null){a=t.stateNode;var r=t.memoizedProps,l=$o(o,r);a.props=l;var h=a.context,v=o.contextType;s=wi,typeof v=="object"&&v!==null&&(s=tt(v));var C=o.getDerivedStateFromProps;v=typeof C=="function"||typeof a.getSnapshotBeforeUpdate=="function",r=t.pendingProps!==r,v||typeof a.UNSAFE_componentWillReceiveProps!="function"&&typeof a.componentWillReceiveProps!="function"||(r||h!==s)&&zu(t,a,i,s),vo=!1;var f=t.memoizedState;a.state=f,Sn(t,i,a,n),Cn(),h=t.memoizedState,r||f!==h||vo?(typeof C=="function"&&(xr(t,o,C,i),h=t.memoizedState),(l=vo||Iu(t,o,l,i,f,h,s))?(v||typeof a.UNSAFE_componentWillMount!="function"&&typeof a.componentWillMount!="function"||(typeof a.componentWillMount=="function"&&a.componentWillMount(),typeof a.UNSAFE_componentWillMount=="function"&&a.UNSAFE_componentWillMount()),typeof a.componentDidMount=="function"&&(t.flags|=4194308)):(typeof a.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=i,t.memoizedState=h),a.props=i,a.state=h,a.context=s,i=l):(typeof a.componentDidMount=="function"&&(t.flags|=4194308),i=!1)}else{a=t.stateNode,er(e,t),s=t.memoizedProps,v=$o(o,s),a.props=v,C=t.pendingProps,f=a.context,h=o.contextType,l=wi,typeof h=="object"&&h!==null&&(l=tt(h)),r=o.getDerivedStateFromProps,(h=typeof r=="function"||typeof a.getSnapshotBeforeUpdate=="function")||typeof a.UNSAFE_componentWillReceiveProps!="function"&&typeof a.componentWillReceiveProps!="function"||(s!==C||f!==l)&&zu(t,a,i,l),vo=!1,f=t.memoizedState,a.state=f,Sn(t,i,a,n),Cn();var y=t.memoizedState;s!==C||f!==y||vo||e!==null&&e.dependencies!==null&&ga(e.dependencies)?(typeof r=="function"&&(xr(t,o,r,i),y=t.memoizedState),(v=vo||Iu(t,o,v,i,f,y,l)||e!==null&&e.dependencies!==null&&ga(e.dependencies))?(h||typeof a.UNSAFE_componentWillUpdate!="function"&&typeof a.componentWillUpdate!="function"||(typeof a.componentWillUpdate=="function"&&a.componentWillUpdate(i,y,l),typeof a.UNSAFE_componentWillUpdate=="function"&&a.UNSAFE_componentWillUpdate(i,y,l)),typeof a.componentDidUpdate=="function"&&(t.flags|=4),typeof a.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof a.componentDidUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof a.getSnapshotBeforeUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),t.memoizedProps=i,t.memoizedState=y),a.props=i,a.state=y,a.context=l,i=v):(typeof a.componentDidUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof a.getSnapshotBeforeUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),i=!1)}return a=i,za(e,t),i=(t.flags&128)!==0,a||i?(a=t.stateNode,o=i&&typeof o.getDerivedStateFromError!="function"?null:a.render(),t.flags|=1,e!==null&&i?(t.child=Zo(t,e.child,null,n),t.child=Zo(t,null,o,n)):ot(e,t,o,n),t.memoizedState=a.state,e=t.child):e=to(e,t,n),e}function Fu(e,t,o,i){return Qo(),t.flags|=256,ot(e,t,o,i),t.child}var Mr={dehydrated:null,treeContext:null,retryLane:0,hydrationErrors:null};function Pr(e){return{baseLanes:e,cachePool:Vc()}}function qr(e,t,o){return e=e!==null?e.childLanes&~o:0,t&&(e|=kt),e}function Ku(e,t,o){var i=t.pendingProps,n=!1,a=(t.flags&128)!==0,s;if((s=a)||(s=e!==null&&e.memoizedState===null?!1:(Le.current&2)!==0),s&&(n=!0,t.flags&=-129),s=(t.flags&32)!==0,t.flags&=-33,e===null){if(ce){if(n?So(t):xo(),(e=Te)?(e=op(e,Gt),e=e!==null&&e.data!=="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:mo!==null?{id:Rt,overflow:Ot}:null,retryLane:536870912,hydrationErrors:null},o=Tc(e),o.return=t,t.child=o,et=t,Te=null)):e=null,e===null)throw yo(t);return pl(e)?t.lanes=32:t.lanes=536870912,null}var r=i.children;return i=i.fallback,n?(xo(),n=t.mode,r=Da({mode:"hidden",children:r},n),i=Wo(i,n,o,null),r.return=t,i.return=t,r.sibling=i,t.child=r,i=t.child,i.memoizedState=Pr(o),i.childLanes=qr(e,s,o),t.memoizedState=Mr,Tn(null,i)):(So(t),Ir(t,r))}var l=e.memoizedState;if(l!==null&&(r=l.dehydrated,r!==null)){if(a)t.flags&256?(So(t),t.flags&=-257,t=zr(e,t,o)):t.memoizedState!==null?(xo(),t.child=e.child,t.flags|=128,t=null):(xo(),r=i.fallback,n=t.mode,i=Da({mode:"visible",children:i.children},n),r=Wo(r,n,o,null),r.flags|=2,i.return=t,r.return=t,i.sibling=r,t.child=i,Zo(t,e.child,null,o),i=t.child,i.memoizedState=Pr(o),i.childLanes=qr(e,s,o),t.memoizedState=Mr,t=Tn(null,i));else if(So(t),pl(r)){if(s=r.nextSibling&&r.nextSibling.dataset,s)var h=s.dgst;s=h,i=Error(m(419)),i.stack="",i.digest=s,mn({value:i,source:null,stack:null}),t=zr(e,t,o)}else if(Oe||ki(e,t,o,!1),s=(o&e.childLanes)!==0,Oe||s){if(s=xe,s!==null&&(i=Gl(s,o),i!==0&&i!==l.retryLane))throw l.retryLane=i,Oo(e,i),yt(s,e,i),Er;dl(r)||Ra(),t=zr(e,t,o)}else dl(r)?(t.flags|=192,t.child=e.child,t=null):(e=l.treeContext,Te=Vt(r.nextSibling),et=t,ce=!0,fo=null,Gt=!1,e!==null&&qc(t,e),t=Ir(t,i.children),t.flags|=4096);return t}return n?(xo(),r=i.fallback,n=t.mode,l=e.child,h=l.sibling,i=_t(l,{mode:"hidden",children:i.children}),i.subtreeFlags=l.subtreeFlags&65011712,h!==null?r=_t(h,r):(r=Wo(r,n,o,null),r.flags|=2),r.return=t,i.return=t,i.sibling=r,t.child=i,Tn(null,i),i=t.child,r=e.child.memoizedState,r===null?r=Pr(o):(n=r.cachePool,n!==null?(l=Be._currentValue,n=n.parent!==l?{parent:l,pool:l}:n):n=Vc(),r={baseLanes:r.baseLanes|o,cachePool:n}),i.memoizedState=r,i.childLanes=qr(e,s,o),t.memoizedState=Mr,Tn(e.child,i)):(So(t),o=e.child,e=o.sibling,o=_t(o,{mode:"visible",children:i.children}),o.return=t,o.sibling=null,e!==null&&(s=t.deletions,s===null?(t.deletions=[e],t.flags|=16):s.push(e)),t.child=o,t.memoizedState=null,o)}function Ir(e,t){return t=Da({mode:"visible",children:t},e.mode),t.return=e,e.child=t}function Da(e,t){return e=wt(22,e,null,t),e.lanes=0,e}function zr(e,t,o){return Zo(t,e.child,null,o),e=Ir(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function _u(e,t,o){e.lanes|=t;var i=e.alternate;i!==null&&(i.lanes|=t),Fs(e.return,t,o)}function Dr(e,t,o,i,n,a){var s=e.memoizedState;s===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:i,tail:o,tailMode:n,treeForkCount:a}:(s.isBackwards=t,s.rendering=null,s.renderingStartTime=0,s.last=i,s.tail=o,s.tailMode=n,s.treeForkCount=a)}function Xu(e,t,o){var i=t.pendingProps,n=i.revealOrder,a=i.tail;i=i.children;var s=Le.current,r=(s&2)!==0;if(r?(s=s&1|2,t.flags|=128):s&=1,T(Le,s),ot(e,t,i,o),i=ce?hn:0,!r&&e!==null&&(e.flags&128)!==0)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&_u(e,o,t);else if(e.tag===19)_u(e,o,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}switch(n){case"forwards":for(o=t.child,n=null;o!==null;)e=o.alternate,e!==null&&Ca(e)===null&&(n=o),o=o.sibling;o=n,o===null?(n=t.child,t.child=null):(n=o.sibling,o.sibling=null),Dr(t,!1,n,o,a,i);break;case"backwards":case"unstable_legacy-backwards":for(o=null,n=t.child,t.child=null;n!==null;){if(e=n.alternate,e!==null&&Ca(e)===null){t.child=n;break}e=n.sibling,n.sibling=o,o=n,n=e}Dr(t,!0,o,null,a,i);break;case"together":Dr(t,!1,null,null,void 0,i);break;default:t.memoizedState=null}return t.child}function to(e,t,o){if(e!==null&&(t.dependencies=e.dependencies),Eo|=t.lanes,(o&t.childLanes)===0)if(e!==null){if(ki(e,t,o,!1),(o&t.childLanes)===0)return null}else return null;if(e!==null&&t.child!==e.child)throw Error(m(153));if(t.child!==null){for(e=t.child,o=_t(e,e.pendingProps),t.child=o,o.return=t;e.sibling!==null;)e=e.sibling,o=o.sibling=_t(e,e.pendingProps),o.return=t;o.sibling=null}return t.child}function Gr(e,t){return(e.lanes&t)!==0?!0:(e=e.dependencies,!!(e!==null&&ga(e)))}function Jg(e,t,o){switch(t.tag){case 3:W(t,t.stateNode.containerInfo),bo(t,Be,e.memoizedState.cache),Qo();break;case 27:case 5:_(t);break;case 4:W(t,t.stateNode.containerInfo);break;case 10:bo(t,t.type,t.memoizedProps.value);break;case 31:if(t.memoizedState!==null)return t.flags|=128,ar(t),null;break;case 13:var i=t.memoizedState;if(i!==null)return i.dehydrated!==null?(So(t),t.flags|=128,null):(o&t.child.childLanes)!==0?Ku(e,t,o):(So(t),e=to(e,t,o),e!==null?e.sibling:null);So(t);break;case 19:var n=(e.flags&128)!==0;if(i=(o&t.childLanes)!==0,i||(ki(e,t,o,!1),i=(o&t.childLanes)!==0),n){if(i)return Xu(e,t,o);t.flags|=128}if(n=t.memoizedState,n!==null&&(n.rendering=null,n.tail=null,n.lastEffect=null),T(Le,Le.current),i)break;return null;case 22:return t.lanes=0,Ru(e,t,o,t.pendingProps);case 24:bo(t,Be,e.memoizedState.cache)}return to(e,t,o)}function Zu(e,t,o){if(e!==null)if(e.memoizedProps!==t.pendingProps)Oe=!0;else{if(!Gr(e,o)&&(t.flags&128)===0)return Oe=!1,Jg(e,t,o);Oe=(e.flags&131072)!==0}else Oe=!1,ce&&(t.flags&1048576)!==0&&Pc(t,hn,t.index);switch(t.lanes=0,t.tag){case 16:e:{var i=t.pendingProps;if(e=_o(t.elementType),t.type=e,typeof e=="function")Us(e)?(i=$o(e,i),t.tag=1,t=Hu(null,t,e,i,o)):(t.tag=0,t=Tr(null,t,e,i,o));else{if(e!=null){var n=e.$$typeof;if(n===Qe){t.tag=11,t=Uu(null,t,e,i,o);break e}else if(n===J){t.tag=14,t=Nu(null,t,e,i,o);break e}}throw t=Ke(e)||e,Error(m(306,t,""))}}return t;case 0:return Tr(e,t,t.type,t.pendingProps,o);case 1:return i=t.type,n=$o(i,t.pendingProps),Hu(e,t,i,n,o);case 3:e:{if(W(t,t.stateNode.containerInfo),e===null)throw Error(m(387));i=t.pendingProps;var a=t.memoizedState;n=a.element,er(e,t),Sn(t,i,null,o);var s=t.memoizedState;if(i=s.cache,bo(t,Be,i),i!==a.cache&&Ks(t,[Be],o,!0),Cn(),i=s.element,a.isDehydrated)if(a={element:i,isDehydrated:!1,cache:s.cache},t.updateQueue.baseState=a,t.memoizedState=a,t.flags&256){t=Fu(e,t,i,o);break e}else if(i!==n){n=It(Error(m(424)),t),mn(n),t=Fu(e,t,i,o);break e}else{switch(e=t.stateNode.containerInfo,e.nodeType){case 9:e=e.body;break;default:e=e.nodeName==="HTML"?e.ownerDocument.body:e}for(Te=Vt(e.firstChild),et=t,ce=!0,fo=null,Gt=!0,o=Rc(t,null,i,o),t.child=o;o;)o.flags=o.flags&-3|4096,o=o.sibling}else{if(Qo(),i===n){t=to(e,t,o);break e}ot(e,t,i,o)}t=t.child}return t;case 26:return za(e,t),e===null?(o=lp(t.type,null,t.pendingProps,null))?t.memoizedState=o:ce||(o=t.type,e=t.pendingProps,i=_a(F.current).createElement(o),i[$e]=t,i[dt]=e,it(i,o,e),_e(i),t.stateNode=i):t.memoizedState=lp(t.type,e.memoizedProps,t.pendingProps,e.memoizedState),null;case 27:return _(t),e===null&&ce&&(i=t.stateNode=ap(t.type,t.pendingProps,F.current),et=t,Gt=!0,n=Te,Io(t.type)?(gl=n,Te=Vt(i.firstChild)):Te=n),ot(e,t,t.pendingProps.children,o),za(e,t),e===null&&(t.flags|=4194304),t.child;case 5:return e===null&&ce&&((n=i=Te)&&(i=Mh(i,t.type,t.pendingProps,Gt),i!==null?(t.stateNode=i,et=t,Te=Vt(i.firstChild),Gt=!1,n=!0):n=!1),n||yo(t)),_(t),n=t.type,a=t.pendingProps,s=e!==null?e.memoizedProps:null,i=a.children,ll(n,a)?i=null:s!==null&&ll(n,s)&&(t.flags|=32),t.memoizedState!==null&&(n=rr(e,t,Og,null,null,o),Rn._currentValue=n),za(e,t),ot(e,t,i,o),t.child;case 6:return e===null&&ce&&((e=o=Te)&&(o=Ph(o,t.pendingProps,Gt),o!==null?(t.stateNode=o,et=t,Te=null,e=!0):e=!1),e||yo(t)),null;case 13:return Ku(e,t,o);case 4:return W(t,t.stateNode.containerInfo),i=t.pendingProps,e===null?t.child=Zo(t,null,i,o):ot(e,t,i,o),t.child;case 11:return Uu(e,t,t.type,t.pendingProps,o);case 7:return ot(e,t,t.pendingProps,o),t.child;case 8:return ot(e,t,t.pendingProps.children,o),t.child;case 12:return ot(e,t,t.pendingProps.children,o),t.child;case 10:return i=t.pendingProps,bo(t,t.type,i.value),ot(e,t,i.children,o),t.child;case 9:return n=t.type._context,i=t.pendingProps.children,Fo(t),n=tt(n),i=i(n),t.flags|=1,ot(e,t,i,o),t.child;case 14:return Nu(e,t,t.type,t.pendingProps,o);case 15:return Bu(e,t,t.type,t.pendingProps,o);case 19:return Xu(e,t,o);case 31:return Zg(e,t,o);case 22:return Ru(e,t,o,t.pendingProps);case 24:return Fo(t),i=tt(Be),e===null?(n=Zs(),n===null&&(n=xe,a=_s(),n.pooledCache=a,a.refCount++,a!==null&&(n.pooledCacheLanes|=o),n=a),t.memoizedState={parent:i,cache:n},$s(t),bo(t,Be,n)):((e.lanes&o)!==0&&(er(e,t),Sn(t,null,null,o),Cn()),n=e.memoizedState,a=t.memoizedState,n.parent!==i?(n={parent:i,cache:i},t.memoizedState=n,t.lanes===0&&(t.memoizedState=t.updateQueue.baseState=n),bo(t,Be,i)):(i=a.cache,bo(t,Be,i),i!==n.cache&&Ks(t,[Be],o,!0))),ot(e,t,t.pendingProps.children,o),t.child;case 29:throw t.pendingProps}throw Error(m(156,t.tag))}function oo(e){e.flags|=4}function jr(e,t,o,i,n){if((t=(e.mode&32)!==0)&&(t=!1),t){if(e.flags|=16777216,(n&335544128)===n)if(e.stateNode.complete)e.flags|=8192;else if(kd())e.flags|=8192;else throw Xo=ya,Js}else e.flags&=-16777217}function Ju(e,t){if(t.type!=="stylesheet"||(t.state.loading&4)!==0)e.flags&=-16777217;else if(e.flags|=16777216,!gp(t))if(kd())e.flags|=8192;else throw Xo=ya,Js}function Ga(e,t){t!==null&&(e.flags|=4),e.flags&16384&&(t=e.tag!==22?Il():536870912,e.lanes|=t,Vi|=t)}function Mn(e,t){if(!ce)switch(e.tailMode){case"hidden":t=e.tail;for(var o=null;t!==null;)t.alternate!==null&&(o=t),t=t.sibling;o===null?e.tail=null:o.sibling=null;break;case"collapsed":o=e.tail;for(var i=null;o!==null;)o.alternate!==null&&(i=o),o=o.sibling;i===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:i.sibling=null}}function Me(e){var t=e.alternate!==null&&e.alternate.child===e.child,o=0,i=0;if(t)for(var n=e.child;n!==null;)o|=n.lanes|n.childLanes,i|=n.subtreeFlags&65011712,i|=n.flags&65011712,n.return=e,n=n.sibling;else for(n=e.child;n!==null;)o|=n.lanes|n.childLanes,i|=n.subtreeFlags,i|=n.flags,n.return=e,n=n.sibling;return e.subtreeFlags|=i,e.childLanes=o,t}function $g(e,t,o){var i=t.pendingProps;switch(Os(t),t.tag){case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return Me(t),null;case 1:return Me(t),null;case 3:return o=t.stateNode,i=null,e!==null&&(i=e.memoizedState.cache),t.memoizedState.cache!==i&&(t.flags|=2048),Jt(Be),Ee(),o.pendingContext&&(o.context=o.pendingContext,o.pendingContext=null),(e===null||e.child===null)&&(xi(t)?oo(t):e===null||e.memoizedState.isDehydrated&&(t.flags&256)===0||(t.flags|=1024,Qs())),Me(t),null;case 26:var n=t.type,a=t.memoizedState;return e===null?(oo(t),a!==null?(Me(t),Ju(t,a)):(Me(t),jr(t,n,null,i,o))):a?a!==e.memoizedState?(oo(t),Me(t),Ju(t,a)):(Me(t),t.flags&=-16777217):(e=e.memoizedProps,e!==i&&oo(t),Me(t),jr(t,n,e,i,o)),null;case 27:if(Bt(t),o=F.current,n=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==i&&oo(t);else{if(!i){if(t.stateNode===null)throw Error(m(166));return Me(t),null}e=P.current,xi(t)?Ic(t):(e=ap(n,i,o),t.stateNode=e,oo(t))}return Me(t),null;case 5:if(Bt(t),n=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==i&&oo(t);else{if(!i){if(t.stateNode===null)throw Error(m(166));return Me(t),null}if(a=P.current,xi(t))Ic(t);else{var s=_a(F.current);switch(a){case 1:a=s.createElementNS("http://www.w3.org/2000/svg",n);break;case 2:a=s.createElementNS("http://www.w3.org/1998/Math/MathML",n);break;default:switch(n){case"svg":a=s.createElementNS("http://www.w3.org/2000/svg",n);break;case"math":a=s.createElementNS("http://www.w3.org/1998/Math/MathML",n);break;case"script":a=s.createElement("div"),a.innerHTML="<script><\/script>",a=a.removeChild(a.firstChild);break;case"select":a=typeof i.is=="string"?s.createElement("select",{is:i.is}):s.createElement("select"),i.multiple?a.multiple=!0:i.size&&(a.size=i.size);break;default:a=typeof i.is=="string"?s.createElement(n,{is:i.is}):s.createElement(n)}}a[$e]=t,a[dt]=i;e:for(s=t.child;s!==null;){if(s.tag===5||s.tag===6)a.appendChild(s.stateNode);else if(s.tag!==4&&s.tag!==27&&s.child!==null){s.child.return=s,s=s.child;continue}if(s===t)break e;for(;s.sibling===null;){if(s.return===null||s.return===t)break e;s=s.return}s.sibling.return=s.return,s=s.sibling}t.stateNode=a;e:switch(it(a,n,i),n){case"button":case"input":case"select":case"textarea":i=!!i.autoFocus;break e;case"img":i=!0;break e;default:i=!1}i&&oo(t)}}return Me(t),jr(t,t.type,e===null?null:e.memoizedProps,t.pendingProps,o),null;case 6:if(e&&t.stateNode!=null)e.memoizedProps!==i&&oo(t);else{if(typeof i!="string"&&t.stateNode===null)throw Error(m(166));if(e=F.current,xi(t)){if(e=t.stateNode,o=t.memoizedProps,i=null,n=et,n!==null)switch(n.tag){case 27:case 5:i=n.memoizedProps}e[$e]=t,e=!!(e.nodeValue===o||i!==null&&i.suppressHydrationWarning===!0||Kd(e.nodeValue,o)),e||yo(t,!0)}else e=_a(e).createTextNode(i),e[$e]=t,t.stateNode=e}return Me(t),null;case 31:if(o=t.memoizedState,e===null||e.memoizedState!==null){if(i=xi(t),o!==null){if(e===null){if(!i)throw Error(m(318));if(e=t.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(m(557));e[$e]=t}else Qo(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;Me(t),e=!1}else o=Qs(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=o),e=!0;if(!e)return t.flags&256?(St(t),t):(St(t),null);if((t.flags&128)!==0)throw Error(m(558))}return Me(t),null;case 13:if(i=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(n=xi(t),i!==null&&i.dehydrated!==null){if(e===null){if(!n)throw Error(m(318));if(n=t.memoizedState,n=n!==null?n.dehydrated:null,!n)throw Error(m(317));n[$e]=t}else Qo(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;Me(t),n=!1}else n=Qs(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=n),n=!0;if(!n)return t.flags&256?(St(t),t):(St(t),null)}return St(t),(t.flags&128)!==0?(t.lanes=o,t):(o=i!==null,e=e!==null&&e.memoizedState!==null,o&&(i=t.child,n=null,i.alternate!==null&&i.alternate.memoizedState!==null&&i.alternate.memoizedState.cachePool!==null&&(n=i.alternate.memoizedState.cachePool.pool),a=null,i.memoizedState!==null&&i.memoizedState.cachePool!==null&&(a=i.memoizedState.cachePool.pool),a!==n&&(i.flags|=2048)),o!==e&&o&&(t.child.flags|=8192),Ga(t,t.updateQueue),Me(t),null);case 4:return Ee(),e===null&&il(t.stateNode.containerInfo),Me(t),null;case 10:return Jt(t.type),Me(t),null;case 19:if(S(Le),i=t.memoizedState,i===null)return Me(t),null;if(n=(t.flags&128)!==0,a=i.rendering,a===null)if(n)Mn(i,!1);else{if(je!==0||e!==null&&(e.flags&128)!==0)for(e=t.child;e!==null;){if(a=Ca(e),a!==null){for(t.flags|=128,Mn(i,!1),e=a.updateQueue,t.updateQueue=e,Ga(t,e),t.subtreeFlags=0,e=o,o=t.child;o!==null;)Ec(o,e),o=o.sibling;return T(Le,Le.current&1|2),ce&&Xt(t,i.treeForkCount),t.child}e=e.sibling}i.tail!==null&&rt()>Ua&&(t.flags|=128,n=!0,Mn(i,!1),t.lanes=4194304)}else{if(!n)if(e=Ca(a),e!==null){if(t.flags|=128,n=!0,e=e.updateQueue,t.updateQueue=e,Ga(t,e),Mn(i,!0),i.tail===null&&i.tailMode==="hidden"&&!a.alternate&&!ce)return Me(t),null}else 2*rt()-i.renderingStartTime>Ua&&o!==536870912&&(t.flags|=128,n=!0,Mn(i,!1),t.lanes=4194304);i.isBackwards?(a.sibling=t.child,t.child=a):(e=i.last,e!==null?e.sibling=a:t.child=a,i.last=a)}return i.tail!==null?(e=i.tail,i.rendering=e,i.tail=e.sibling,i.renderingStartTime=rt(),e.sibling=null,o=Le.current,T(Le,n?o&1|2:o&1),ce&&Xt(t,i.treeForkCount),e):(Me(t),null);case 22:case 23:return St(t),nr(),i=t.memoizedState!==null,e!==null?e.memoizedState!==null!==i&&(t.flags|=8192):i&&(t.flags|=8192),i?(o&536870912)!==0&&(t.flags&128)===0&&(Me(t),t.subtreeFlags&6&&(t.flags|=8192)):Me(t),o=t.updateQueue,o!==null&&Ga(t,o.retryQueue),o=null,e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(o=e.memoizedState.cachePool.pool),i=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(i=t.memoizedState.cachePool.pool),i!==o&&(t.flags|=2048),e!==null&&S(Ko),null;case 24:return o=null,e!==null&&(o=e.memoizedState.cache),t.memoizedState.cache!==o&&(t.flags|=2048),Jt(Be),Me(t),null;case 25:return null;case 30:return null}throw Error(m(156,t.tag))}function eh(e,t){switch(Os(t),t.tag){case 1:return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return Jt(Be),Ee(),e=t.flags,(e&65536)!==0&&(e&128)===0?(t.flags=e&-65537|128,t):null;case 26:case 27:case 5:return Bt(t),null;case 31:if(t.memoizedState!==null){if(St(t),t.alternate===null)throw Error(m(340));Qo()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 13:if(St(t),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(m(340));Qo()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return S(Le),null;case 4:return Ee(),null;case 10:return Jt(t.type),null;case 22:case 23:return St(t),nr(),e!==null&&S(Ko),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 24:return Jt(Be),null;case 25:return null;default:return null}}function $u(e,t){switch(Os(t),t.tag){case 3:Jt(Be),Ee();break;case 26:case 27:case 5:Bt(t);break;case 4:Ee();break;case 31:t.memoizedState!==null&&St(t);break;case 13:St(t);break;case 19:S(Le);break;case 10:Jt(t.type);break;case 22:case 23:St(t),nr(),e!==null&&S(Ko);break;case 24:Jt(Be)}}function Pn(e,t){try{var o=t.updateQueue,i=o!==null?o.lastEffect:null;if(i!==null){var n=i.next;o=n;do{if((o.tag&e)===e){i=void 0;var a=o.create,s=o.inst;i=a(),s.destroy=i}o=o.next}while(o!==n)}}catch(r){ye(t,t.return,r)}}function ko(e,t,o){try{var i=t.updateQueue,n=i!==null?i.lastEffect:null;if(n!==null){var a=n.next;i=a;do{if((i.tag&e)===e){var s=i.inst,r=s.destroy;if(r!==void 0){s.destroy=void 0,n=t;var l=o,h=r;try{h()}catch(v){ye(n,l,v)}}}i=i.next}while(i!==a)}}catch(v){ye(t,t.return,v)}}function ed(e){var t=e.updateQueue;if(t!==null){var o=e.stateNode;try{Wc(t,o)}catch(i){ye(e,e.return,i)}}}function td(e,t,o){o.props=$o(e.type,e.memoizedProps),o.state=e.memoizedState;try{o.componentWillUnmount()}catch(i){ye(e,t,i)}}function qn(e,t){try{var o=e.ref;if(o!==null){switch(e.tag){case 26:case 27:case 5:var i=e.stateNode;break;case 30:i=e.stateNode;break;default:i=e.stateNode}typeof o=="function"?e.refCleanup=o(i):o.current=i}}catch(n){ye(e,t,n)}}function Wt(e,t){var o=e.ref,i=e.refCleanup;if(o!==null)if(typeof i=="function")try{i()}catch(n){ye(e,t,n)}finally{e.refCleanup=null,e=e.alternate,e!=null&&(e.refCleanup=null)}else if(typeof o=="function")try{o(null)}catch(n){ye(e,t,n)}else o.current=null}function od(e){var t=e.type,o=e.memoizedProps,i=e.stateNode;try{e:switch(t){case"button":case"input":case"select":case"textarea":o.autoFocus&&i.focus();break e;case"img":o.src?i.src=o.src:o.srcSet&&(i.srcset=o.srcSet)}}catch(n){ye(e,e.return,n)}}function Vr(e,t,o){try{var i=e.stateNode;Sh(i,e.type,o,t),i[dt]=t}catch(n){ye(e,e.return,n)}}function id(e){return e.tag===5||e.tag===3||e.tag===26||e.tag===27&&Io(e.type)||e.tag===4}function Yr(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||id(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.tag===27&&Io(e.type)||e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function Lr(e,t,o){var i=e.tag;if(i===5||i===6)e=e.stateNode,t?(o.nodeType===9?o.body:o.nodeName==="HTML"?o.ownerDocument.body:o).insertBefore(e,t):(t=o.nodeType===9?o.body:o.nodeName==="HTML"?o.ownerDocument.body:o,t.appendChild(e),o=o._reactRootContainer,o!=null||t.onclick!==null||(t.onclick=Ft));else if(i!==4&&(i===27&&Io(e.type)&&(o=e.stateNode,t=null),e=e.child,e!==null))for(Lr(e,t,o),e=e.sibling;e!==null;)Lr(e,t,o),e=e.sibling}function ja(e,t,o){var i=e.tag;if(i===5||i===6)e=e.stateNode,t?o.insertBefore(e,t):o.appendChild(e);else if(i!==4&&(i===27&&Io(e.type)&&(o=e.stateNode),e=e.child,e!==null))for(ja(e,t,o),e=e.sibling;e!==null;)ja(e,t,o),e=e.sibling}function nd(e){var t=e.stateNode,o=e.memoizedProps;try{for(var i=e.type,n=t.attributes;n.length;)t.removeAttributeNode(n[0]);it(t,i,o),t[$e]=e,t[dt]=o}catch(a){ye(e,e.return,a)}}var io=!1,We=!1,Ur=!1,ad=typeof WeakSet=="function"?WeakSet:Set,Xe=null;function th(e,t){if(e=e.containerInfo,sl=os,e=yc(e),zs(e)){if("selectionStart"in e)var o={start:e.selectionStart,end:e.selectionEnd};else e:{o=(o=e.ownerDocument)&&o.defaultView||window;var i=o.getSelection&&o.getSelection();if(i&&i.rangeCount!==0){o=i.anchorNode;var n=i.anchorOffset,a=i.focusNode;i=i.focusOffset;try{o.nodeType,a.nodeType}catch{o=null;break e}var s=0,r=-1,l=-1,h=0,v=0,C=e,f=null;t:for(;;){for(var y;C!==o||n!==0&&C.nodeType!==3||(r=s+n),C!==a||i!==0&&C.nodeType!==3||(l=s+i),C.nodeType===3&&(s+=C.nodeValue.length),(y=C.firstChild)!==null;)f=C,C=y;for(;;){if(C===e)break t;if(f===o&&++h===n&&(r=s),f===a&&++v===i&&(l=s),(y=C.nextSibling)!==null)break;C=f,f=C.parentNode}C=y}o=r===-1||l===-1?null:{start:r,end:l}}else o=null}o=o||{start:0,end:0}}else o=null;for(rl={focusedElem:e,selectionRange:o},os=!1,Xe=t;Xe!==null;)if(t=Xe,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,Xe=e;else for(;Xe!==null;){switch(t=Xe,a=t.alternate,e=t.flags,t.tag){case 0:if((e&4)!==0&&(e=t.updateQueue,e=e!==null?e.events:null,e!==null))for(o=0;o<e.length;o++)n=e[o],n.ref.impl=n.nextImpl;break;case 11:case 15:break;case 1:if((e&1024)!==0&&a!==null){e=void 0,o=t,n=a.memoizedProps,a=a.memoizedState,i=o.stateNode;try{var z=$o(o.type,n);e=i.getSnapshotBeforeUpdate(z,a),i.__reactInternalSnapshotBeforeUpdate=e}catch(B){ye(o,o.return,B)}}break;case 3:if((e&1024)!==0){if(e=t.stateNode.containerInfo,o=e.nodeType,o===9)ul(e);else if(o===1)switch(e.nodeName){case"HEAD":case"HTML":case"BODY":ul(e);break;default:e.textContent=""}}break;case 5:case 26:case 27:case 6:case 4:case 17:break;default:if((e&1024)!==0)throw Error(m(163))}if(e=t.sibling,e!==null){e.return=t.return,Xe=e;break}Xe=t.return}}function sd(e,t,o){var i=o.flags;switch(o.tag){case 0:case 11:case 15:ao(e,o),i&4&&Pn(5,o);break;case 1:if(ao(e,o),i&4)if(e=o.stateNode,t===null)try{e.componentDidMount()}catch(s){ye(o,o.return,s)}else{var n=$o(o.type,t.memoizedProps);t=t.memoizedState;try{e.componentDidUpdate(n,t,e.__reactInternalSnapshotBeforeUpdate)}catch(s){ye(o,o.return,s)}}i&64&&ed(o),i&512&&qn(o,o.return);break;case 3:if(ao(e,o),i&64&&(e=o.updateQueue,e!==null)){if(t=null,o.child!==null)switch(o.child.tag){case 27:case 5:t=o.child.stateNode;break;case 1:t=o.child.stateNode}try{Wc(e,t)}catch(s){ye(o,o.return,s)}}break;case 27:t===null&&i&4&&nd(o);case 26:case 5:ao(e,o),t===null&&i&4&&od(o),i&512&&qn(o,o.return);break;case 12:ao(e,o);break;case 31:ao(e,o),i&4&&cd(e,o);break;case 13:ao(e,o),i&4&&ud(e,o),i&64&&(e=o.memoizedState,e!==null&&(e=e.dehydrated,e!==null&&(o=uh.bind(null,o),qh(e,o))));break;case 22:if(i=o.memoizedState!==null||io,!i){t=t!==null&&t.memoizedState!==null||We,n=io;var a=We;io=i,(We=t)&&!a?so(e,o,(o.subtreeFlags&8772)!==0):ao(e,o),io=n,We=a}break;case 30:break;default:ao(e,o)}}function rd(e){var t=e.alternate;t!==null&&(e.alternate=null,rd(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&ms(t)),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}var qe=null,gt=!1;function no(e,t,o){for(o=o.child;o!==null;)ld(e,t,o),o=o.sibling}function ld(e,t,o){if(Q&&typeof Q.onCommitFiberUnmount=="function")try{Q.onCommitFiberUnmount(q,o)}catch{}switch(o.tag){case 26:We||Wt(o,t),no(e,t,o),o.memoizedState?o.memoizedState.count--:o.stateNode&&(o=o.stateNode,o.parentNode.removeChild(o));break;case 27:We||Wt(o,t);var i=qe,n=gt;Io(o.type)&&(qe=o.stateNode,gt=!1),no(e,t,o),Un(o.stateNode),qe=i,gt=n;break;case 5:We||Wt(o,t);case 6:if(i=qe,n=gt,qe=null,no(e,t,o),qe=i,gt=n,qe!==null)if(gt)try{(qe.nodeType===9?qe.body:qe.nodeName==="HTML"?qe.ownerDocument.body:qe).removeChild(o.stateNode)}catch(a){ye(o,t,a)}else try{qe.removeChild(o.stateNode)}catch(a){ye(o,t,a)}break;case 18:qe!==null&&(gt?(e=qe,ep(e.nodeType===9?e.body:e.nodeName==="HTML"?e.ownerDocument.body:e,o.stateNode),Wi(e)):ep(qe,o.stateNode));break;case 4:i=qe,n=gt,qe=o.stateNode.containerInfo,gt=!0,no(e,t,o),qe=i,gt=n;break;case 0:case 11:case 14:case 15:ko(2,o,t),We||ko(4,o,t),no(e,t,o);break;case 1:We||(Wt(o,t),i=o.stateNode,typeof i.componentWillUnmount=="function"&&td(o,t,i)),no(e,t,o);break;case 21:no(e,t,o);break;case 22:We=(i=We)||o.memoizedState!==null,no(e,t,o),We=i;break;default:no(e,t,o)}}function cd(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null))){e=e.dehydrated;try{Wi(e)}catch(o){ye(t,t.return,o)}}}function ud(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null&&(e=e.dehydrated,e!==null))))try{Wi(e)}catch(o){ye(t,t.return,o)}}function oh(e){switch(e.tag){case 31:case 13:case 19:var t=e.stateNode;return t===null&&(t=e.stateNode=new ad),t;case 22:return e=e.stateNode,t=e._retryCache,t===null&&(t=e._retryCache=new ad),t;default:throw Error(m(435,e.tag))}}function Va(e,t){var o=oh(e);t.forEach(function(i){if(!o.has(i)){o.add(i);var n=dh.bind(null,e,i);i.then(n,n)}})}function ht(e,t){var o=t.deletions;if(o!==null)for(var i=0;i<o.length;i++){var n=o[i],a=e,s=t,r=s;e:for(;r!==null;){switch(r.tag){case 27:if(Io(r.type)){qe=r.stateNode,gt=!1;break e}break;case 5:qe=r.stateNode,gt=!1;break e;case 3:case 4:qe=r.stateNode.containerInfo,gt=!0;break e}r=r.return}if(qe===null)throw Error(m(160));ld(a,s,n),qe=null,gt=!1,a=n.alternate,a!==null&&(a.return=null),n.return=null}if(t.subtreeFlags&13886)for(t=t.child;t!==null;)dd(t,e),t=t.sibling}var Ut=null;function dd(e,t){var o=e.alternate,i=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:ht(t,e),mt(e),i&4&&(ko(3,e,e.return),Pn(3,e),ko(5,e,e.return));break;case 1:ht(t,e),mt(e),i&512&&(We||o===null||Wt(o,o.return)),i&64&&io&&(e=e.updateQueue,e!==null&&(i=e.callbacks,i!==null&&(o=e.shared.hiddenCallbacks,e.shared.hiddenCallbacks=o===null?i:o.concat(i))));break;case 26:var n=Ut;if(ht(t,e),mt(e),i&512&&(We||o===null||Wt(o,o.return)),i&4){var a=o!==null?o.memoizedState:null;if(i=e.memoizedState,o===null)if(i===null)if(e.stateNode===null){e:{i=e.type,o=e.memoizedProps,n=n.ownerDocument||n;t:switch(i){case"title":a=n.getElementsByTagName("title")[0],(!a||a[on]||a[$e]||a.namespaceURI==="http://www.w3.org/2000/svg"||a.hasAttribute("itemprop"))&&(a=n.createElement(i),n.head.insertBefore(a,n.querySelector("head > title"))),it(a,i,o),a[$e]=e,_e(a),i=a;break e;case"link":var s=dp("link","href",n).get(i+(o.href||""));if(s){for(var r=0;r<s.length;r++)if(a=s[r],a.getAttribute("href")===(o.href==null||o.href===""?null:o.href)&&a.getAttribute("rel")===(o.rel==null?null:o.rel)&&a.getAttribute("title")===(o.title==null?null:o.title)&&a.getAttribute("crossorigin")===(o.crossOrigin==null?null:o.crossOrigin)){s.splice(r,1);break t}}a=n.createElement(i),it(a,i,o),n.head.appendChild(a);break;case"meta":if(s=dp("meta","content",n).get(i+(o.content||""))){for(r=0;r<s.length;r++)if(a=s[r],a.getAttribute("content")===(o.content==null?null:""+o.content)&&a.getAttribute("name")===(o.name==null?null:o.name)&&a.getAttribute("property")===(o.property==null?null:o.property)&&a.getAttribute("http-equiv")===(o.httpEquiv==null?null:o.httpEquiv)&&a.getAttribute("charset")===(o.charSet==null?null:o.charSet)){s.splice(r,1);break t}}a=n.createElement(i),it(a,i,o),n.head.appendChild(a);break;default:throw Error(m(468,i))}a[$e]=e,_e(a),i=a}e.stateNode=i}else pp(n,e.type,e.stateNode);else e.stateNode=up(n,i,e.memoizedProps);else a!==i?(a===null?o.stateNode!==null&&(o=o.stateNode,o.parentNode.removeChild(o)):a.count--,i===null?pp(n,e.type,e.stateNode):up(n,i,e.memoizedProps)):i===null&&e.stateNode!==null&&Vr(e,e.memoizedProps,o.memoizedProps)}break;case 27:ht(t,e),mt(e),i&512&&(We||o===null||Wt(o,o.return)),o!==null&&i&4&&Vr(e,e.memoizedProps,o.memoizedProps);break;case 5:if(ht(t,e),mt(e),i&512&&(We||o===null||Wt(o,o.return)),e.flags&32){n=e.stateNode;try{gi(n,"")}catch(z){ye(e,e.return,z)}}i&4&&e.stateNode!=null&&(n=e.memoizedProps,Vr(e,n,o!==null?o.memoizedProps:n)),i&1024&&(Ur=!0);break;case 6:if(ht(t,e),mt(e),i&4){if(e.stateNode===null)throw Error(m(162));i=e.memoizedProps,o=e.stateNode;try{o.nodeValue=i}catch(z){ye(e,e.return,z)}}break;case 3:if(Ja=null,n=Ut,Ut=Xa(t.containerInfo),ht(t,e),Ut=n,mt(e),i&4&&o!==null&&o.memoizedState.isDehydrated)try{Wi(t.containerInfo)}catch(z){ye(e,e.return,z)}Ur&&(Ur=!1,pd(e));break;case 4:i=Ut,Ut=Xa(e.stateNode.containerInfo),ht(t,e),mt(e),Ut=i;break;case 12:ht(t,e),mt(e);break;case 31:ht(t,e),mt(e),i&4&&(i=e.updateQueue,i!==null&&(e.updateQueue=null,Va(e,i)));break;case 13:ht(t,e),mt(e),e.child.flags&8192&&e.memoizedState!==null!=(o!==null&&o.memoizedState!==null)&&(La=rt()),i&4&&(i=e.updateQueue,i!==null&&(e.updateQueue=null,Va(e,i)));break;case 22:n=e.memoizedState!==null;var l=o!==null&&o.memoizedState!==null,h=io,v=We;if(io=h||n,We=v||l,ht(t,e),We=v,io=h,mt(e),i&8192)e:for(t=e.stateNode,t._visibility=n?t._visibility&-2:t._visibility|1,n&&(o===null||l||io||We||ei(e)),o=null,t=e;;){if(t.tag===5||t.tag===26){if(o===null){l=o=t;try{if(a=l.stateNode,n)s=a.style,typeof s.setProperty=="function"?s.setProperty("display","none","important"):s.display="none";else{r=l.stateNode;var C=l.memoizedProps.style,f=C!=null&&C.hasOwnProperty("display")?C.display:null;r.style.display=f==null||typeof f=="boolean"?"":(""+f).trim()}}catch(z){ye(l,l.return,z)}}}else if(t.tag===6){if(o===null){l=t;try{l.stateNode.nodeValue=n?"":l.memoizedProps}catch(z){ye(l,l.return,z)}}}else if(t.tag===18){if(o===null){l=t;try{var y=l.stateNode;n?tp(y,!0):tp(l.stateNode,!1)}catch(z){ye(l,l.return,z)}}}else if((t.tag!==22&&t.tag!==23||t.memoizedState===null||t===e)&&t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break e;for(;t.sibling===null;){if(t.return===null||t.return===e)break e;o===t&&(o=null),t=t.return}o===t&&(o=null),t.sibling.return=t.return,t=t.sibling}i&4&&(i=e.updateQueue,i!==null&&(o=i.retryQueue,o!==null&&(i.retryQueue=null,Va(e,o))));break;case 19:ht(t,e),mt(e),i&4&&(i=e.updateQueue,i!==null&&(e.updateQueue=null,Va(e,i)));break;case 30:break;case 21:break;default:ht(t,e),mt(e)}}function mt(e){var t=e.flags;if(t&2){try{for(var o,i=e.return;i!==null;){if(id(i)){o=i;break}i=i.return}if(o==null)throw Error(m(160));switch(o.tag){case 27:var n=o.stateNode,a=Yr(e);ja(e,a,n);break;case 5:var s=o.stateNode;o.flags&32&&(gi(s,""),o.flags&=-33);var r=Yr(e);ja(e,r,s);break;case 3:case 4:var l=o.stateNode.containerInfo,h=Yr(e);Lr(e,h,l);break;default:throw Error(m(161))}}catch(v){ye(e,e.return,v)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function pd(e){if(e.subtreeFlags&1024)for(e=e.child;e!==null;){var t=e;pd(t),t.tag===5&&t.flags&1024&&t.stateNode.reset(),e=e.sibling}}function ao(e,t){if(t.subtreeFlags&8772)for(t=t.child;t!==null;)sd(e,t.alternate,t),t=t.sibling}function ei(e){for(e=e.child;e!==null;){var t=e;switch(t.tag){case 0:case 11:case 14:case 15:ko(4,t,t.return),ei(t);break;case 1:Wt(t,t.return);var o=t.stateNode;typeof o.componentWillUnmount=="function"&&td(t,t.return,o),ei(t);break;case 27:Un(t.stateNode);case 26:case 5:Wt(t,t.return),ei(t);break;case 22:t.memoizedState===null&&ei(t);break;case 30:ei(t);break;default:ei(t)}e=e.sibling}}function so(e,t,o){for(o=o&&(t.subtreeFlags&8772)!==0,t=t.child;t!==null;){var i=t.alternate,n=e,a=t,s=a.flags;switch(a.tag){case 0:case 11:case 15:so(n,a,o),Pn(4,a);break;case 1:if(so(n,a,o),i=a,n=i.stateNode,typeof n.componentDidMount=="function")try{n.componentDidMount()}catch(h){ye(i,i.return,h)}if(i=a,n=i.updateQueue,n!==null){var r=i.stateNode;try{var l=n.shared.hiddenCallbacks;if(l!==null)for(n.shared.hiddenCallbacks=null,n=0;n<l.length;n++)Oc(l[n],r)}catch(h){ye(i,i.return,h)}}o&&s&64&&ed(a),qn(a,a.return);break;case 27:nd(a);case 26:case 5:so(n,a,o),o&&i===null&&s&4&&od(a),qn(a,a.return);break;case 12:so(n,a,o);break;case 31:so(n,a,o),o&&s&4&&cd(n,a);break;case 13:so(n,a,o),o&&s&4&&ud(n,a);break;case 22:a.memoizedState===null&&so(n,a,o),qn(a,a.return);break;case 30:break;default:so(n,a,o)}t=t.sibling}}function Nr(e,t){var o=null;e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(o=e.memoizedState.cachePool.pool),e=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(e=t.memoizedState.cachePool.pool),e!==o&&(e!=null&&e.refCount++,o!=null&&fn(o))}function Br(e,t){e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&fn(e))}function Nt(e,t,o,i){if(t.subtreeFlags&10256)for(t=t.child;t!==null;)gd(e,t,o,i),t=t.sibling}function gd(e,t,o,i){var n=t.flags;switch(t.tag){case 0:case 11:case 15:Nt(e,t,o,i),n&2048&&Pn(9,t);break;case 1:Nt(e,t,o,i);break;case 3:Nt(e,t,o,i),n&2048&&(e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&fn(e)));break;case 12:if(n&2048){Nt(e,t,o,i),e=t.stateNode;try{var a=t.memoizedProps,s=a.id,r=a.onPostCommit;typeof r=="function"&&r(s,t.alternate===null?"mount":"update",e.passiveEffectDuration,-0)}catch(l){ye(t,t.return,l)}}else Nt(e,t,o,i);break;case 31:Nt(e,t,o,i);break;case 13:Nt(e,t,o,i);break;case 23:break;case 22:a=t.stateNode,s=t.alternate,t.memoizedState!==null?a._visibility&2?Nt(e,t,o,i):In(e,t):a._visibility&2?Nt(e,t,o,i):(a._visibility|=2,Di(e,t,o,i,(t.subtreeFlags&10256)!==0||!1)),n&2048&&Nr(s,t);break;case 24:Nt(e,t,o,i),n&2048&&Br(t.alternate,t);break;default:Nt(e,t,o,i)}}function Di(e,t,o,i,n){for(n=n&&((t.subtreeFlags&10256)!==0||!1),t=t.child;t!==null;){var a=e,s=t,r=o,l=i,h=s.flags;switch(s.tag){case 0:case 11:case 15:Di(a,s,r,l,n),Pn(8,s);break;case 23:break;case 22:var v=s.stateNode;s.memoizedState!==null?v._visibility&2?Di(a,s,r,l,n):In(a,s):(v._visibility|=2,Di(a,s,r,l,n)),n&&h&2048&&Nr(s.alternate,s);break;case 24:Di(a,s,r,l,n),n&&h&2048&&Br(s.alternate,s);break;default:Di(a,s,r,l,n)}t=t.sibling}}function In(e,t){if(t.subtreeFlags&10256)for(t=t.child;t!==null;){var o=e,i=t,n=i.flags;switch(i.tag){case 22:In(o,i),n&2048&&Nr(i.alternate,i);break;case 24:In(o,i),n&2048&&Br(i.alternate,i);break;default:In(o,i)}t=t.sibling}}var zn=8192;function Gi(e,t,o){if(e.subtreeFlags&zn)for(e=e.child;e!==null;)hd(e,t,o),e=e.sibling}function hd(e,t,o){switch(e.tag){case 26:Gi(e,t,o),e.flags&zn&&e.memoizedState!==null&&Rh(o,Ut,e.memoizedState,e.memoizedProps);break;case 5:Gi(e,t,o);break;case 3:case 4:var i=Ut;Ut=Xa(e.stateNode.containerInfo),Gi(e,t,o),Ut=i;break;case 22:e.memoizedState===null&&(i=e.alternate,i!==null&&i.memoizedState!==null?(i=zn,zn=16777216,Gi(e,t,o),zn=i):Gi(e,t,o));break;default:Gi(e,t,o)}}function md(e){var t=e.alternate;if(t!==null&&(e=t.child,e!==null)){t.child=null;do t=e.sibling,e.sibling=null,e=t;while(e!==null)}}function Dn(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var o=0;o<t.length;o++){var i=t[o];Xe=i,yd(i,e)}md(e)}if(e.subtreeFlags&10256)for(e=e.child;e!==null;)fd(e),e=e.sibling}function fd(e){switch(e.tag){case 0:case 11:case 15:Dn(e),e.flags&2048&&ko(9,e,e.return);break;case 3:Dn(e);break;case 12:Dn(e);break;case 22:var t=e.stateNode;e.memoizedState!==null&&t._visibility&2&&(e.return===null||e.return.tag!==13)?(t._visibility&=-3,Ya(e)):Dn(e);break;default:Dn(e)}}function Ya(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var o=0;o<t.length;o++){var i=t[o];Xe=i,yd(i,e)}md(e)}for(e=e.child;e!==null;){switch(t=e,t.tag){case 0:case 11:case 15:ko(8,t,t.return),Ya(t);break;case 22:o=t.stateNode,o._visibility&2&&(o._visibility&=-3,Ya(t));break;default:Ya(t)}e=e.sibling}}function yd(e,t){for(;Xe!==null;){var o=Xe;switch(o.tag){case 0:case 11:case 15:ko(8,o,t);break;case 23:case 22:if(o.memoizedState!==null&&o.memoizedState.cachePool!==null){var i=o.memoizedState.cachePool.pool;i!=null&&i.refCount++}break;case 24:fn(o.memoizedState.cache)}if(i=o.child,i!==null)i.return=o,Xe=i;else e:for(o=e;Xe!==null;){i=Xe;var n=i.sibling,a=i.return;if(rd(i),i===o){Xe=null;break e}if(n!==null){n.return=a,Xe=n;break e}Xe=a}}}var ih={getCacheForType:function(e){var t=tt(Be),o=t.data.get(e);return o===void 0&&(o=e(),t.data.set(e,o)),o},cacheSignal:function(){return tt(Be).controller.signal}},nh=typeof WeakMap=="function"?WeakMap:Map,ge=0,xe=null,ie=null,ae=0,fe=0,xt=null,Ao=!1,ji=!1,Rr=!1,ro=0,je=0,Eo=0,ti=0,Or=0,kt=0,Vi=0,Gn=null,ft=null,Wr=!1,La=0,bd=0,Ua=1/0,Na=null,To=null,He=0,Mo=null,Yi=null,lo=0,Qr=0,Hr=null,vd=null,jn=0,Fr=null;function At(){return(ge&2)!==0&&ae!==0?ae&-ae:b.T!==null?$r():jl()}function wd(){if(kt===0)if((ae&536870912)===0||ce){var e=po;po<<=1,(po&3932160)===0&&(po=262144),kt=e}else kt=536870912;return e=Ct.current,e!==null&&(e.flags|=32),kt}function yt(e,t,o){(e===xe&&(fe===2||fe===9)||e.cancelPendingCommit!==null)&&(Li(e,0),Po(e,ae,kt,!1)),tn(e,o),((ge&2)===0||e!==xe)&&(e===xe&&((ge&2)===0&&(ti|=o),je===4&&Po(e,ae,kt,!1)),Qt(e))}function Cd(e,t,o){if((ge&6)!==0)throw Error(m(327));var i=!o&&(t&127)===0&&(t&e.expiredLanes)===0||en(e,t),n=i?rh(e,t):_r(e,t,!0),a=i;do{if(n===0){ji&&!i&&Po(e,t,0,!1);break}else{if(o=e.current.alternate,a&&!ah(o)){n=_r(e,t,!1),a=!1;continue}if(n===2){if(a=t,e.errorRecoveryDisabledLanes&a)var s=0;else s=e.pendingLanes&-536870913,s=s!==0?s:s&536870912?536870912:0;if(s!==0){t=s;e:{var r=e;n=Gn;var l=r.current.memoizedState.isDehydrated;if(l&&(Li(r,s).flags|=256),s=_r(r,s,!1),s!==2){if(Rr&&!l){r.errorRecoveryDisabledLanes|=a,ti|=a,n=4;break e}a=ft,ft=n,a!==null&&(ft===null?ft=a:ft.push.apply(ft,a))}n=s}if(a=!1,n!==2)continue}}if(n===1){Li(e,0),Po(e,t,0,!0);break}e:{switch(i=e,a=n,a){case 0:case 1:throw Error(m(345));case 4:if((t&4194048)!==t)break;case 6:Po(i,t,kt,!Ao);break e;case 2:ft=null;break;case 3:case 5:break;default:throw Error(m(329))}if((t&62914560)===t&&(n=La+300-rt(),10<n)){if(Po(i,t,kt,!Ao),Xn(i,0,!0)!==0)break e;lo=t,i.timeoutHandle=Jd(Sd.bind(null,i,o,ft,Na,Wr,t,kt,ti,Vi,Ao,a,"Throttled",-0,0),n);break e}Sd(i,o,ft,Na,Wr,t,kt,ti,Vi,Ao,a,null,-0,0)}}break}while(!0);Qt(e)}function Sd(e,t,o,i,n,a,s,r,l,h,v,C,f,y){if(e.timeoutHandle=-1,C=t.subtreeFlags,C&8192||(C&16785408)===16785408){C={stylesheets:null,count:0,imgCount:0,imgBytes:0,suspenseyImages:[],waitingForImages:!0,waitingForViewTransition:!1,unsuspend:Ft},hd(t,a,C);var z=(a&62914560)===a?La-rt():(a&4194048)===a?bd-rt():0;if(z=Oh(C,z),z!==null){lo=a,e.cancelPendingCommit=z(qd.bind(null,e,t,a,o,i,n,s,r,l,v,C,null,f,y)),Po(e,a,s,!h);return}}qd(e,t,a,o,i,n,s,r,l)}function ah(e){for(var t=e;;){var o=t.tag;if((o===0||o===11||o===15)&&t.flags&16384&&(o=t.updateQueue,o!==null&&(o=o.stores,o!==null)))for(var i=0;i<o.length;i++){var n=o[i],a=n.getSnapshot;n=n.value;try{if(!vt(a(),n))return!1}catch{return!1}}if(o=t.child,t.subtreeFlags&16384&&o!==null)o.return=t,t=o;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function Po(e,t,o,i){t&=~Or,t&=~ti,e.suspendedLanes|=t,e.pingedLanes&=~t,i&&(e.warmLanes|=t),i=e.expirationTimes;for(var n=t;0<n;){var a=31-Ye(n),s=1<<a;i[a]=-1,n&=~s}o!==0&&zl(e,o,t)}function Ba(){return(ge&6)===0?(Vn(0),!1):!0}function Kr(){if(ie!==null){if(fe===0)var e=ie.return;else e=ie,Zt=Ho=null,ur(e),Mi=null,bn=0,e=ie;for(;e!==null;)$u(e.alternate,e),e=e.return;ie=null}}function Li(e,t){var o=e.timeoutHandle;o!==-1&&(e.timeoutHandle=-1,Ah(o)),o=e.cancelPendingCommit,o!==null&&(e.cancelPendingCommit=null,o()),lo=0,Kr(),xe=e,ie=o=_t(e.current,null),ae=t,fe=0,xt=null,Ao=!1,ji=en(e,t),Rr=!1,Vi=kt=Or=ti=Eo=je=0,ft=Gn=null,Wr=!1,(t&8)!==0&&(t|=t&32);var i=e.entangledLanes;if(i!==0)for(e=e.entanglements,i&=t;0<i;){var n=31-Ye(i),a=1<<n;t|=e[n],i&=~a}return ro=t,la(),o}function xd(e,t){$=null,b.H=En,t===Ti||t===fa?(t=Uc(),fe=3):t===Js?(t=Uc(),fe=4):fe=t===Er?8:t!==null&&typeof t=="object"&&typeof t.then=="function"?6:1,xt=t,ie===null&&(je=1,qa(e,It(t,e.current)))}function kd(){var e=Ct.current;return e===null?!0:(ae&4194048)===ae?jt===null:(ae&62914560)===ae||(ae&536870912)!==0?e===jt:!1}function Ad(){var e=b.H;return b.H=En,e===null?En:e}function Ed(){var e=b.A;return b.A=ih,e}function Ra(){je=4,Ao||(ae&4194048)!==ae&&Ct.current!==null||(ji=!0),(Eo&134217727)===0&&(ti&134217727)===0||xe===null||Po(xe,ae,kt,!1)}function _r(e,t,o){var i=ge;ge|=2;var n=Ad(),a=Ed();(xe!==e||ae!==t)&&(Na=null,Li(e,t)),t=!1;var s=je;e:do try{if(fe!==0&&ie!==null){var r=ie,l=xt;switch(fe){case 8:Kr(),s=6;break e;case 3:case 2:case 9:case 6:Ct.current===null&&(t=!0);var h=fe;if(fe=0,xt=null,Ui(e,r,l,h),o&&ji){s=0;break e}break;default:h=fe,fe=0,xt=null,Ui(e,r,l,h)}}sh(),s=je;break}catch(v){xd(e,v)}while(!0);return t&&e.shellSuspendCounter++,Zt=Ho=null,ge=i,b.H=n,b.A=a,ie===null&&(xe=null,ae=0,la()),s}function sh(){for(;ie!==null;)Td(ie)}function rh(e,t){var o=ge;ge|=2;var i=Ad(),n=Ed();xe!==e||ae!==t?(Na=null,Ua=rt()+500,Li(e,t)):ji=en(e,t);e:do try{if(fe!==0&&ie!==null){t=ie;var a=xt;t:switch(fe){case 1:fe=0,xt=null,Ui(e,t,a,1);break;case 2:case 9:if(Yc(a)){fe=0,xt=null,Md(t);break}t=function(){fe!==2&&fe!==9||xe!==e||(fe=7),Qt(e)},a.then(t,t);break e;case 3:fe=7;break e;case 4:fe=5;break e;case 7:Yc(a)?(fe=0,xt=null,Md(t)):(fe=0,xt=null,Ui(e,t,a,7));break;case 5:var s=null;switch(ie.tag){case 26:s=ie.memoizedState;case 5:case 27:var r=ie;if(s?gp(s):r.stateNode.complete){fe=0,xt=null;var l=r.sibling;if(l!==null)ie=l;else{var h=r.return;h!==null?(ie=h,Oa(h)):ie=null}break t}}fe=0,xt=null,Ui(e,t,a,5);break;case 6:fe=0,xt=null,Ui(e,t,a,6);break;case 8:Kr(),je=6;break e;default:throw Error(m(462))}}lh();break}catch(v){xd(e,v)}while(!0);return Zt=Ho=null,b.H=i,b.A=n,ge=o,ie!==null?0:(xe=null,ae=0,la(),je)}function lh(){for(;ie!==null&&!Ji();)Td(ie)}function Td(e){var t=Zu(e.alternate,e,ro);e.memoizedProps=e.pendingProps,t===null?Oa(e):ie=t}function Md(e){var t=e,o=t.alternate;switch(t.tag){case 15:case 0:t=Qu(o,t,t.pendingProps,t.type,void 0,ae);break;case 11:t=Qu(o,t,t.pendingProps,t.type.render,t.ref,ae);break;case 5:ur(t);default:$u(o,t),t=ie=Ec(t,ro),t=Zu(o,t,ro)}e.memoizedProps=e.pendingProps,t===null?Oa(e):ie=t}function Ui(e,t,o,i){Zt=Ho=null,ur(t),Mi=null,bn=0;var n=t.return;try{if(Xg(e,n,t,o,ae)){je=1,qa(e,It(o,e.current)),ie=null;return}}catch(a){if(n!==null)throw ie=n,a;je=1,qa(e,It(o,e.current)),ie=null;return}t.flags&32768?(ce||i===1?e=!0:ji||(ae&536870912)!==0?e=!1:(Ao=e=!0,(i===2||i===9||i===3||i===6)&&(i=Ct.current,i!==null&&i.tag===13&&(i.flags|=16384))),Pd(t,e)):Oa(t)}function Oa(e){var t=e;do{if((t.flags&32768)!==0){Pd(t,Ao);return}e=t.return;var o=$g(t.alternate,t,ro);if(o!==null){ie=o;return}if(t=t.sibling,t!==null){ie=t;return}ie=t=e}while(t!==null);je===0&&(je=5)}function Pd(e,t){do{var o=eh(e.alternate,e);if(o!==null){o.flags&=32767,ie=o;return}if(o=e.return,o!==null&&(o.flags|=32768,o.subtreeFlags=0,o.deletions=null),!t&&(e=e.sibling,e!==null)){ie=e;return}ie=e=o}while(e!==null);je=6,ie=null}function qd(e,t,o,i,n,a,s,r,l){e.cancelPendingCommit=null;do Wa();while(He!==0);if((ge&6)!==0)throw Error(m(327));if(t!==null){if(t===e.current)throw Error(m(177));if(a=t.lanes|t.childLanes,a|=Ys,Bp(e,o,a,s,r,l),e===xe&&(ie=xe=null,ae=0),Yi=t,Mo=e,lo=o,Qr=a,Hr=n,vd=i,(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?(e.callbackNode=null,e.callbackPriority=0,ph(x,function(){return jd(),null})):(e.callbackNode=null,e.callbackPriority=0),i=(t.flags&13878)!==0,(t.subtreeFlags&13878)!==0||i){i=b.T,b.T=null,n=E.p,E.p=2,s=ge,ge|=4;try{th(e,t,o)}finally{ge=s,E.p=n,b.T=i}}He=1,Id(),zd(),Dd()}}function Id(){if(He===1){He=0;var e=Mo,t=Yi,o=(t.flags&13878)!==0;if((t.subtreeFlags&13878)!==0||o){o=b.T,b.T=null;var i=E.p;E.p=2;var n=ge;ge|=4;try{dd(t,e);var a=rl,s=yc(e.containerInfo),r=a.focusedElem,l=a.selectionRange;if(s!==r&&r&&r.ownerDocument&&fc(r.ownerDocument.documentElement,r)){if(l!==null&&zs(r)){var h=l.start,v=l.end;if(v===void 0&&(v=h),"selectionStart"in r)r.selectionStart=h,r.selectionEnd=Math.min(v,r.value.length);else{var C=r.ownerDocument||document,f=C&&C.defaultView||window;if(f.getSelection){var y=f.getSelection(),z=r.textContent.length,B=Math.min(l.start,z),Ce=l.end===void 0?B:Math.min(l.end,z);!y.extend&&B>Ce&&(s=Ce,Ce=B,B=s);var p=mc(r,B),u=mc(r,Ce);if(p&&u&&(y.rangeCount!==1||y.anchorNode!==p.node||y.anchorOffset!==p.offset||y.focusNode!==u.node||y.focusOffset!==u.offset)){var g=C.createRange();g.setStart(p.node,p.offset),y.removeAllRanges(),B>Ce?(y.addRange(g),y.extend(u.node,u.offset)):(g.setEnd(u.node,u.offset),y.addRange(g))}}}}for(C=[],y=r;y=y.parentNode;)y.nodeType===1&&C.push({element:y,left:y.scrollLeft,top:y.scrollTop});for(typeof r.focus=="function"&&r.focus(),r=0;r<C.length;r++){var w=C[r];w.element.scrollLeft=w.left,w.element.scrollTop=w.top}}os=!!sl,rl=sl=null}finally{ge=n,E.p=i,b.T=o}}e.current=t,He=2}}function zd(){if(He===2){He=0;var e=Mo,t=Yi,o=(t.flags&8772)!==0;if((t.subtreeFlags&8772)!==0||o){o=b.T,b.T=null;var i=E.p;E.p=2;var n=ge;ge|=4;try{sd(e,t.alternate,t)}finally{ge=n,E.p=i,b.T=o}}He=3}}function Dd(){if(He===4||He===3){He=0,cs();var e=Mo,t=Yi,o=lo,i=vd;(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?He=5:(He=0,Yi=Mo=null,Gd(e,e.pendingLanes));var n=e.pendingLanes;if(n===0&&(To=null),gs(o),t=t.stateNode,Q&&typeof Q.onCommitFiberRoot=="function")try{Q.onCommitFiberRoot(q,t,void 0,(t.current.flags&128)===128)}catch{}if(i!==null){t=b.T,n=E.p,E.p=2,b.T=null;try{for(var a=e.onRecoverableError,s=0;s<i.length;s++){var r=i[s];a(r.value,{componentStack:r.stack})}}finally{b.T=t,E.p=n}}(lo&3)!==0&&Wa(),Qt(e),n=e.pendingLanes,(o&261930)!==0&&(n&42)!==0?e===Fr?jn++:(jn=0,Fr=e):jn=0,Vn(0)}}function Gd(e,t){(e.pooledCacheLanes&=t)===0&&(t=e.pooledCache,t!=null&&(e.pooledCache=null,fn(t)))}function Wa(){return Id(),zd(),Dd(),jd()}function jd(){if(He!==5)return!1;var e=Mo,t=Qr;Qr=0;var o=gs(lo),i=b.T,n=E.p;try{E.p=32>o?32:o,b.T=null,o=Hr,Hr=null;var a=Mo,s=lo;if(He=0,Yi=Mo=null,lo=0,(ge&6)!==0)throw Error(m(331));var r=ge;if(ge|=4,fd(a.current),gd(a,a.current,s,o),ge=r,Vn(0,!1),Q&&typeof Q.onPostCommitFiberRoot=="function")try{Q.onPostCommitFiberRoot(q,a)}catch{}return!0}finally{E.p=n,b.T=i,Gd(e,t)}}function Vd(e,t,o){t=It(o,t),t=Ar(e.stateNode,t,2),e=Co(e,t,2),e!==null&&(tn(e,2),Qt(e))}function ye(e,t,o){if(e.tag===3)Vd(e,e,o);else for(;t!==null;){if(t.tag===3){Vd(t,e,o);break}else if(t.tag===1){var i=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof i.componentDidCatch=="function"&&(To===null||!To.has(i))){e=It(o,e),o=Yu(2),i=Co(t,o,2),i!==null&&(Lu(o,i,t,e),tn(i,2),Qt(i));break}}t=t.return}}function Xr(e,t,o){var i=e.pingCache;if(i===null){i=e.pingCache=new nh;var n=new Set;i.set(t,n)}else n=i.get(t),n===void 0&&(n=new Set,i.set(t,n));n.has(o)||(Rr=!0,n.add(o),e=ch.bind(null,e,t,o),t.then(e,e))}function ch(e,t,o){var i=e.pingCache;i!==null&&i.delete(t),e.pingedLanes|=e.suspendedLanes&o,e.warmLanes&=~o,xe===e&&(ae&o)===o&&(je===4||je===3&&(ae&62914560)===ae&&300>rt()-La?(ge&2)===0&&Li(e,0):Or|=o,Vi===ae&&(Vi=0)),Qt(e)}function Yd(e,t){t===0&&(t=Il()),e=Oo(e,t),e!==null&&(tn(e,t),Qt(e))}function uh(e){var t=e.memoizedState,o=0;t!==null&&(o=t.retryLane),Yd(e,o)}function dh(e,t){var o=0;switch(e.tag){case 31:case 13:var i=e.stateNode,n=e.memoizedState;n!==null&&(o=n.retryLane);break;case 19:i=e.stateNode;break;case 22:i=e.stateNode._retryCache;break;default:throw Error(m(314))}i!==null&&i.delete(t),Yd(e,o)}function ph(e,t){return ai(e,t)}var Qa=null,Ni=null,Zr=!1,Ha=!1,Jr=!1,qo=0;function Qt(e){e!==Ni&&e.next===null&&(Ni===null?Qa=Ni=e:Ni=Ni.next=e),Ha=!0,Zr||(Zr=!0,hh())}function Vn(e,t){if(!Jr&&Ha){Jr=!0;do for(var o=!1,i=Qa;i!==null;){if(e!==0){var n=i.pendingLanes;if(n===0)var a=0;else{var s=i.suspendedLanes,r=i.pingedLanes;a=(1<<31-Ye(42|e)+1)-1,a&=n&~(s&~r),a=a&201326741?a&201326741|1:a?a|2:0}a!==0&&(o=!0,Bd(i,a))}else a=ae,a=Xn(i,i===xe?a:0,i.cancelPendingCommit!==null||i.timeoutHandle!==-1),(a&3)===0||en(i,a)||(o=!0,Bd(i,a));i=i.next}while(o);Jr=!1}}function gh(){Ld()}function Ld(){Ha=Zr=!1;var e=0;qo!==0&&kh()&&(e=qo);for(var t=rt(),o=null,i=Qa;i!==null;){var n=i.next,a=Ud(i,t);a===0?(i.next=null,o===null?Qa=n:o.next=n,n===null&&(Ni=o)):(o=i,(e!==0||(a&3)!==0)&&(Ha=!0)),i=n}He!==0&&He!==5||Vn(e),qo!==0&&(qo=0)}function Ud(e,t){for(var o=e.suspendedLanes,i=e.pingedLanes,n=e.expirationTimes,a=e.pendingLanes&-62914561;0<a;){var s=31-Ye(a),r=1<<s,l=n[s];l===-1?((r&o)===0||(r&i)!==0)&&(n[s]=Np(r,t)):l<=t&&(e.expiredLanes|=r),a&=~r}if(t=xe,o=ae,o=Xn(e,e===t?o:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),i=e.callbackNode,o===0||e===t&&(fe===2||fe===9)||e.cancelPendingCommit!==null)return i!==null&&i!==null&&Zi(i),e.callbackNode=null,e.callbackPriority=0;if((o&3)===0||en(e,o)){if(t=o&-o,t===e.callbackPriority)return t;switch(i!==null&&Zi(i),gs(o)){case 2:case 8:o=$i;break;case 32:o=x;break;case 268435456:o=j;break;default:o=x}return i=Nd.bind(null,e),o=ai(o,i),e.callbackPriority=t,e.callbackNode=o,t}return i!==null&&i!==null&&Zi(i),e.callbackPriority=2,e.callbackNode=null,2}function Nd(e,t){if(He!==0&&He!==5)return e.callbackNode=null,e.callbackPriority=0,null;var o=e.callbackNode;if(Wa()&&e.callbackNode!==o)return null;var i=ae;return i=Xn(e,e===xe?i:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),i===0?null:(Cd(e,i,t),Ud(e,rt()),e.callbackNode!=null&&e.callbackNode===o?Nd.bind(null,e):null)}function Bd(e,t){if(Wa())return null;Cd(e,t,!0)}function hh(){Eh(function(){(ge&6)!==0?ai(_n,gh):Ld()})}function $r(){if(qo===0){var e=Ai;e===0&&(e=uo,uo<<=1,(uo&261888)===0&&(uo=256)),qo=e}return qo}function Rd(e){return e==null||typeof e=="symbol"||typeof e=="boolean"?null:typeof e=="function"?e:ea(""+e)}function Od(e,t){var o=t.ownerDocument.createElement("input");return o.name=t.name,o.value=t.value,e.id&&o.setAttribute("form",e.id),t.parentNode.insertBefore(o,t),e=new FormData(e),o.parentNode.removeChild(o),e}function mh(e,t,o,i,n){if(t==="submit"&&o&&o.stateNode===n){var a=Rd((n[dt]||null).action),s=i.submitter;s&&(t=(t=s[dt]||null)?Rd(t.formAction):s.getAttribute("formAction"),t!==null&&(a=t,s=null));var r=new na("action","action",null,i,n);e.push({event:r,listeners:[{instance:null,listener:function(){if(i.defaultPrevented){if(qo!==0){var l=s?Od(n,s):new FormData(n);vr(o,{pending:!0,data:l,method:n.method,action:a},null,l)}}else typeof a=="function"&&(r.preventDefault(),l=s?Od(n,s):new FormData(n),vr(o,{pending:!0,data:l,method:n.method,action:a},a,l))},currentTarget:n}]})}}for(var el=0;el<Vs.length;el++){var tl=Vs[el],fh=tl.toLowerCase(),yh=tl[0].toUpperCase()+tl.slice(1);Lt(fh,"on"+yh)}Lt(wc,"onAnimationEnd"),Lt(Cc,"onAnimationIteration"),Lt(Sc,"onAnimationStart"),Lt("dblclick","onDoubleClick"),Lt("focusin","onFocus"),Lt("focusout","onBlur"),Lt(Dg,"onTransitionRun"),Lt(Gg,"onTransitionStart"),Lt(jg,"onTransitionCancel"),Lt(xc,"onTransitionEnd"),di("onMouseEnter",["mouseout","mouseover"]),di("onMouseLeave",["mouseout","mouseover"]),di("onPointerEnter",["pointerout","pointerover"]),di("onPointerLeave",["pointerout","pointerover"]),Uo("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),Uo("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),Uo("onBeforeInput",["compositionend","keypress","textInput","paste"]),Uo("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),Uo("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),Uo("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var Yn="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),bh=new Set("beforetoggle cancel close invalid load scroll scrollend toggle".split(" ").concat(Yn));function Wd(e,t){t=(t&4)!==0;for(var o=0;o<e.length;o++){var i=e[o],n=i.event;i=i.listeners;e:{var a=void 0;if(t)for(var s=i.length-1;0<=s;s--){var r=i[s],l=r.instance,h=r.currentTarget;if(r=r.listener,l!==a&&n.isPropagationStopped())break e;a=r,n.currentTarget=h;try{a(n)}catch(v){ra(v)}n.currentTarget=null,a=l}else for(s=0;s<i.length;s++){if(r=i[s],l=r.instance,h=r.currentTarget,r=r.listener,l!==a&&n.isPropagationStopped())break e;a=r,n.currentTarget=h;try{a(n)}catch(v){ra(v)}n.currentTarget=null,a=l}}}}function ne(e,t){var o=t[hs];o===void 0&&(o=t[hs]=new Set);var i=e+"__bubble";o.has(i)||(Qd(t,e,2,!1),o.add(i))}function ol(e,t,o){var i=0;t&&(i|=4),Qd(o,e,i,t)}var Fa="_reactListening"+Math.random().toString(36).slice(2);function il(e){if(!e[Fa]){e[Fa]=!0,Ll.forEach(function(o){o!=="selectionchange"&&(bh.has(o)||ol(o,!1,e),ol(o,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[Fa]||(t[Fa]=!0,ol("selectionchange",!1,t))}}function Qd(e,t,o,i){switch(wp(t)){case 2:var n=Hh;break;case 8:n=Fh;break;default:n=bl}o=n.bind(null,t,o,e),n=void 0,!xs||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(n=!0),i?n!==void 0?e.addEventListener(t,o,{capture:!0,passive:n}):e.addEventListener(t,o,!0):n!==void 0?e.addEventListener(t,o,{passive:n}):e.addEventListener(t,o,!1)}function nl(e,t,o,i,n){var a=i;if((t&1)===0&&(t&2)===0&&i!==null)e:for(;;){if(i===null)return;var s=i.tag;if(s===3||s===4){var r=i.stateNode.containerInfo;if(r===n)break;if(s===4)for(s=i.return;s!==null;){var l=s.tag;if((l===3||l===4)&&s.stateNode.containerInfo===n)return;s=s.return}for(;r!==null;){if(s=li(r),s===null)return;if(l=s.tag,l===5||l===6||l===26||l===27){i=a=s;continue e}r=r.parentNode}}i=i.return}Xl(function(){var h=a,v=Cs(o),C=[];e:{var f=kc.get(e);if(f!==void 0){var y=na,z=e;switch(e){case"keypress":if(oa(o)===0)break e;case"keydown":case"keyup":y=dg;break;case"focusin":z="focus",y=Ts;break;case"focusout":z="blur",y=Ts;break;case"beforeblur":case"afterblur":y=Ts;break;case"click":if(o.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":y=$l;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":y=$p;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":y=hg;break;case wc:case Cc:case Sc:y=og;break;case xc:y=fg;break;case"scroll":case"scrollend":y=Zp;break;case"wheel":y=bg;break;case"copy":case"cut":case"paste":y=ng;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":y=tc;break;case"toggle":case"beforetoggle":y=wg}var B=(t&4)!==0,Ce=!B&&(e==="scroll"||e==="scrollend"),p=B?f!==null?f+"Capture":null:f;B=[];for(var u=h,g;u!==null;){var w=u;if(g=w.stateNode,w=w.tag,w!==5&&w!==26&&w!==27||g===null||p===null||(w=an(u,p),w!=null&&B.push(Ln(u,w,g))),Ce)break;u=u.return}0<B.length&&(f=new y(f,z,null,o,v),C.push({event:f,listeners:B}))}}if((t&7)===0){e:{if(f=e==="mouseover"||e==="pointerover",y=e==="mouseout"||e==="pointerout",f&&o!==ws&&(z=o.relatedTarget||o.fromElement)&&(li(z)||z[ri]))break e;if((y||f)&&(f=v.window===v?v:(f=v.ownerDocument)?f.defaultView||f.parentWindow:window,y?(z=o.relatedTarget||o.toElement,y=h,z=z?li(z):null,z!==null&&(Ce=L(z),B=z.tag,z!==Ce||B!==5&&B!==27&&B!==6)&&(z=null)):(y=null,z=h),y!==z)){if(B=$l,w="onMouseLeave",p="onMouseEnter",u="mouse",(e==="pointerout"||e==="pointerover")&&(B=tc,w="onPointerLeave",p="onPointerEnter",u="pointer"),Ce=y==null?f:nn(y),g=z==null?f:nn(z),f=new B(w,u+"leave",y,o,v),f.target=Ce,f.relatedTarget=g,w=null,li(v)===h&&(B=new B(p,u+"enter",z,o,v),B.target=g,B.relatedTarget=Ce,w=B),Ce=w,y&&z)t:{for(B=vh,p=y,u=z,g=0,w=p;w;w=B(w))g++;w=0;for(var Y=u;Y;Y=B(Y))w++;for(;0<g-w;)p=B(p),g--;for(;0<w-g;)u=B(u),w--;for(;g--;){if(p===u||u!==null&&p===u.alternate){B=p;break t}p=B(p),u=B(u)}B=null}else B=null;y!==null&&Hd(C,f,y,B,!1),z!==null&&Ce!==null&&Hd(C,Ce,z,B,!0)}}e:{if(f=h?nn(h):window,y=f.nodeName&&f.nodeName.toLowerCase(),y==="select"||y==="input"&&f.type==="file")var de=cc;else if(rc(f))if(uc)de=qg;else{de=Mg;var G=Tg}else y=f.nodeName,!y||y.toLowerCase()!=="input"||f.type!=="checkbox"&&f.type!=="radio"?h&&vs(h.elementType)&&(de=cc):de=Pg;if(de&&(de=de(e,h))){lc(C,de,o,v);break e}G&&G(e,f,h),e==="focusout"&&h&&f.type==="number"&&h.memoizedProps.value!=null&&bs(f,"number",f.value)}switch(G=h?nn(h):window,e){case"focusin":(rc(G)||G.contentEditable==="true")&&(yi=G,Ds=h,gn=null);break;case"focusout":gn=Ds=yi=null;break;case"mousedown":Gs=!0;break;case"contextmenu":case"mouseup":case"dragend":Gs=!1,bc(C,o,v);break;case"selectionchange":if(zg)break;case"keydown":case"keyup":bc(C,o,v)}var ee;if(Ps)e:{switch(e){case"compositionstart":var se="onCompositionStart";break e;case"compositionend":se="onCompositionEnd";break e;case"compositionupdate":se="onCompositionUpdate";break e}se=void 0}else fi?ac(e,o)&&(se="onCompositionEnd"):e==="keydown"&&o.keyCode===229&&(se="onCompositionStart");se&&(oc&&o.locale!=="ko"&&(fi||se!=="onCompositionStart"?se==="onCompositionEnd"&&fi&&(ee=Zl()):(ho=v,ks="value"in ho?ho.value:ho.textContent,fi=!0)),G=Ka(h,se),0<G.length&&(se=new ec(se,e,null,o,v),C.push({event:se,listeners:G}),ee?se.data=ee:(ee=sc(o),ee!==null&&(se.data=ee)))),(ee=Sg?xg(e,o):kg(e,o))&&(se=Ka(h,"onBeforeInput"),0<se.length&&(G=new ec("onBeforeInput","beforeinput",null,o,v),C.push({event:G,listeners:se}),G.data=ee)),mh(C,e,h,o,v)}Wd(C,t)})}function Ln(e,t,o){return{instance:e,listener:t,currentTarget:o}}function Ka(e,t){for(var o=t+"Capture",i=[];e!==null;){var n=e,a=n.stateNode;if(n=n.tag,n!==5&&n!==26&&n!==27||a===null||(n=an(e,o),n!=null&&i.unshift(Ln(e,n,a)),n=an(e,t),n!=null&&i.push(Ln(e,n,a))),e.tag===3)return i;e=e.return}return[]}function vh(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5&&e.tag!==27);return e||null}function Hd(e,t,o,i,n){for(var a=t._reactName,s=[];o!==null&&o!==i;){var r=o,l=r.alternate,h=r.stateNode;if(r=r.tag,l!==null&&l===i)break;r!==5&&r!==26&&r!==27||h===null||(l=h,n?(h=an(o,a),h!=null&&s.unshift(Ln(o,h,l))):n||(h=an(o,a),h!=null&&s.push(Ln(o,h,l)))),o=o.return}s.length!==0&&e.push({event:t,listeners:s})}var wh=/\r\n?/g,Ch=/\u0000|\uFFFD/g;function Fd(e){return(typeof e=="string"?e:""+e).replace(wh,`
`).replace(Ch,"")}function Kd(e,t){return t=Fd(t),Fd(e)===t}function we(e,t,o,i,n,a){switch(o){case"children":typeof i=="string"?t==="body"||t==="textarea"&&i===""||gi(e,i):(typeof i=="number"||typeof i=="bigint")&&t!=="body"&&gi(e,""+i);break;case"className":Jn(e,"class",i);break;case"tabIndex":Jn(e,"tabindex",i);break;case"dir":case"role":case"viewBox":case"width":case"height":Jn(e,o,i);break;case"style":Kl(e,i,a);break;case"data":if(t!=="object"){Jn(e,"data",i);break}case"src":case"href":if(i===""&&(t!=="a"||o!=="href")){e.removeAttribute(o);break}if(i==null||typeof i=="function"||typeof i=="symbol"||typeof i=="boolean"){e.removeAttribute(o);break}i=ea(""+i),e.setAttribute(o,i);break;case"action":case"formAction":if(typeof i=="function"){e.setAttribute(o,"javascript:throw new Error('A React form was unexpectedly submitted. If you called form.submit() manually, consider using form.requestSubmit() instead. If you\\'re trying to use event.stopPropagation() in a submit event handler, consider also calling event.preventDefault().')");break}else typeof a=="function"&&(o==="formAction"?(t!=="input"&&we(e,t,"name",n.name,n,null),we(e,t,"formEncType",n.formEncType,n,null),we(e,t,"formMethod",n.formMethod,n,null),we(e,t,"formTarget",n.formTarget,n,null)):(we(e,t,"encType",n.encType,n,null),we(e,t,"method",n.method,n,null),we(e,t,"target",n.target,n,null)));if(i==null||typeof i=="symbol"||typeof i=="boolean"){e.removeAttribute(o);break}i=ea(""+i),e.setAttribute(o,i);break;case"onClick":i!=null&&(e.onclick=Ft);break;case"onScroll":i!=null&&ne("scroll",e);break;case"onScrollEnd":i!=null&&ne("scrollend",e);break;case"dangerouslySetInnerHTML":if(i!=null){if(typeof i!="object"||!("__html"in i))throw Error(m(61));if(o=i.__html,o!=null){if(n.children!=null)throw Error(m(60));e.innerHTML=o}}break;case"multiple":e.multiple=i&&typeof i!="function"&&typeof i!="symbol";break;case"muted":e.muted=i&&typeof i!="function"&&typeof i!="symbol";break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"defaultValue":case"defaultChecked":case"innerHTML":case"ref":break;case"autoFocus":break;case"xlinkHref":if(i==null||typeof i=="function"||typeof i=="boolean"||typeof i=="symbol"){e.removeAttribute("xlink:href");break}o=ea(""+i),e.setAttributeNS("http://www.w3.org/1999/xlink","xlink:href",o);break;case"contentEditable":case"spellCheck":case"draggable":case"value":case"autoReverse":case"externalResourcesRequired":case"focusable":case"preserveAlpha":i!=null&&typeof i!="function"&&typeof i!="symbol"?e.setAttribute(o,""+i):e.removeAttribute(o);break;case"inert":case"allowFullScreen":case"async":case"autoPlay":case"controls":case"default":case"defer":case"disabled":case"disablePictureInPicture":case"disableRemotePlayback":case"formNoValidate":case"hidden":case"loop":case"noModule":case"noValidate":case"open":case"playsInline":case"readOnly":case"required":case"reversed":case"scoped":case"seamless":case"itemScope":i&&typeof i!="function"&&typeof i!="symbol"?e.setAttribute(o,""):e.removeAttribute(o);break;case"capture":case"download":i===!0?e.setAttribute(o,""):i!==!1&&i!=null&&typeof i!="function"&&typeof i!="symbol"?e.setAttribute(o,i):e.removeAttribute(o);break;case"cols":case"rows":case"size":case"span":i!=null&&typeof i!="function"&&typeof i!="symbol"&&!isNaN(i)&&1<=i?e.setAttribute(o,i):e.removeAttribute(o);break;case"rowSpan":case"start":i==null||typeof i=="function"||typeof i=="symbol"||isNaN(i)?e.removeAttribute(o):e.setAttribute(o,i);break;case"popover":ne("beforetoggle",e),ne("toggle",e),Zn(e,"popover",i);break;case"xlinkActuate":Ht(e,"http://www.w3.org/1999/xlink","xlink:actuate",i);break;case"xlinkArcrole":Ht(e,"http://www.w3.org/1999/xlink","xlink:arcrole",i);break;case"xlinkRole":Ht(e,"http://www.w3.org/1999/xlink","xlink:role",i);break;case"xlinkShow":Ht(e,"http://www.w3.org/1999/xlink","xlink:show",i);break;case"xlinkTitle":Ht(e,"http://www.w3.org/1999/xlink","xlink:title",i);break;case"xlinkType":Ht(e,"http://www.w3.org/1999/xlink","xlink:type",i);break;case"xmlBase":Ht(e,"http://www.w3.org/XML/1998/namespace","xml:base",i);break;case"xmlLang":Ht(e,"http://www.w3.org/XML/1998/namespace","xml:lang",i);break;case"xmlSpace":Ht(e,"http://www.w3.org/XML/1998/namespace","xml:space",i);break;case"is":Zn(e,"is",i);break;case"innerText":case"textContent":break;default:(!(2<o.length)||o[0]!=="o"&&o[0]!=="O"||o[1]!=="n"&&o[1]!=="N")&&(o=_p.get(o)||o,Zn(e,o,i))}}function al(e,t,o,i,n,a){switch(o){case"style":Kl(e,i,a);break;case"dangerouslySetInnerHTML":if(i!=null){if(typeof i!="object"||!("__html"in i))throw Error(m(61));if(o=i.__html,o!=null){if(n.children!=null)throw Error(m(60));e.innerHTML=o}}break;case"children":typeof i=="string"?gi(e,i):(typeof i=="number"||typeof i=="bigint")&&gi(e,""+i);break;case"onScroll":i!=null&&ne("scroll",e);break;case"onScrollEnd":i!=null&&ne("scrollend",e);break;case"onClick":i!=null&&(e.onclick=Ft);break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"innerHTML":case"ref":break;case"innerText":case"textContent":break;default:if(!Ul.hasOwnProperty(o))e:{if(o[0]==="o"&&o[1]==="n"&&(n=o.endsWith("Capture"),t=o.slice(2,n?o.length-7:void 0),a=e[dt]||null,a=a!=null?a[o]:null,typeof a=="function"&&e.removeEventListener(t,a,n),typeof i=="function")){typeof a!="function"&&a!==null&&(o in e?e[o]=null:e.hasAttribute(o)&&e.removeAttribute(o)),e.addEventListener(t,i,n);break e}o in e?e[o]=i:i===!0?e.setAttribute(o,""):Zn(e,o,i)}}}function it(e,t,o){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"img":ne("error",e),ne("load",e);var i=!1,n=!1,a;for(a in o)if(o.hasOwnProperty(a)){var s=o[a];if(s!=null)switch(a){case"src":i=!0;break;case"srcSet":n=!0;break;case"children":case"dangerouslySetInnerHTML":throw Error(m(137,t));default:we(e,t,a,s,o,null)}}n&&we(e,t,"srcSet",o.srcSet,o,null),i&&we(e,t,"src",o.src,o,null);return;case"input":ne("invalid",e);var r=a=s=n=null,l=null,h=null;for(i in o)if(o.hasOwnProperty(i)){var v=o[i];if(v!=null)switch(i){case"name":n=v;break;case"type":s=v;break;case"checked":l=v;break;case"defaultChecked":h=v;break;case"value":a=v;break;case"defaultValue":r=v;break;case"children":case"dangerouslySetInnerHTML":if(v!=null)throw Error(m(137,t));break;default:we(e,t,i,v,o,null)}}Wl(e,a,r,l,h,s,n,!1);return;case"select":ne("invalid",e),i=s=a=null;for(n in o)if(o.hasOwnProperty(n)&&(r=o[n],r!=null))switch(n){case"value":a=r;break;case"defaultValue":s=r;break;case"multiple":i=r;default:we(e,t,n,r,o,null)}t=a,o=s,e.multiple=!!i,t!=null?pi(e,!!i,t,!1):o!=null&&pi(e,!!i,o,!0);return;case"textarea":ne("invalid",e),a=n=i=null;for(s in o)if(o.hasOwnProperty(s)&&(r=o[s],r!=null))switch(s){case"value":i=r;break;case"defaultValue":n=r;break;case"children":a=r;break;case"dangerouslySetInnerHTML":if(r!=null)throw Error(m(91));break;default:we(e,t,s,r,o,null)}Hl(e,i,n,a);return;case"option":for(l in o)if(o.hasOwnProperty(l)&&(i=o[l],i!=null))switch(l){case"selected":e.selected=i&&typeof i!="function"&&typeof i!="symbol";break;default:we(e,t,l,i,o,null)}return;case"dialog":ne("beforetoggle",e),ne("toggle",e),ne("cancel",e),ne("close",e);break;case"iframe":case"object":ne("load",e);break;case"video":case"audio":for(i=0;i<Yn.length;i++)ne(Yn[i],e);break;case"image":ne("error",e),ne("load",e);break;case"details":ne("toggle",e);break;case"embed":case"source":case"link":ne("error",e),ne("load",e);case"area":case"base":case"br":case"col":case"hr":case"keygen":case"meta":case"param":case"track":case"wbr":case"menuitem":for(h in o)if(o.hasOwnProperty(h)&&(i=o[h],i!=null))switch(h){case"children":case"dangerouslySetInnerHTML":throw Error(m(137,t));default:we(e,t,h,i,o,null)}return;default:if(vs(t)){for(v in o)o.hasOwnProperty(v)&&(i=o[v],i!==void 0&&al(e,t,v,i,o,void 0));return}}for(r in o)o.hasOwnProperty(r)&&(i=o[r],i!=null&&we(e,t,r,i,o,null))}function Sh(e,t,o,i){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"input":var n=null,a=null,s=null,r=null,l=null,h=null,v=null;for(y in o){var C=o[y];if(o.hasOwnProperty(y)&&C!=null)switch(y){case"checked":break;case"value":break;case"defaultValue":l=C;default:i.hasOwnProperty(y)||we(e,t,y,null,i,C)}}for(var f in i){var y=i[f];if(C=o[f],i.hasOwnProperty(f)&&(y!=null||C!=null))switch(f){case"type":a=y;break;case"name":n=y;break;case"checked":h=y;break;case"defaultChecked":v=y;break;case"value":s=y;break;case"defaultValue":r=y;break;case"children":case"dangerouslySetInnerHTML":if(y!=null)throw Error(m(137,t));break;default:y!==C&&we(e,t,f,y,i,C)}}ys(e,s,r,l,h,v,a,n);return;case"select":y=s=r=f=null;for(a in o)if(l=o[a],o.hasOwnProperty(a)&&l!=null)switch(a){case"value":break;case"multiple":y=l;default:i.hasOwnProperty(a)||we(e,t,a,null,i,l)}for(n in i)if(a=i[n],l=o[n],i.hasOwnProperty(n)&&(a!=null||l!=null))switch(n){case"value":f=a;break;case"defaultValue":r=a;break;case"multiple":s=a;default:a!==l&&we(e,t,n,a,i,l)}t=r,o=s,i=y,f!=null?pi(e,!!o,f,!1):!!i!=!!o&&(t!=null?pi(e,!!o,t,!0):pi(e,!!o,o?[]:"",!1));return;case"textarea":y=f=null;for(r in o)if(n=o[r],o.hasOwnProperty(r)&&n!=null&&!i.hasOwnProperty(r))switch(r){case"value":break;case"children":break;default:we(e,t,r,null,i,n)}for(s in i)if(n=i[s],a=o[s],i.hasOwnProperty(s)&&(n!=null||a!=null))switch(s){case"value":f=n;break;case"defaultValue":y=n;break;case"children":break;case"dangerouslySetInnerHTML":if(n!=null)throw Error(m(91));break;default:n!==a&&we(e,t,s,n,i,a)}Ql(e,f,y);return;case"option":for(var z in o)if(f=o[z],o.hasOwnProperty(z)&&f!=null&&!i.hasOwnProperty(z))switch(z){case"selected":e.selected=!1;break;default:we(e,t,z,null,i,f)}for(l in i)if(f=i[l],y=o[l],i.hasOwnProperty(l)&&f!==y&&(f!=null||y!=null))switch(l){case"selected":e.selected=f&&typeof f!="function"&&typeof f!="symbol";break;default:we(e,t,l,f,i,y)}return;case"img":case"link":case"area":case"base":case"br":case"col":case"embed":case"hr":case"keygen":case"meta":case"param":case"source":case"track":case"wbr":case"menuitem":for(var B in o)f=o[B],o.hasOwnProperty(B)&&f!=null&&!i.hasOwnProperty(B)&&we(e,t,B,null,i,f);for(h in i)if(f=i[h],y=o[h],i.hasOwnProperty(h)&&f!==y&&(f!=null||y!=null))switch(h){case"children":case"dangerouslySetInnerHTML":if(f!=null)throw Error(m(137,t));break;default:we(e,t,h,f,i,y)}return;default:if(vs(t)){for(var Ce in o)f=o[Ce],o.hasOwnProperty(Ce)&&f!==void 0&&!i.hasOwnProperty(Ce)&&al(e,t,Ce,void 0,i,f);for(v in i)f=i[v],y=o[v],!i.hasOwnProperty(v)||f===y||f===void 0&&y===void 0||al(e,t,v,f,i,y);return}}for(var p in o)f=o[p],o.hasOwnProperty(p)&&f!=null&&!i.hasOwnProperty(p)&&we(e,t,p,null,i,f);for(C in i)f=i[C],y=o[C],!i.hasOwnProperty(C)||f===y||f==null&&y==null||we(e,t,C,f,i,y)}function _d(e){switch(e){case"css":case"script":case"font":case"img":case"image":case"input":case"link":return!0;default:return!1}}function xh(){if(typeof performance.getEntriesByType=="function"){for(var e=0,t=0,o=performance.getEntriesByType("resource"),i=0;i<o.length;i++){var n=o[i],a=n.transferSize,s=n.initiatorType,r=n.duration;if(a&&r&&_d(s)){for(s=0,r=n.responseEnd,i+=1;i<o.length;i++){var l=o[i],h=l.startTime;if(h>r)break;var v=l.transferSize,C=l.initiatorType;v&&_d(C)&&(l=l.responseEnd,s+=v*(l<r?1:(r-h)/(l-h)))}if(--i,t+=8*(a+s)/(n.duration/1e3),e++,10<e)break}}if(0<e)return t/e/1e6}return navigator.connection&&(e=navigator.connection.downlink,typeof e=="number")?e:5}var sl=null,rl=null;function _a(e){return e.nodeType===9?e:e.ownerDocument}function Xd(e){switch(e){case"http://www.w3.org/2000/svg":return 1;case"http://www.w3.org/1998/Math/MathML":return 2;default:return 0}}function Zd(e,t){if(e===0)switch(t){case"svg":return 1;case"math":return 2;default:return 0}return e===1&&t==="foreignObject"?0:e}function ll(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.children=="bigint"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var cl=null;function kh(){var e=window.event;return e&&e.type==="popstate"?e===cl?!1:(cl=e,!0):(cl=null,!1)}var Jd=typeof setTimeout=="function"?setTimeout:void 0,Ah=typeof clearTimeout=="function"?clearTimeout:void 0,$d=typeof Promise=="function"?Promise:void 0,Eh=typeof queueMicrotask=="function"?queueMicrotask:typeof $d<"u"?function(e){return $d.resolve(null).then(e).catch(Th)}:Jd;function Th(e){setTimeout(function(){throw e})}function Io(e){return e==="head"}function ep(e,t){var o=t,i=0;do{var n=o.nextSibling;if(e.removeChild(o),n&&n.nodeType===8)if(o=n.data,o==="/$"||o==="/&"){if(i===0){e.removeChild(n),Wi(t);return}i--}else if(o==="$"||o==="$?"||o==="$~"||o==="$!"||o==="&")i++;else if(o==="html")Un(e.ownerDocument.documentElement);else if(o==="head"){o=e.ownerDocument.head,Un(o);for(var a=o.firstChild;a;){var s=a.nextSibling,r=a.nodeName;a[on]||r==="SCRIPT"||r==="STYLE"||r==="LINK"&&a.rel.toLowerCase()==="stylesheet"||o.removeChild(a),a=s}}else o==="body"&&Un(e.ownerDocument.body);o=n}while(o);Wi(t)}function tp(e,t){var o=e;e=0;do{var i=o.nextSibling;if(o.nodeType===1?t?(o._stashedDisplay=o.style.display,o.style.display="none"):(o.style.display=o._stashedDisplay||"",o.getAttribute("style")===""&&o.removeAttribute("style")):o.nodeType===3&&(t?(o._stashedText=o.nodeValue,o.nodeValue=""):o.nodeValue=o._stashedText||""),i&&i.nodeType===8)if(o=i.data,o==="/$"){if(e===0)break;e--}else o!=="$"&&o!=="$?"&&o!=="$~"&&o!=="$!"||e++;o=i}while(o)}function ul(e){var t=e.firstChild;for(t&&t.nodeType===10&&(t=t.nextSibling);t;){var o=t;switch(t=t.nextSibling,o.nodeName){case"HTML":case"HEAD":case"BODY":ul(o),ms(o);continue;case"SCRIPT":case"STYLE":continue;case"LINK":if(o.rel.toLowerCase()==="stylesheet")continue}e.removeChild(o)}}function Mh(e,t,o,i){for(;e.nodeType===1;){var n=o;if(e.nodeName.toLowerCase()!==t.toLowerCase()){if(!i&&(e.nodeName!=="INPUT"||e.type!=="hidden"))break}else if(i){if(!e[on])switch(t){case"meta":if(!e.hasAttribute("itemprop"))break;return e;case"link":if(a=e.getAttribute("rel"),a==="stylesheet"&&e.hasAttribute("data-precedence"))break;if(a!==n.rel||e.getAttribute("href")!==(n.href==null||n.href===""?null:n.href)||e.getAttribute("crossorigin")!==(n.crossOrigin==null?null:n.crossOrigin)||e.getAttribute("title")!==(n.title==null?null:n.title))break;return e;case"style":if(e.hasAttribute("data-precedence"))break;return e;case"script":if(a=e.getAttribute("src"),(a!==(n.src==null?null:n.src)||e.getAttribute("type")!==(n.type==null?null:n.type)||e.getAttribute("crossorigin")!==(n.crossOrigin==null?null:n.crossOrigin))&&a&&e.hasAttribute("async")&&!e.hasAttribute("itemprop"))break;return e;default:return e}}else if(t==="input"&&e.type==="hidden"){var a=n.name==null?null:""+n.name;if(n.type==="hidden"&&e.getAttribute("name")===a)return e}else return e;if(e=Vt(e.nextSibling),e===null)break}return null}function Ph(e,t,o){if(t==="")return null;for(;e.nodeType!==3;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!o||(e=Vt(e.nextSibling),e===null))return null;return e}function op(e,t){for(;e.nodeType!==8;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!t||(e=Vt(e.nextSibling),e===null))return null;return e}function dl(e){return e.data==="$?"||e.data==="$~"}function pl(e){return e.data==="$!"||e.data==="$?"&&e.ownerDocument.readyState!=="loading"}function qh(e,t){var o=e.ownerDocument;if(e.data==="$~")e._reactRetry=t;else if(e.data!=="$?"||o.readyState!=="loading")t();else{var i=function(){t(),o.removeEventListener("DOMContentLoaded",i)};o.addEventListener("DOMContentLoaded",i),e._reactRetry=i}}function Vt(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?"||t==="$~"||t==="&"||t==="F!"||t==="F")break;if(t==="/$"||t==="/&")return null}}return e}var gl=null;function ip(e){e=e.nextSibling;for(var t=0;e;){if(e.nodeType===8){var o=e.data;if(o==="/$"||o==="/&"){if(t===0)return Vt(e.nextSibling);t--}else o!=="$"&&o!=="$!"&&o!=="$?"&&o!=="$~"&&o!=="&"||t++}e=e.nextSibling}return null}function np(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var o=e.data;if(o==="$"||o==="$!"||o==="$?"||o==="$~"||o==="&"){if(t===0)return e;t--}else o!=="/$"&&o!=="/&"||t++}e=e.previousSibling}return null}function ap(e,t,o){switch(t=_a(o),e){case"html":if(e=t.documentElement,!e)throw Error(m(452));return e;case"head":if(e=t.head,!e)throw Error(m(453));return e;case"body":if(e=t.body,!e)throw Error(m(454));return e;default:throw Error(m(451))}}function Un(e){for(var t=e.attributes;t.length;)e.removeAttributeNode(t[0]);ms(e)}var Yt=new Map,sp=new Set;function Xa(e){return typeof e.getRootNode=="function"?e.getRootNode():e.nodeType===9?e:e.ownerDocument}var co=E.d;E.d={f:Ih,r:zh,D:Dh,C:Gh,L:jh,m:Vh,X:Lh,S:Yh,M:Uh};function Ih(){var e=co.f(),t=Ba();return e||t}function zh(e){var t=ci(e);t!==null&&t.tag===5&&t.type==="form"?xu(t):co.r(e)}var Bi=typeof document>"u"?null:document;function rp(e,t,o){var i=Bi;if(i&&typeof t=="string"&&t){var n=Pt(t);n='link[rel="'+e+'"][href="'+n+'"]',typeof o=="string"&&(n+='[crossorigin="'+o+'"]'),sp.has(n)||(sp.add(n),e={rel:e,crossOrigin:o,href:t},i.querySelector(n)===null&&(t=i.createElement("link"),it(t,"link",e),_e(t),i.head.appendChild(t)))}}function Dh(e){co.D(e),rp("dns-prefetch",e,null)}function Gh(e,t){co.C(e,t),rp("preconnect",e,t)}function jh(e,t,o){co.L(e,t,o);var i=Bi;if(i&&e&&t){var n='link[rel="preload"][as="'+Pt(t)+'"]';t==="image"&&o&&o.imageSrcSet?(n+='[imagesrcset="'+Pt(o.imageSrcSet)+'"]',typeof o.imageSizes=="string"&&(n+='[imagesizes="'+Pt(o.imageSizes)+'"]')):n+='[href="'+Pt(e)+'"]';var a=n;switch(t){case"style":a=Ri(e);break;case"script":a=Oi(e)}Yt.has(a)||(e=N({rel:"preload",href:t==="image"&&o&&o.imageSrcSet?void 0:e,as:t},o),Yt.set(a,e),i.querySelector(n)!==null||t==="style"&&i.querySelector(Nn(a))||t==="script"&&i.querySelector(Bn(a))||(t=i.createElement("link"),it(t,"link",e),_e(t),i.head.appendChild(t)))}}function Vh(e,t){co.m(e,t);var o=Bi;if(o&&e){var i=t&&typeof t.as=="string"?t.as:"script",n='link[rel="modulepreload"][as="'+Pt(i)+'"][href="'+Pt(e)+'"]',a=n;switch(i){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":a=Oi(e)}if(!Yt.has(a)&&(e=N({rel:"modulepreload",href:e},t),Yt.set(a,e),o.querySelector(n)===null)){switch(i){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":if(o.querySelector(Bn(a)))return}i=o.createElement("link"),it(i,"link",e),_e(i),o.head.appendChild(i)}}}function Yh(e,t,o){co.S(e,t,o);var i=Bi;if(i&&e){var n=ui(i).hoistableStyles,a=Ri(e);t=t||"default";var s=n.get(a);if(!s){var r={loading:0,preload:null};if(s=i.querySelector(Nn(a)))r.loading=5;else{e=N({rel:"stylesheet",href:e,"data-precedence":t},o),(o=Yt.get(a))&&hl(e,o);var l=s=i.createElement("link");_e(l),it(l,"link",e),l._p=new Promise(function(h,v){l.onload=h,l.onerror=v}),l.addEventListener("load",function(){r.loading|=1}),l.addEventListener("error",function(){r.loading|=2}),r.loading|=4,Za(s,t,i)}s={type:"stylesheet",instance:s,count:1,state:r},n.set(a,s)}}}function Lh(e,t){co.X(e,t);var o=Bi;if(o&&e){var i=ui(o).hoistableScripts,n=Oi(e),a=i.get(n);a||(a=o.querySelector(Bn(n)),a||(e=N({src:e,async:!0},t),(t=Yt.get(n))&&ml(e,t),a=o.createElement("script"),_e(a),it(a,"link",e),o.head.appendChild(a)),a={type:"script",instance:a,count:1,state:null},i.set(n,a))}}function Uh(e,t){co.M(e,t);var o=Bi;if(o&&e){var i=ui(o).hoistableScripts,n=Oi(e),a=i.get(n);a||(a=o.querySelector(Bn(n)),a||(e=N({src:e,async:!0,type:"module"},t),(t=Yt.get(n))&&ml(e,t),a=o.createElement("script"),_e(a),it(a,"link",e),o.head.appendChild(a)),a={type:"script",instance:a,count:1,state:null},i.set(n,a))}}function lp(e,t,o,i){var n=(n=F.current)?Xa(n):null;if(!n)throw Error(m(446));switch(e){case"meta":case"title":return null;case"style":return typeof o.precedence=="string"&&typeof o.href=="string"?(t=Ri(o.href),o=ui(n).hoistableStyles,i=o.get(t),i||(i={type:"style",instance:null,count:0,state:null},o.set(t,i)),i):{type:"void",instance:null,count:0,state:null};case"link":if(o.rel==="stylesheet"&&typeof o.href=="string"&&typeof o.precedence=="string"){e=Ri(o.href);var a=ui(n).hoistableStyles,s=a.get(e);if(s||(n=n.ownerDocument||n,s={type:"stylesheet",instance:null,count:0,state:{loading:0,preload:null}},a.set(e,s),(a=n.querySelector(Nn(e)))&&!a._p&&(s.instance=a,s.state.loading=5),Yt.has(e)||(o={rel:"preload",as:"style",href:o.href,crossOrigin:o.crossOrigin,integrity:o.integrity,media:o.media,hrefLang:o.hrefLang,referrerPolicy:o.referrerPolicy},Yt.set(e,o),a||Nh(n,e,o,s.state))),t&&i===null)throw Error(m(528,""));return s}if(t&&i!==null)throw Error(m(529,""));return null;case"script":return t=o.async,o=o.src,typeof o=="string"&&t&&typeof t!="function"&&typeof t!="symbol"?(t=Oi(o),o=ui(n).hoistableScripts,i=o.get(t),i||(i={type:"script",instance:null,count:0,state:null},o.set(t,i)),i):{type:"void",instance:null,count:0,state:null};default:throw Error(m(444,e))}}function Ri(e){return'href="'+Pt(e)+'"'}function Nn(e){return'link[rel="stylesheet"]['+e+"]"}function cp(e){return N({},e,{"data-precedence":e.precedence,precedence:null})}function Nh(e,t,o,i){e.querySelector('link[rel="preload"][as="style"]['+t+"]")?i.loading=1:(t=e.createElement("link"),i.preload=t,t.addEventListener("load",function(){return i.loading|=1}),t.addEventListener("error",function(){return i.loading|=2}),it(t,"link",o),_e(t),e.head.appendChild(t))}function Oi(e){return'[src="'+Pt(e)+'"]'}function Bn(e){return"script[async]"+e}function up(e,t,o){if(t.count++,t.instance===null)switch(t.type){case"style":var i=e.querySelector('style[data-href~="'+Pt(o.href)+'"]');if(i)return t.instance=i,_e(i),i;var n=N({},o,{"data-href":o.href,"data-precedence":o.precedence,href:null,precedence:null});return i=(e.ownerDocument||e).createElement("style"),_e(i),it(i,"style",n),Za(i,o.precedence,e),t.instance=i;case"stylesheet":n=Ri(o.href);var a=e.querySelector(Nn(n));if(a)return t.state.loading|=4,t.instance=a,_e(a),a;i=cp(o),(n=Yt.get(n))&&hl(i,n),a=(e.ownerDocument||e).createElement("link"),_e(a);var s=a;return s._p=new Promise(function(r,l){s.onload=r,s.onerror=l}),it(a,"link",i),t.state.loading|=4,Za(a,o.precedence,e),t.instance=a;case"script":return a=Oi(o.src),(n=e.querySelector(Bn(a)))?(t.instance=n,_e(n),n):(i=o,(n=Yt.get(a))&&(i=N({},o),ml(i,n)),e=e.ownerDocument||e,n=e.createElement("script"),_e(n),it(n,"link",i),e.head.appendChild(n),t.instance=n);case"void":return null;default:throw Error(m(443,t.type))}else t.type==="stylesheet"&&(t.state.loading&4)===0&&(i=t.instance,t.state.loading|=4,Za(i,o.precedence,e));return t.instance}function Za(e,t,o){for(var i=o.querySelectorAll('link[rel="stylesheet"][data-precedence],style[data-precedence]'),n=i.length?i[i.length-1]:null,a=n,s=0;s<i.length;s++){var r=i[s];if(r.dataset.precedence===t)a=r;else if(a!==n)break}a?a.parentNode.insertBefore(e,a.nextSibling):(t=o.nodeType===9?o.head:o,t.insertBefore(e,t.firstChild))}function hl(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.title==null&&(e.title=t.title)}function ml(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.integrity==null&&(e.integrity=t.integrity)}var Ja=null;function dp(e,t,o){if(Ja===null){var i=new Map,n=Ja=new Map;n.set(o,i)}else n=Ja,i=n.get(o),i||(i=new Map,n.set(o,i));if(i.has(e))return i;for(i.set(e,null),o=o.getElementsByTagName(e),n=0;n<o.length;n++){var a=o[n];if(!(a[on]||a[$e]||e==="link"&&a.getAttribute("rel")==="stylesheet")&&a.namespaceURI!=="http://www.w3.org/2000/svg"){var s=a.getAttribute(t)||"";s=e+s;var r=i.get(s);r?r.push(a):i.set(s,[a])}}return i}function pp(e,t,o){e=e.ownerDocument||e,e.head.insertBefore(o,t==="title"?e.querySelector("head > title"):null)}function Bh(e,t,o){if(o===1||t.itemProp!=null)return!1;switch(e){case"meta":case"title":return!0;case"style":if(typeof t.precedence!="string"||typeof t.href!="string"||t.href==="")break;return!0;case"link":if(typeof t.rel!="string"||typeof t.href!="string"||t.href===""||t.onLoad||t.onError)break;switch(t.rel){case"stylesheet":return e=t.disabled,typeof t.precedence=="string"&&e==null;default:return!0}case"script":if(t.async&&typeof t.async!="function"&&typeof t.async!="symbol"&&!t.onLoad&&!t.onError&&t.src&&typeof t.src=="string")return!0}return!1}function gp(e){return!(e.type==="stylesheet"&&(e.state.loading&3)===0)}function Rh(e,t,o,i){if(o.type==="stylesheet"&&(typeof i.media!="string"||matchMedia(i.media).matches!==!1)&&(o.state.loading&4)===0){if(o.instance===null){var n=Ri(i.href),a=t.querySelector(Nn(n));if(a){t=a._p,t!==null&&typeof t=="object"&&typeof t.then=="function"&&(e.count++,e=$a.bind(e),t.then(e,e)),o.state.loading|=4,o.instance=a,_e(a);return}a=t.ownerDocument||t,i=cp(i),(n=Yt.get(n))&&hl(i,n),a=a.createElement("link"),_e(a);var s=a;s._p=new Promise(function(r,l){s.onload=r,s.onerror=l}),it(a,"link",i),o.instance=a}e.stylesheets===null&&(e.stylesheets=new Map),e.stylesheets.set(o,t),(t=o.state.preload)&&(o.state.loading&3)===0&&(e.count++,o=$a.bind(e),t.addEventListener("load",o),t.addEventListener("error",o))}}var fl=0;function Oh(e,t){return e.stylesheets&&e.count===0&&ts(e,e.stylesheets),0<e.count||0<e.imgCount?function(o){var i=setTimeout(function(){if(e.stylesheets&&ts(e,e.stylesheets),e.unsuspend){var a=e.unsuspend;e.unsuspend=null,a()}},6e4+t);0<e.imgBytes&&fl===0&&(fl=62500*xh());var n=setTimeout(function(){if(e.waitingForImages=!1,e.count===0&&(e.stylesheets&&ts(e,e.stylesheets),e.unsuspend)){var a=e.unsuspend;e.unsuspend=null,a()}},(e.imgBytes>fl?50:800)+t);return e.unsuspend=o,function(){e.unsuspend=null,clearTimeout(i),clearTimeout(n)}}:null}function $a(){if(this.count--,this.count===0&&(this.imgCount===0||!this.waitingForImages)){if(this.stylesheets)ts(this,this.stylesheets);else if(this.unsuspend){var e=this.unsuspend;this.unsuspend=null,e()}}}var es=null;function ts(e,t){e.stylesheets=null,e.unsuspend!==null&&(e.count++,es=new Map,t.forEach(Wh,e),es=null,$a.call(e))}function Wh(e,t){if(!(t.state.loading&4)){var o=es.get(e);if(o)var i=o.get(null);else{o=new Map,es.set(e,o);for(var n=e.querySelectorAll("link[data-precedence],style[data-precedence]"),a=0;a<n.length;a++){var s=n[a];(s.nodeName==="LINK"||s.getAttribute("media")!=="not all")&&(o.set(s.dataset.precedence,s),i=s)}i&&o.set(null,i)}n=t.instance,s=n.getAttribute("data-precedence"),a=o.get(s)||i,a===i&&o.set(null,n),o.set(s,n),this.count++,i=$a.bind(this),n.addEventListener("load",i),n.addEventListener("error",i),a?a.parentNode.insertBefore(n,a.nextSibling):(e=e.nodeType===9?e.head:e,e.insertBefore(n,e.firstChild)),t.state.loading|=4}}var Rn={$$typeof:Ie,Provider:null,Consumer:null,_currentValue:V,_currentValue2:V,_threadCount:0};function Qh(e,t,o,i,n,a,s,r,l){this.tag=1,this.containerInfo=e,this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.next=this.pendingContext=this.context=this.cancelPendingCommit=null,this.callbackPriority=0,this.expirationTimes=ds(-1),this.entangledLanes=this.shellSuspendCounter=this.errorRecoveryDisabledLanes=this.expiredLanes=this.warmLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=ds(0),this.hiddenUpdates=ds(null),this.identifierPrefix=i,this.onUncaughtError=n,this.onCaughtError=a,this.onRecoverableError=s,this.pooledCache=null,this.pooledCacheLanes=0,this.formState=l,this.incompleteTransitions=new Map}function hp(e,t,o,i,n,a,s,r,l,h,v,C){return e=new Qh(e,t,o,s,l,h,v,C,r),t=1,a===!0&&(t|=24),a=wt(3,null,null,t),e.current=a,a.stateNode=e,t=_s(),t.refCount++,e.pooledCache=t,t.refCount++,a.memoizedState={element:i,isDehydrated:o,cache:t},$s(a),e}function mp(e){return e?(e=wi,e):wi}function fp(e,t,o,i,n,a){n=mp(n),i.context===null?i.context=n:i.pendingContext=n,i=wo(t),i.payload={element:o},a=a===void 0?null:a,a!==null&&(i.callback=a),o=Co(e,i,t),o!==null&&(yt(o,e,t),wn(o,e,t))}function yp(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var o=e.retryLane;e.retryLane=o!==0&&o<t?o:t}}function yl(e,t){yp(e,t),(e=e.alternate)&&yp(e,t)}function bp(e){if(e.tag===13||e.tag===31){var t=Oo(e,67108864);t!==null&&yt(t,e,67108864),yl(e,67108864)}}function vp(e){if(e.tag===13||e.tag===31){var t=At();t=ps(t);var o=Oo(e,t);o!==null&&yt(o,e,t),yl(e,t)}}var os=!0;function Hh(e,t,o,i){var n=b.T;b.T=null;var a=E.p;try{E.p=2,bl(e,t,o,i)}finally{E.p=a,b.T=n}}function Fh(e,t,o,i){var n=b.T;b.T=null;var a=E.p;try{E.p=8,bl(e,t,o,i)}finally{E.p=a,b.T=n}}function bl(e,t,o,i){if(os){var n=vl(i);if(n===null)nl(e,t,i,is,o),Cp(e,i);else if(_h(n,e,t,o,i))i.stopPropagation();else if(Cp(e,i),t&4&&-1<Kh.indexOf(e)){for(;n!==null;){var a=ci(n);if(a!==null)switch(a.tag){case 3:if(a=a.stateNode,a.current.memoizedState.isDehydrated){var s=Tt(a.pendingLanes);if(s!==0){var r=a;for(r.pendingLanes|=2,r.entangledLanes|=2;s;){var l=1<<31-Ye(s);r.entanglements[1]|=l,s&=~l}Qt(a),(ge&6)===0&&(Ua=rt()+500,Vn(0))}}break;case 31:case 13:r=Oo(a,2),r!==null&&yt(r,a,2),Ba(),yl(a,2)}if(a=vl(i),a===null&&nl(e,t,i,is,o),a===n)break;n=a}n!==null&&i.stopPropagation()}else nl(e,t,i,null,o)}}function vl(e){return e=Cs(e),wl(e)}var is=null;function wl(e){if(is=null,e=li(e),e!==null){var t=L(e);if(t===null)e=null;else{var o=t.tag;if(o===13){if(e=D(t),e!==null)return e;e=null}else if(o===31){if(e=R(t),e!==null)return e;e=null}else if(o===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null)}}return is=e,null}function wp(e){switch(e){case"beforetoggle":case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"toggle":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 2;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 8;case"message":switch(Kn()){case _n:return 2;case $i:return 8;case x:case A:return 32;case j:return 268435456;default:return 32}default:return 32}}var Cl=!1,zo=null,Do=null,Go=null,On=new Map,Wn=new Map,jo=[],Kh="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset".split(" ");function Cp(e,t){switch(e){case"focusin":case"focusout":zo=null;break;case"dragenter":case"dragleave":Do=null;break;case"mouseover":case"mouseout":Go=null;break;case"pointerover":case"pointerout":On.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Wn.delete(t.pointerId)}}function Qn(e,t,o,i,n,a){return e===null||e.nativeEvent!==a?(e={blockedOn:t,domEventName:o,eventSystemFlags:i,nativeEvent:a,targetContainers:[n]},t!==null&&(t=ci(t),t!==null&&bp(t)),e):(e.eventSystemFlags|=i,t=e.targetContainers,n!==null&&t.indexOf(n)===-1&&t.push(n),e)}function _h(e,t,o,i,n){switch(t){case"focusin":return zo=Qn(zo,e,t,o,i,n),!0;case"dragenter":return Do=Qn(Do,e,t,o,i,n),!0;case"mouseover":return Go=Qn(Go,e,t,o,i,n),!0;case"pointerover":var a=n.pointerId;return On.set(a,Qn(On.get(a)||null,e,t,o,i,n)),!0;case"gotpointercapture":return a=n.pointerId,Wn.set(a,Qn(Wn.get(a)||null,e,t,o,i,n)),!0}return!1}function Sp(e){var t=li(e.target);if(t!==null){var o=L(t);if(o!==null){if(t=o.tag,t===13){if(t=D(o),t!==null){e.blockedOn=t,Vl(e.priority,function(){vp(o)});return}}else if(t===31){if(t=R(o),t!==null){e.blockedOn=t,Vl(e.priority,function(){vp(o)});return}}else if(t===3&&o.stateNode.current.memoizedState.isDehydrated){e.blockedOn=o.tag===3?o.stateNode.containerInfo:null;return}}}e.blockedOn=null}function ns(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var o=vl(e.nativeEvent);if(o===null){o=e.nativeEvent;var i=new o.constructor(o.type,o);ws=i,o.target.dispatchEvent(i),ws=null}else return t=ci(o),t!==null&&bp(t),e.blockedOn=o,!1;t.shift()}return!0}function xp(e,t,o){ns(e)&&o.delete(t)}function Xh(){Cl=!1,zo!==null&&ns(zo)&&(zo=null),Do!==null&&ns(Do)&&(Do=null),Go!==null&&ns(Go)&&(Go=null),On.forEach(xp),Wn.forEach(xp)}function as(e,t){e.blockedOn===t&&(e.blockedOn=null,Cl||(Cl=!0,M.unstable_scheduleCallback(M.unstable_NormalPriority,Xh)))}var ss=null;function kp(e){ss!==e&&(ss=e,M.unstable_scheduleCallback(M.unstable_NormalPriority,function(){ss===e&&(ss=null);for(var t=0;t<e.length;t+=3){var o=e[t],i=e[t+1],n=e[t+2];if(typeof i!="function"){if(wl(i||o)===null)continue;break}var a=ci(o);a!==null&&(e.splice(t,3),t-=3,vr(a,{pending:!0,data:n,method:o.method,action:i},i,n))}}))}function Wi(e){function t(l){return as(l,e)}zo!==null&&as(zo,e),Do!==null&&as(Do,e),Go!==null&&as(Go,e),On.forEach(t),Wn.forEach(t);for(var o=0;o<jo.length;o++){var i=jo[o];i.blockedOn===e&&(i.blockedOn=null)}for(;0<jo.length&&(o=jo[0],o.blockedOn===null);)Sp(o),o.blockedOn===null&&jo.shift();if(o=(e.ownerDocument||e).$$reactFormReplay,o!=null)for(i=0;i<o.length;i+=3){var n=o[i],a=o[i+1],s=n[dt]||null;if(typeof a=="function")s||kp(o);else if(s){var r=null;if(a&&a.hasAttribute("formAction")){if(n=a,s=a[dt]||null)r=s.formAction;else if(wl(n)!==null)continue}else r=s.action;typeof r=="function"?o[i+1]=r:(o.splice(i,3),i-=3),kp(o)}}}function Ap(){function e(a){a.canIntercept&&a.info==="react-transition"&&a.intercept({handler:function(){return new Promise(function(s){return n=s})},focusReset:"manual",scroll:"manual"})}function t(){n!==null&&(n(),n=null),i||setTimeout(o,20)}function o(){if(!i&&!navigation.transition){var a=navigation.currentEntry;a&&a.url!=null&&navigation.navigate(a.url,{state:a.getState(),info:"react-transition",history:"replace"})}}if(typeof navigation=="object"){var i=!1,n=null;return navigation.addEventListener("navigate",e),navigation.addEventListener("navigatesuccess",t),navigation.addEventListener("navigateerror",t),setTimeout(o,100),function(){i=!0,navigation.removeEventListener("navigate",e),navigation.removeEventListener("navigatesuccess",t),navigation.removeEventListener("navigateerror",t),n!==null&&(n(),n=null)}}}function Sl(e){this._internalRoot=e}rs.prototype.render=Sl.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(m(409));var o=t.current,i=At();fp(o,i,e,t,null,null)},rs.prototype.unmount=Sl.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;fp(e.current,2,null,e,null,null),Ba(),t[ri]=null}};function rs(e){this._internalRoot=e}rs.prototype.unstable_scheduleHydration=function(e){if(e){var t=jl();e={blockedOn:null,target:e,priority:t};for(var o=0;o<jo.length&&t!==0&&t<jo[o].priority;o++);jo.splice(o,0,e),o===0&&Sp(e)}};var Ep=Z.version;if(Ep!=="19.2.0")throw Error(m(527,Ep,"19.2.0"));E.findDOMNode=function(e){var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(m(188)):(e=Object.keys(e).join(","),Error(m(268,e)));return e=k(t),e=e!==null?re(e):null,e=e===null?null:e.stateNode,e};var Zh={bundleType:0,version:"19.2.0",rendererPackageName:"react-dom",currentDispatcherRef:b,reconcilerVersion:"19.2.0"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var ls=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!ls.isDisabled&&ls.supportsFiber)try{q=ls.inject(Zh),Q=ls}catch{}}return Fn.createRoot=function(e,t){if(!K(e))throw Error(m(299));var o=!1,i="",n=Du,a=Gu,s=ju;return t!=null&&(t.unstable_strictMode===!0&&(o=!0),t.identifierPrefix!==void 0&&(i=t.identifierPrefix),t.onUncaughtError!==void 0&&(n=t.onUncaughtError),t.onCaughtError!==void 0&&(a=t.onCaughtError),t.onRecoverableError!==void 0&&(s=t.onRecoverableError)),t=hp(e,1,!1,null,null,o,i,null,n,a,s,Ap),e[ri]=t.current,il(e),new Sl(t)},Fn.hydrateRoot=function(e,t,o){if(!K(e))throw Error(m(299));var i=!1,n="",a=Du,s=Gu,r=ju,l=null;return o!=null&&(o.unstable_strictMode===!0&&(i=!0),o.identifierPrefix!==void 0&&(n=o.identifierPrefix),o.onUncaughtError!==void 0&&(a=o.onUncaughtError),o.onCaughtError!==void 0&&(s=o.onCaughtError),o.onRecoverableError!==void 0&&(r=o.onRecoverableError),o.formState!==void 0&&(l=o.formState)),t=hp(e,1,!0,t,o??null,i,n,l,a,s,r,Ap),t.context=mp(null),o=t.current,i=At(),i=ps(i),n=wo(i),n.callback=null,Co(o,n,i),o=i,t.current.lanes=o,tn(t,o),Qt(t),e[ri]=t.current,il(e),new rs(t)},Fn.version="19.2.0",Fn}var Vp;function rm(){if(Vp)return Al.exports;Vp=1;function M(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(M)}catch(Z){console.error(Z)}}return M(),Al.exports=sm(),Al.exports}var lm=rm();const Et=[{id:0,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your organization needs to manage multiple teams, each with several projects. The finance team requires cost tracking per team, and the security team needs to enforce policies at the team level. What resource hierarchy should you implement?",options:["Organization > Projects (no folders) > Resources","Multiple Organizations, one per team, linked via VPC peering","Individual Google accounts per team with shared billing","Organization > Folders (per team) > Projects > Resources"],correct:3,explanation:"Using folders per team allows cost tracking via labels/billing filters and enables policy enforcement at the folder level. This follows Google's recommended practice for enterprise resource management.",wrongExplanations:{1:"Multiple organizations cannot be linked through VPC peering and defeats the purpose of centralized management. Each organization would require separate billing and policy management.",2:"Without folders, you lose the ability to apply department-level policies and organizational structure. This makes management difficult as the number of projects grows.",3:"Individual Google accounts lack the enterprise features needed for centralized management, policy inheritance, and organizational controls required at scale."}},{id:1,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects and accounts - Gemini Cloud Assist",question:"You need to analyze your project's resource configuration to identify unused resources and security vulnerabilities. You want to use AI-powered assistance for this task. What should you do?",options:["Use Cloud Console to manually review each resource type","Configure Cloud Asset Inventory and use Gemini Cloud Assist to analyze resources","Export all resources to BigQuery and write SQL queries","Write custom scripts to query all APIs and analyze resource configurations"],correct:1,explanation:"Cloud Asset Inventory combined with Gemini Cloud Assist provides AI-powered analysis of your resources, identifying optimization opportunities, unused resources, and potential security issues. This is a new feature emphasized in the 2025 exam.",wrongExplanations:{1:"Custom scripts require significant development effort, are prone to errors, and lack the AI-powered insights that Gemini Cloud Assist provides automatically.",2:"Manual review doesn't scale, is time-consuming, and won't provide the intelligent recommendations that Gemini Cloud Assist offers.",3:"While BigQuery can store resource data, you'd still need to write complex queries and lack the AI-powered recommendations that Gemini provides."}},{id:2,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing configuration",question:"Your project needs to prevent cost overruns due to accidental resource provisioning. The finance team requires notifications at 50%, 75%, and 100% of the budget. What is the most effective approach?",options:["Set a spending limit on the credit card associated with the billing account","Monitor spending daily and manually stop resources when approaching budget","Set up billing budgets with alerts at 50%, 75%, and 100%, and configure quotas for compute resources","Create a Cloud Function to check costs hourly and delete resources if over budget"],correct:2,explanation:"Billing budgets provide proactive notifications at specified thresholds. Combined with quotas, this prevents accidental resource overprovisioning. Budgets alert but don't stop billing; quotas prevent resource creation beyond limits.",wrongExplanations:{1:"Manual monitoring doesn't scale, is error-prone, and won't provide timely alerts when costs spike unexpectedly.",2:"Credit card limits don't prevent resource creation; your card will incur charges and Google will bill you. This doesn't provide the granular control needed.",3:"Automatically deleting resources when over budget could cause production outages. Budgets with alerts allow informed decision-making before taking action."}},{id:3,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - GKE Autopilot",question:"You need to deploy a microservices application on GKE with minimal operational overhead. The application has variable traffic patterns requiring automatic scaling. You want Google to manage the cluster infrastructure. What should you do?",options:["Deploy individual Compute Engine instances and install Kubernetes manually","Deploy a Standard GKE cluster and manually configure node pools with autoscaling","Deploy a GKE Autopilot cluster and configure Horizontal Pod Autoscaler for your services","Use Cloud Run instead as it's always the better choice for microservices"],correct:2,explanation:"GKE Autopilot provides a fully managed Kubernetes experience where Google manages nodes, scaling, and security. Combined with HPA, it automatically handles variable traffic with minimal operational overhead.",wrongExplanations:{1:"Standard GKE requires you to manage node pools, upgrades, and scaling configurations. This increases operational overhead compared to Autopilot.",2:"Manual Kubernetes installation requires significant expertise and ongoing maintenance. It's the most operationally intensive option.",3:"Cloud Run is great for stateless services but doesn't provide the same Kubernetes ecosystem benefits, custom networking, or stateful workload support that GKE offers."}},{id:4,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data storage - Database selection",question:"You're migrating a PostgreSQL database that supports a financial analytics application with complex queries. The application requires strong consistency, ACID transactions, and SQL support. Which database should you choose?",options:["Cloud Spanner with PostgreSQL interface","BigQuery with streaming inserts","Firestore in Datastore mode","Cloud SQL for PostgreSQL with high availability configuration"],correct:3,explanation:"Cloud SQL for PostgreSQL provides a managed PostgreSQL service with ACID transactions, strong consistency, and full SQL support. HA configuration ensures reliability for production workloads.",wrongExplanations:{1:"Firestore is a NoSQL database that doesn't support complex SQL queries or PostgreSQL compatibility. It's designed for document-based, real-time applications.",2:"BigQuery is an analytics data warehouse, not an OLTP database. It's optimized for analytical queries, not transactional workloads with frequent updates.",3:"Spanner is for globally distributed, horizontally scalable workloads. It's more expensive and complex than needed for a PostgreSQL migration focused on analytics."}},{id:5,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Load balancers",question:"Your web application serves global users and requires SSL termination, URL-based routing to different backend services, and Cloud CDN integration. Which load balancer should you use?",options:["Cloud Run with built-in load balancing","Network Load Balancer (TCP/UDP)","External Application Load Balancer (HTTP(S) Load Balancer)","Internal Application Load Balancer"],correct:2,explanation:"External Application Load Balancer provides Layer 7 load balancing with SSL termination, URL-based routing, and native Cloud CDN integration for global content delivery.",wrongExplanations:{1:"Network Load Balancer operates at Layer 4 (TCP/UDP) and doesn't provide URL-based routing or native Cloud CDN integration. It's for non-HTTP workloads.",2:"Internal Application Load Balancer is for traffic within your VPC, not for serving external users. It's used for internal microservices communication.",3:"While Cloud Run has load balancing, it doesn't give you the same control over routing rules, backend service management, and CDN integration for complex applications."}},{id:6,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - VPC design",question:"You need to connect your on-premises data center to GCP with predictable bandwidth and low latency. The connection must be private and not traverse the public internet. What should you use?",options:["Dedicated Interconnect or Partner Interconnect depending on location","Direct peering with Google","Cloud VPN with high-bandwidth tunnels","Public internet with VPN encryption"],correct:0,explanation:"Dedicated or Partner Interconnect provides private, high-bandwidth, low-latency connections between on-premises and GCP without using the public internet. Choose based on proximity to Google facilities.",wrongExplanations:{1:"Cloud VPN traverses the public internet (encrypted), which doesn't meet the 'not traverse public internet' requirement and has variable latency.",2:"Direct peering is for accessing Google services, not for private connections to your VPC. It doesn't provide the private connectivity needed.",3:"Public internet explicitly violates the requirement. Even with VPN encryption, traffic still goes over the public internet with variable performance."}},{id:7,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud NGFW",question:"You need to implement advanced network security for your VPC that includes intrusion detection, threat prevention, and URL filtering. What should you configure?",options:["Network tags with hierarchical firewall policies","Cloud Armor with custom security policies","VPC firewall rules with deny rules for specific IP ranges","Cloud Next Generation Firewall (Cloud NGFW) with threat prevention profiles"],correct:3,explanation:"Cloud NGFW provides advanced security features including IDS/IPS, threat prevention, URL filtering, and TLS inspection - capabilities not available in standard VPC firewall rules.",wrongExplanations:{1:"VPC firewall rules provide basic allow/deny based on IP, port, and protocol but lack intrusion detection, threat prevention, and URL filtering capabilities.",2:"Cloud Armor protects against DDoS and web attacks at Layer 7 (HTTP/HTTPS) but doesn't provide network-level intrusion detection or URL filtering for all traffic types.",3:"Network tags with firewall policies improve organization but don't add advanced security features like threat detection and prevention."}},{id:8,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Infrastructure as code - Fabric FAST",question:"Your enterprise needs to deploy a multi-environment, multi-project GCP foundation following Google's best practices for large organizations. What tool should you use?",options:["Deploy manually through Cloud Console and export configurations","Write custom Terraform modules from scratch","Use gcloud commands in shell scripts","Cloud Foundation Toolkit's Fabric FAST framework"],correct:3,explanation:"Fabric FAST (Fabric is Abstractions for Service and Transfer) is specifically designed for enterprise GCP foundations, providing pre-built, tested patterns for multi-environment deployments following Google's best practices.",wrongExplanations:{1:"Custom Terraform requires significant development time, testing, and may not follow Google's best practices. Fabric FAST provides proven patterns.",2:"gcloud scripts are imperative and difficult to maintain. They don't provide the declarative infrastructure management needed for enterprise environments.",3:"Manual deployment is error-prone, not reproducible, and doesn't scale. Exporting configurations doesn't capture all settings and requires manual synchronization."}},{id:9,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE Autopilot",question:"Your application running on GKE Autopilot is experiencing OOMKilled errors. You need to adjust Pod resource requests. What should you do?",options:["Migrate to Standard GKE where you have more control","Scale up the node pool to have larger machines","Contact Google Cloud Support to increase node capacity","Update the Pod specification's resource requests for memory and redeploy"],correct:3,explanation:"In GKE Autopilot, you configure resource requests in your Pod specs. Autopilot automatically provisions appropriate nodes. You don't manage nodes directly - Google handles that based on your Pod requirements.",wrongExplanations:{1:"In Autopilot, you don't manage node pools directly. Google automatically provisions nodes based on Pod resource requests. Trying to scale node pools won't work.",2:"Google manages node capacity in Autopilot automatically. You don't need to contact support - simply update your Pod resource requests and Autopilot handles provisioning.",3:"Migrating to Standard GKE for this issue is unnecessary. Autopilot can handle resource adjustments through Pod specifications - you just need to update them correctly."}},{id:10,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Database Center",question:"You have multiple database instances across Cloud SQL, AlloyDB, and Spanner. You need a centralized view of database health, performance, and recommendations. What should you use?",options:["Create custom Cloud Monitoring dashboards for each database type","Set up separate monitoring tools for Cloud SQL, AlloyDB, and Spanner","Use gcloud commands to query each database service separately","Database Center to manage and monitor your Google Cloud database fleet"],correct:3,explanation:"Database Center provides a unified interface for managing all Google Cloud databases (Cloud SQL, AlloyDB, Spanner, Firestore) with health monitoring, performance insights, and optimization recommendations.",wrongExplanations:{1:"Custom dashboards require manual setup for each database and don't provide the unified insights and recommendations that Database Center offers automatically.",2:"gcloud commands require scripting and manual aggregation. You won't get the centralized view, health monitoring, and AI-powered recommendations Database Center provides.",3:"Separate monitoring tools increase complexity, costs, and maintenance. Database Center provides native integration with all Google Cloud database services."}},{id:11,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring and logging - Ops Agent",question:"You need to collect metrics and logs from your Compute Engine instances for Cloud Monitoring and Cloud Logging. What is the recommended approach?",options:["Install and configure the Ops Agent on your instances","Use the legacy Stackdriver Logging and Monitoring agents","Write custom scripts to push logs via the Cloud Logging API","Manually export logs to Cloud Storage and import to Cloud Logging"],correct:0,explanation:"Ops Agent is the recommended unified agent that replaces legacy Stackdriver agents. It provides optimized collection of metrics and logs with better performance and easier configuration.",wrongExplanations:{1:"Legacy Stackdriver agents are deprecated. Ops Agent provides better performance, simpler configuration, and is the current recommended solution.",2:"Custom scripts add maintenance overhead and complexity. Ops Agent handles metrics and logs collection automatically with built-in integrations.",3:"Manual export/import is inefficient and loses real-time monitoring capabilities. Ops Agent provides automated, real-time log and metric collection."}},{id:12,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring and logging - Error Reporting",question:"Your application deployed on Cloud Run is experiencing intermittent errors. You want to automatically group similar errors and receive notifications. What should you use?",options:["Query Cloud Logging manually to find error patterns","Error Reporting to automatically group and track errors with Cloud Logging integration","Set up custom log-based metrics in Cloud Logging","Use Cloud Trace to identify errors"],correct:1,explanation:"Error Reporting automatically groups similar errors from Cloud Logging, provides real-time notifications, shows error trends, and integrates with alerting - perfect for tracking application errors.",wrongExplanations:{1:"Log-based metrics require manual configuration and don't provide automatic error grouping or detailed error analysis that Error Reporting offers.",2:"Manual querying is time-consuming, reactive, and doesn't provide automatic grouping or notifications when new errors occur.",3:"Cloud Trace is for distributed tracing and latency analysis, not error detection and grouping. It's used to understand request flow, not aggregate errors."}},{id:13,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Cloud Run - Secrets management",question:"Your Cloud Run service needs to access a database password. How should you securely provide this credential?",options:["Pass the password as a query parameter in the Cloud Run URL","Store the password in a Cloud Storage bucket and read it at startup","Store the password in Secret Manager and mount it as an environment variable in Cloud Run","Hard-code the password in the container image"],correct:2,explanation:"Secret Manager provides secure storage for sensitive data with encryption, versioning, and audit logging. Cloud Run can directly access secrets as environment variables or volume mounts.",wrongExplanations:{1:"Hard-coding passwords in container images exposes them to anyone with access to the image. Images should never contain secrets.",2:"Query parameters are logged and visible in URLs. This is extremely insecure for any sensitive data.",3:"While better than hard-coding, Cloud Storage doesn't provide the specialized secret management features like rotation, versioning, and audit logging that Secret Manager offers."}},{id:14,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Compute Engine",question:"You need to perform maintenance on a Compute Engine instance without losing its ephemeral IP address. What should you do?",options:["Take a snapshot and restore to a new instance","Stop the instance, perform maintenance, then start it again","Delete and recreate the instance with the same configuration","Use live migration while the instance is running"],correct:1,explanation:"Stopping and starting an instance preserves its ephemeral IP address. The instance retains the same internal and external IPs when restarted.",wrongExplanations:{1:"Deleting and recreating an instance assigns new IP addresses. The ephemeral IP is released when the instance is deleted.",2:"Live migration is for Google's maintenance events, not for user-initiated maintenance. You can't trigger it manually for your own maintenance tasks.",3:"Restoring to a new instance creates a different instance with different IP addresses. The original IPs aren't transferred."}},{id:15,domain:"Configuring access and security",subdomain:"4.1 IAM - Least privilege",question:"A data analyst needs read-only access to BigQuery datasets to run queries but should not be able to create or delete datasets. What IAM role should you grant?",options:["roles/bigquery.dataViewer at the dataset level","roles/bigquery.admin with conditions limiting to read operations","roles/viewer at the project level","roles/bigquery.user at the project level"],correct:0,explanation:"bigquery.dataViewer provides read-only access to table data and metadata at the dataset level, following least privilege. It doesn't allow resource creation or modification.",wrongExplanations:{1:"Project Viewer grants read access to ALL resources in the project, not just BigQuery. This violates least privilege by providing unnecessary permissions.",2:"bigquery.user allows running queries and creating jobs, which could include INSERT/UPDATE/DELETE operations. It provides more permissions than needed for read-only access.",3:"bigquery.admin grants full BigQuery permissions including creating and deleting resources. IAM conditions don't effectively restrict this to read-only operations."}},{id:16,domain:"Configuring access and security",subdomain:"4.1 IAM - Organization policies",question:"You need to prevent all users in your organization from creating external IP addresses on Compute Engine instances to improve security. What should you do?",options:["Remove the Compute Instance Admin role from all users","Manually review and delete external IPs daily","Set the 'constraints/compute.vmExternalIpAccess' organization policy to deny all","Use VPC firewall rules to block all external traffic"],correct:2,explanation:"Organization policies provide centralized control over resource configurations. The vmExternalIpAccess constraint prevents creation of instances with external IPs across the entire organization.",wrongExplanations:{1:"Removing roles prevents users from creating any instances, not just preventing external IPs. This is too restrictive and breaks legitimate use cases.",2:"Firewall rules control traffic flow but don't prevent the assignment of external IP addresses. Instances can still have external IPs even if firewall blocks traffic.",3:"Manual review is reactive, doesn't scale, and allows windows where insecure configurations exist. Organization policies prevent the issue proactively."}},{id:17,domain:"Configuring access and security",subdomain:"4.1 IAM - Custom roles",question:"You need to create a role that allows starting and stopping Compute Engine instances but not creating or deleting them. What should you do?",options:["Grant Compute Admin role with IAM conditions","Use the predefined Compute Instance Admin role","Use the Compute Viewer role","Create a custom role with compute.instances.start and compute.instances.stop permissions"],correct:3,explanation:"Custom roles allow you to bundle specific permissions for precise access control. Including only start and stop permissions follows least privilege.",wrongExplanations:{1:"Compute Instance Admin includes create and delete permissions, violating the requirement. It grants far more access than needed.",2:"IAM conditions work on resource attributes (like resource names or tags), not on specific operations. You can't use conditions to limit Admin role to only start/stop.",3:"Compute Viewer only allows reading instance information, not starting or stopping them. It doesn't provide the needed operational permissions."}},{id:18,domain:"Configuring access and security",subdomain:"4.1 IAM - Service accounts",question:"Your Cloud Function needs to write data to BigQuery. Following best practices, how should you configure access?",options:["Use the default App Engine service account","Use your personal user account","Grant the function Owner role for simplicity","Create a dedicated service account with only BigQuery Data Editor role and assign it to the function"],correct:3,explanation:"Dedicated service accounts with minimal required permissions (least privilege) are the best practice. BigQuery Data Editor allows writing data without unnecessary permissions.",wrongExplanations:{1:"Default App Engine service account has Editor permissions across the project - far more than needed. This violates least privilege principle.",2:"Personal user accounts shouldn't be used for services. This breaks automation and creates security and auditing issues.",3:"Owner role grants full control over all resources - massive security risk. Always follow least privilege by granting only required permissions."}},{id:19,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Best practices",question:"You are deploying a Cloud Function that needs to write to Cloud Storage and publish to Pub/Sub. How should you configure the service account?",options:["Use the default App Engine service account","Use your personal account to deploy the function","Create a dedicated service account with only Storage Object Creator and Pub/Sub Publisher roles","Create a service account with Owner role for simplicity"],correct:2,explanation:"A dedicated service account with only required permissions (Storage Object Creator and Pub/Sub Publisher) follows least privilege and provides clear audit trails for the function's actions.",wrongExplanations:{1:"Default App Engine service account has Editor-level permissions on the project - far more than needed. This violates least privilege.",2:"Owner role grants full control over all resources, creating massive security risks. If the function is compromised, attackers have complete project access.",3:"Personal accounts break automation, make auditing difficult, and tie permissions to a specific person rather than the service requirement."}},{id:20,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"Your application deployment fails with a 'QUOTA_EXCEEDED' error when creating Compute Engine instances. You need more resources immediately. What should you do?",options:["Request a quota increase through the Cloud Console quotas page for your project","Delete existing resources to free up quota","Contact Google Cloud Sales to purchase more quota","Create a new project and deploy there"],correct:0,explanation:"Quota increases can be requested through the Cloud Console. For immediate needs, explain the business justification. Most requests are approved quickly, especially for established accounts.",wrongExplanations:{1:"Creating a new project resets quotas to default limits, which might not solve the problem. You'd also lose the existing project's configuration and resources.",2:"Quota isn't purchased - it's requested based on need. Sales can help with very large increases, but standard increases go through the Console quota request process.",3:"Deleting resources only helps if you're genuinely over-provisioned. If you need more capacity for legitimate use, request a quota increase instead."}},{id:21,domain:"Setting up a cloud solution environment",subdomain:"1.1 Cloud Identity - User management",question:"Your organization has 500 employees, and you need to provision Google Cloud access for all of them using your existing Active Directory. What should you do?",options:["Have each user create their own Gmail account","Manually create Google accounts for each user","Use a single shared Google account for all users","Set up Cloud Identity with Google Cloud Directory Sync (GCDS) to synchronize users from Active Directory"],correct:3,explanation:"Cloud Identity with GCDS automatically syncs users and groups from Active Directory to Cloud Identity, enabling centralized management and SSO. This is the enterprise solution for user management.",wrongExplanations:{1:"Manual creation doesn't scale, is error-prone, and creates ongoing maintenance burden. When users join/leave, you must manually update Google Cloud.",2:"Individual Gmail accounts don't integrate with enterprise systems, make auditing difficult, and can't be centrally managed. This isn't suitable for enterprise use.",3:"Shared accounts completely break access control, auditing, and security. You can't track who did what or apply least privilege principles."}},{id:22,domain:"Setting up a cloud solution environment",subdomain:"1.2 Billing - Budget alerts",question:"You set up a billing budget with a 100% threshold alert, but you're still exceeding your budget. What's the likely issue?",options:["Budget alerts only notify; they don't stop spending. You need to implement automated responses or manual intervention","The alert wasn't properly configured","Google Cloud doesn't enforce budgets","Budget limits automatically stop resource creation"],correct:0,explanation:"Billing budgets are informational only - they send alerts but don't prevent spending. You must take action (manual or automated via Cloud Functions/Pub/Sub) to control costs.",wrongExplanations:{1:"While misconfiguration is possible, the more common issue is expecting budgets to enforce limits. Budget alerts work even if they don't stop spending.",2:"Budget limits don't automatically stop anything. Google Cloud will continue to bill you for resources you create regardless of budget settings.",3:"Google Cloud does track spending and send budget alerts, but budgets are advisory, not enforced limits. You remain responsible for managing your costs."}},{id:23,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Compute - Preemptible VMs",question:"You're running a batch processing job that can tolerate interruptions. You want to minimize costs. What type of Compute Engine instance should you use?",options:["Instance templates with autoscaling","Standard VMs with sustained use discounts","Committed use contracts for 1-year term","Spot VMs (formerly Preemptible VMs) which offer significant discounts for interruptible workloads"],correct:3,explanation:"Spot VMs offer up to 91% discount compared to standard instances. They're perfect for fault-tolerant, batch processing workloads that can handle interruptions and restarts.",wrongExplanations:{1:"Sustained use discounts apply automatically but don't provide as much savings as Spot VMs. For interruptible batch jobs, Spot VMs are more cost-effective.",2:"Committed use contracts require long-term commitment and don't provide as much discount as Spot VMs. They're better for predictable, continuous workloads.",3:"Autoscaling helps with variable load but doesn't reduce per-instance costs. For batch jobs that can tolerate interruptions, Spot VMs are the cost-optimization choice."}},{id:24,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Cloud SQL - High availability",question:"Your production database on Cloud SQL needs to survive a zonal failure with minimal downtime. What should you configure?",options:["Export data to Cloud Storage regularly","Create manual backups hourly","Use read replicas in multiple zones","Enable high availability (HA) configuration which creates a standby replica in another zone"],correct:3,explanation:"Cloud SQL HA configuration maintains a standby instance in another zone with synchronous replication. Automatic failover occurs if the primary zone fails, minimizing downtime.",wrongExplanations:{1:"Backups help with data recovery but don't provide automatic failover. Restoring from backup takes time and results in significant downtime.",2:"Read replicas are for read scaling, not automatic failover. They use asynchronous replication and aren't promoted automatically during failures.",3:"Exports to Cloud Storage help with disaster recovery but don't provide high availability or automatic failover. Recovery time would be lengthy."}},{id:25,domain:"Planning and implementing a cloud solution",subdomain:"2.2 BigQuery - Cost optimization",question:"Your BigQuery queries are expensive because they scan entire tables. Most queries only need data from the last 30 days. How can you reduce costs?",options:["Create separate tables for each day","Use clustering only","Export old data to Cloud Storage","Partition the table by date and use WHERE clauses on the partition column"],correct:3,explanation:"Partitioning by date allows BigQuery to prune unnecessary data. Queries with WHERE clauses on the partition column only scan relevant partitions, dramatically reducing costs.",wrongExplanations:{1:"Exporting data loses BigQuery's query capabilities and creates management overhead. Partitioning keeps data in BigQuery while reducing scan costs.",2:"Separate daily tables create management complexity and make cross-day queries difficult. Partitioned tables are the native solution for time-based data.",3:"Clustering alone doesn't eliminate unnecessary data scanning. Partitioning is required to physically separate data and prune partitions at query time."}},{id:26,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Cloud Load Balancing - SSL certificates",question:"You're setting up an HTTPS load balancer and need to manage SSL certificates with automatic renewal. What should you use?",options:["Self-signed certificates","Google-managed SSL certificates which automatically provision and renew certificates","Certificates purchased from a third-party CA and manually uploaded","Manual Let's Encrypt certificates uploaded to Cloud Load Balancing"],correct:1,explanation:"Google-managed certificates automatically provision (via Let's Encrypt) and renew SSL certificates for your domains. This eliminates manual certificate management overhead.",wrongExplanations:{1:"Self-signed certificates trigger browser warnings and aren't trusted by clients. They're only suitable for testing, never production.",2:"Manual Let's Encrypt certificates require you to handle renewal every 90 days. Google-managed certificates automate this process completely.",3:"Third-party certificates cost money and require manual renewal and upload. Google-managed certificates are free and fully automated."}},{id:27,domain:"Planning and implementing a cloud solution",subdomain:"2.3 VPC - Shared VPC",question:"Your organization has multiple projects that need to communicate privately and share network resources. What networking architecture should you implement?",options:["Public IP addresses for cross-project communication","VPC Network Peering between all projects","Separate VPCs with Cloud VPN connections","Shared VPC where a host project shares VPC networks with service projects"],correct:3,explanation:"Shared VPC allows centralized network administration where service projects can use networks from a host project. This simplifies management and maintains private connectivity.",wrongExplanations:{1:"VPC Peering works but creates a mesh topology that becomes complex with many projects. Shared VPC provides better centralized management for organizations.",2:"Cloud VPN is for connecting to on-premises or other clouds, not for internal GCP project connectivity. It adds unnecessary complexity and costs.",3:"Public IPs expose services to the internet and incur egress charges. Private connectivity through Shared VPC is more secure and cost-effective."}},{id:28,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Terraform - State management",question:"You're using Terraform to manage GCP infrastructure. Where should you store the Terraform state file for a production environment?",options:["Cloud Storage bucket with versioning enabled and appropriate IAM controls","Store in Cloud SQL database","Local file on your workstation","Commit to Git repository"],correct:0,explanation:"Cloud Storage with versioning provides durable, shared state storage with history. IAM controls limit access. This enables team collaboration and disaster recovery.",wrongExplanations:{1:"Local state files don't allow team collaboration and can be easily lost. They're only suitable for personal testing, never for production or team environments.",2:"State files contain sensitive information (resource IDs, metadata). Committing to Git exposes this data and creates merge conflicts with team collaboration.",3:"Cloud SQL adds unnecessary complexity for Terraform state. Cloud Storage is the recommended backend with built-in versioning and locking support."}},{id:29,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Compute Engine - Live migration",question:"Google announces maintenance for the zone where your Compute Engine instance runs. What happens to your instance by default?",options:["The instance is automatically terminated","Nothing happens; you must manually migrate","The instance is live-migrated to another host in the same zone with no downtime","The instance is moved to another zone"],correct:2,explanation:"Compute Engine live migration moves running instances to different physical hosts during maintenance events. This happens automatically with no downtime for most instance types.",wrongExplanations:{1:"Instances are not terminated during maintenance by default. Live migration keeps them running. Termination only occurs if you've set the maintenance policy to 'TERMINATE'.",2:"Instances stay in the same zone during live migration. Cross-zone migration would change the instance's internal IP and isn't done automatically.",3:"Live migration is automatic for most instances. You don't need to take action unless you're using instance types that don't support live migration (like preemptible VMs)."}},{id:30,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 GKE - Node pools",question:"You need to run both CPU-intensive and memory-intensive workloads on GKE. How should you configure your cluster?",options:["Create multiple node pools with different machine types optimized for each workload, then use node selectors","Create separate clusters for each workload type","Use a single node pool with the largest available machine type","Use Autopilot which doesn't allow custom node configuration"],correct:0,explanation:"Multiple node pools allow workload-optimized machine types. Node selectors or node affinity ensure pods schedule on appropriate nodes, optimizing cost and performance.",wrongExplanations:{1:"Single large machine types waste resources. CPU-intensive workloads don't need excessive memory and vice versa. This increases costs unnecessarily.",2:"Separate clusters add management overhead, increase costs (multiple control planes), and make cross-workload communication more complex.",3:"Autopilot does allow resource specification through Pod requests. However, the question is about Standard GKE where multiple node pools with selectors is the right approach."}},{id:31,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cloud Storage - Lifecycle management",question:"You have log files in Cloud Storage that must be retained for 7 years for compliance but are rarely accessed after 90 days. How should you optimize costs?",options:["Keep everything in Standard storage for compliance","Manually move old files to Archive storage quarterly","Delete files after 90 days and restore from backups if needed","Create a lifecycle policy to move objects to Archive storage after 90 days"],correct:3,explanation:"Lifecycle policies automatically transition objects between storage classes. Archive storage provides lowest cost for long-term retention while maintaining compliance requirements.",wrongExplanations:{1:"Manual processes are error-prone, don't scale, and may miss files. Lifecycle policies automate transitions reliably without manual intervention.",2:"Different storage classes have the same durability and compliance capabilities. Standard storage is much more expensive for rarely-accessed data.",3:"Deleting files violates compliance requirements. Archive storage maintains data for the required retention period at much lower cost than standard storage."}},{id:32,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Cloud Monitoring - Uptime checks",question:"You want to monitor whether your web application is accessible from different global locations. What should you configure?",options:["Set up Compute Engine instances worldwide to test access","Use Cloud Trace to monitor application availability","Write a Cloud Function to ping your application every minute","Cloud Monitoring uptime checks from multiple geographic locations"],correct:3,explanation:"Uptime checks are specifically designed for monitoring endpoint availability from multiple global locations. They integrate with alerting and are included with Cloud Monitoring.",wrongExplanations:{1:"Custom Cloud Functions add unnecessary complexity and costs. Uptime checks provide this functionality natively with better integration and features.",2:"Compute Engine instances for monitoring are expensive and complex to manage. Uptime checks provide the same functionality at lower cost.",3:"Cloud Trace is for distributed tracing and latency analysis, not availability monitoring. It shows request flow, not uptime status."}},{id:33,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Cloud Logging - Log sinks",question:"You need to retain audit logs for 5 years for compliance but Cloud Logging's default retention is 30 days. What should you do?",options:["Manually export logs monthly","Increase Cloud Logging retention to 5 years","Use BigQuery for all logging","Create a log sink to export logs to Cloud Storage with appropriate retention policies"],correct:3,explanation:"Log sinks export logs to Cloud Storage, BigQuery, or Pub/Sub. Cloud Storage with bucket retention policies provides cost-effective long-term log retention for compliance.",wrongExplanations:{1:"Cloud Logging supports custom retention (up to 3650 days/10 years) but it's more expensive than Cloud Storage for long-term retention. Log sinks to Storage are more cost-effective.",2:"Manual export processes are unreliable and may miss logs during system outages or human error. Automated log sinks ensure continuous, reliable export.",3:"BigQuery can store logs but is optimized for queries, not cold storage. Cloud Storage is more cost-effective for compliance retention with infrequent access."}},{id:34,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cloud Storage - Object management",question:"You accidentally deleted critical files from a Cloud Storage bucket. The files were deleted 4 days ago. How can you recover them?",options:["Check the Cloud Storage trash folder","Contact Google Cloud Support to restore from backup","Recover from local backups since Cloud Storage doesn't keep deleted files","List object versions in the bucket and restore the previous version if Object Versioning is enabled"],correct:3,explanation:"If Object Versioning is enabled, deleted objects become non-current versions that can be restored. Without versioning, deleted objects are permanently gone after a short grace period.",wrongExplanations:{1:"Google Cloud Support cannot restore deleted Cloud Storage objects. Storage is customer-managed, and deletions are permanent without versioning.",2:"Cloud Storage doesn't have a trash folder. Once deleted (and the grace period passes), objects are gone unless versioning was enabled.",3:"Without Object Versioning, deleted files cannot be recovered from Cloud Storage. This is why versioning and proper backup strategies are critical."}},{id:35,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Data solutions - BigQuery",question:"Your BigQuery query is returning an error: 'Resources exceeded during query execution'. What should you try first?",options:["Switch to Dataflow for processing","Request a BigQuery quota increase","Add WHERE clauses to limit the data scanned or partition the table","Export data to Cloud Storage and process locally"],correct:2,explanation:"Limiting data scanned through WHERE clauses or partitioning reduces resource usage. Partitioned tables allow BigQuery to prune unnecessary data, dramatically improving performance and staying within limits.",wrongExplanations:{1:"Quota increases don't fix poorly optimized queries. The query is likely scanning too much data unnecessarily. Optimize first before requesting more quota.",2:"Switching to Dataflow is overkill for a query optimization issue. BigQuery can handle large datasets efficiently with proper table design and query optimization.",3:"Exporting data loses BigQuery's power and creates complexity. Fix the query or table structure instead."}},{id:36,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 GKE - Troubleshooting",question:"A Pod in your GKE cluster is stuck in 'CrashLoopBackOff' status. What's the first step to diagnose the issue?",options:["Restart the entire cluster","Check Pod logs using 'kubectl logs <pod-name>' to see application errors","Delete the Pod and recreate it","Scale the deployment to zero and back"],correct:1,explanation:"Pod logs contain application output and error messages that explain why the container is crashing. This is always the first diagnostic step for CrashLoopBackOff issues.",wrongExplanations:{1:"Deleting the Pod might temporarily mask the issue but doesn't solve the underlying problem. The new Pod will likely crash the same way. Investigate first.",2:"Scaling down doesn't provide diagnostic information and is just a different way of deleting Pods. Check logs first to understand why it's crashing.",3:"Restarting the cluster is extremely disruptive and won't fix application-level issues. CrashLoopBackOff indicates a Pod problem, not a cluster problem."}},{id:37,domain:"Configuring access and security",subdomain:"4.1 IAM - Predefined roles",question:"A developer needs to deploy applications to App Engine but should not be able to modify IAM policies. What role should you grant?",options:["Project Owner role","App Engine Deployer role at the project level","Editor role at the project level","App Engine Admin role"],correct:1,explanation:"App Engine Deployer allows deploying applications but not modifying App Engine settings or IAM. This follows least privilege for the developer's needs.",wrongExplanations:{1:"App Engine Admin can modify App Engine settings including IAM policies. This violates the requirement.",2:"Editor role grants broad permissions including IAM modifications across many services. It far exceeds what the developer needs.",3:"Owner role includes full IAM control and billing management. This is the most privileged role and completely violates least privilege here."}},{id:38,domain:"Configuring access and security",subdomain:"4.1 IAM - Resource hierarchy",question:"You granted a user Editor role at the folder level. What access does this provide?",options:["Editor access only to the folder itself, not projects within","Editor access to all projects within that folder and its subfolders","Read-only access to projects in the folder","Editor access to the entire organization"],correct:1,explanation:"IAM permissions are inherited down the resource hierarchy. A role granted at folder level applies to all projects and resources within that folder and its subfolders.",wrongExplanations:{1:"Folders are containers for projects. IAM roles on folders automatically apply to all contained resources through inheritance.",2:"Folder-level roles don't grant organization-level access. They're scoped to that folder and its children.",3:"Editor role grants modification permissions, not read-only. The question asked about Editor role specifically."}},{id:39,domain:"Configuring access and security",subdomain:"4.2 VPC Service Controls",question:"You need to prevent data exfiltration from sensitive BigQuery datasets. What security control should you implement?",options:["IAM conditions limiting access by IP address","VPC firewall rules blocking BigQuery API access","VPC Service Controls to create a security perimeter around the BigQuery project","Cloud Armor security policies"],correct:2,explanation:"VPC Service Controls create security perimeters that prevent data from leaving defined GCP resources, even if someone has IAM permissions. This is specifically designed to prevent exfiltration.",wrongExplanations:{1:"IAM conditions can restrict access by attributes but don't prevent data copying to authorized locations. Someone with proper IAM could still exfiltrate data to another GCP project.",2:"Cloud Armor protects against external web-based attacks but doesn't control data movement between GCP resources or prevent authorized users from copying data.",3:"VPC firewall rules control network traffic to Compute Engine, not API-level access to BigQuery. They don't prevent data exfiltration through BigQuery API."}},{id:40,domain:"Configuring access and security",subdomain:"4.2 Binary Authorization",question:"You want to ensure only container images from your approved registry can be deployed to GKE. What should you implement?",options:["Binary Authorization with policies requiring images from specific registries","Container Analysis API to scan images","VPC Service Controls around GKE","IAM policies restricting who can deploy"],correct:0,explanation:"Binary Authorization enforces deployment policies that can require images to be from specific registries, be signed, or pass vulnerability scans before deployment to GKE.",wrongExplanations:{1:"Container Analysis scans for vulnerabilities but doesn't enforce where images come from or prevent deployment of unapproved images.",2:"VPC Service Controls limit data exfiltration but don't control which container images can be deployed.",3:"IAM controls who can deploy but not what they can deploy. Authorized users could still deploy images from unapproved registries."}},{id:41,domain:"Configuring access and security",subdomain:"4.1 IAM - Conditions",question:"You need to grant a contractor temporary access to Cloud Storage that automatically expires after 30 days. How should you configure this?",options:["Set a calendar reminder to remove permissions","Grant IAM role with a condition that expires after 30 days using temporal constraints","Manually revoke access after 30 days","Use a service account that you'll delete after 30 days"],correct:1,explanation:"IAM conditions support temporal constraints (expiration dates). The permission automatically expires on the specified date without manual intervention.",wrongExplanations:{1:"Manual revocation is error-prone. People forget, get busy, or leave the company. Automated expiration through IAM conditions is more reliable.",2:"Service account deletion is disruptive if the account is being used. IAM conditions allow fine-grained, automatic expiration of specific permissions.",3:"Calendar reminders depend on humans remembering and acting. IAM conditions provide automated, enforced expiration that doesn't rely on manual processes."}},{id:42,domain:"Configuring access and security",subdomain:"4.3 Data encryption - Customer-managed keys",question:"Your compliance requirements mandate control over encryption keys used for Cloud Storage data. What should you use?",options:["Default encryption (Google-managed keys)","Client-side encryption before uploading","Customer-Managed Encryption Keys (CMEK) using Cloud KMS","Customer-Supplied Encryption Keys (CSEK)"],correct:2,explanation:"CMEK with Cloud KMS gives you control over key management while Google handles the cryptographic operations. This balances security, compliance, and operational simplicity.",wrongExplanations:{1:"Google-managed keys don't give you control over key management. While secure, they don't meet compliance requirements for customer control.",2:"CSEK requires you to provide keys with every operation and manage key storage yourself. CMEK with Cloud KMS is easier to manage while meeting compliance needs.",3:"Client-side encryption works but adds application complexity and you lose features like server-side operations. CMEK provides the compliance benefit more simply."}},{id:43,domain:"Configuring access and security",subdomain:"4.4 Audit logging",question:"You need to track all IAM policy changes across your organization for security auditing. What should you use?",options:["Enable Data Access audit logs","Set up custom Cloud Logging sinks","Use Cloud Asset Inventory","Admin Activity audit logs which are always enabled and log IAM changes"],correct:3,explanation:"Admin Activity logs automatically track administrative actions including IAM policy changes. They're enabled by default and don't incur charges.",wrongExplanations:{1:"Custom logging solutions are unnecessary when Admin Activity logs already capture IAM changes. This adds complexity without benefit.",2:"Data Access logs track who accessed data, not who changed IAM policies. Admin Activity logs are the correct choice for IAM changes.",3:"Cloud Asset Inventory tracks resource state but isn't designed for real-time audit logging. Admin Activity logs provide detailed IAM change tracking."}},{id:44,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Key management",question:"You discovered a service account key file in a public GitHub repository. What should you do immediately?",options:["Make the repository private","Delete the service account key in GCP and rotate all credentials","Contact GitHub to remove the file","Change the service account's IAM roles"],correct:1,explanation:"Once exposed, keys must be immediately revoked. Delete the key in GCP, rotate any other keys, and investigate what resources were accessed. Repository changes don't remove the key from Git history.",wrongExplanations:{1:"Making the repository private doesn't remove the key from Git history. Anyone who cloned/forked it already has the key. The key must be deleted.",2:"Changing roles doesn't invalidate the exposed key. The key can still be used with its original permissions until explicitly deleted.",3:"Contacting GitHub won't quickly secure your environment. Even if they remove it, it may be cached/archived elsewhere. Immediate key deletion is critical."}},{id:45,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Config Connector",question:"Your team manages Kubernetes resources and wants to manage GCP resources using the same Kubernetes tools and workflows. What should you use?",options:["gcloud commands in Kubernetes Jobs","Config Connector to manage GCP resources as Kubernetes Custom Resources","Manual GCP resource creation separate from Kubernetes","Terraform with Kubernetes provider"],correct:1,explanation:"Config Connector extends Kubernetes to manage GCP resources as Custom Resources (CRDs), enabling you to use kubectl and GitOps workflows for both Kubernetes and GCP infrastructure.",wrongExplanations:{1:"Terraform with Kubernetes provider manages Kubernetes resources from Terraform, not the reverse. Config Connector manages GCP from Kubernetes.",2:"gcloud in Jobs is imperative and doesn't integrate with Kubernetes' declarative model. Config Connector provides true Kubernetes-native management.",3:"Manual creation defeats infrastructure-as-code principles and creates operational inconsistency between Kubernetes and GCP resource management."}},{id:46,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Database - Query Insights",question:"Your Cloud SQL database is experiencing slow query performance. You need to identify which queries are consuming the most resources. What tool should you use?",options:["Use Cloud Monitoring to view CPU metrics","Query Insights in Cloud SQL to analyze query performance and get optimization recommendations","Enable general query logging and manually analyze logs","Run EXPLAIN on all queries manually"],correct:1,explanation:"Query Insights automatically identifies slow queries, provides performance statistics, execution plans, and recommendations for indexes and optimization without manual intervention.",wrongExplanations:{1:"General query logs capture all queries but don't provide performance analysis, rankings, or recommendations. Manual analysis is time-consuming.",2:"CPU metrics show overall load but don't identify specific problematic queries or suggest optimizations.",3:"Manually running EXPLAIN on queries requires knowing which queries to investigate. Query Insights automatically identifies and analyzes problematic queries."}},{id:47,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Cloud Run - Traffic splitting",question:"You deployed a new version of your Cloud Run service and want to gradually shift traffic from the old version to test stability. What should you do?",options:["Deploy the new version to a separate service and use a load balancer","Use Cloud Run's traffic splitting feature to route a percentage of traffic to the new revision","Delete the old revision and hope the new one works","Use feature flags in your application code"],correct:1,explanation:"Cloud Run's built-in traffic splitting allows you to split traffic between revisions by percentage, enabling canary deployments and gradual rollouts with quick rollback capability.",wrongExplanations:{1:"Separate services with a load balancer adds complexity. Cloud Run has native traffic management that's simpler and integrated.",2:"Deleting old revisions removes the ability to quickly roll back if issues arise. Traffic splitting allows safe, gradual rollout with instant rollback.",3:"Feature flags add application complexity. Cloud Run's infrastructure-level traffic splitting is simpler and works regardless of application code."}},{id:48,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Cloud Trace",question:"Your microservices application has high latency, but you're unsure which service is causing delays. What tool should you use to investigate?",options:["Cloud Trace to visualize request flow and identify latency bottlenecks across services","Cloud Profiler to analyze CPU usage","Cloud Logging to search for error messages","Cloud Monitoring dashboards showing CPU usage"],correct:0,explanation:"Cloud Trace provides distributed tracing that shows how requests flow through microservices, where time is spent, and which services contribute to latency.",wrongExplanations:{1:"CPU metrics show resource usage but don't reveal how requests flow between services or where latency occurs in the request path.",2:"Logs show events but don't visualize request flow timing or show latency distribution across service calls.",3:"Cloud Profiler analyzes code-level CPU and memory usage within a service, not request flow and latency across multiple services."}},{id:49,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 GKE - kubectl",question:"You need to view logs from a specific Pod in your GKE cluster. What command should you use?",options:["kubectl logs <pod-name> to fetch logs from the Pod's containers","gcloud logging read to query Cloud Logging","docker logs <container-id>","kubectl get pod <pod-name> --show-logs"],correct:0,explanation:"kubectl logs is the standard command for viewing Pod logs in Kubernetes. It fetches logs from the Pod's containers, optionally filtering by namespace.",wrongExplanations:{1:"gcloud logging read queries Cloud Logging, which works but adds latency and requires proper log ingestion setup. kubectl logs is more direct for real-time troubleshooting.",2:"kubectl get doesn't have a --show-logs option. The get command is for retrieving resource definitions and status, not logs.",3:"docker logs requires direct node access and container ID knowledge. In GKE, you work at the Kubernetes abstraction level with kubectl, not directly with Docker."}},{id:50,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your organization is setting up a new Google Cloud environment. You need to organize resources for three business units (Sales, Engineering, and Marketing), each with development and production environments. You want to enforce different organizational policies per business unit and enable centralized billing. What is the most appropriate resource hierarchy structure?",options:["Create six Projects (one for each business unit and environment combination) under the Organization root, and apply policies at the Project level","Create separate Organizations for each business unit, create dev and prod Projects in each Organization, and link to a centralized billing account","Create an Organization, create three Folders for each business unit, create dev and prod Projects within each Folder, and apply policies at the Folder level","Create a single Project with labels for business-unit and environment, and use IAM conditions to enforce policies"],correct:2,explanation:"The correct approach is to use Folders to represent business units within a single Organization. This allows you to: 1) Apply organizational policies at the Folder level that automatically inherit to all child Projects, 2) Maintain centralized billing through the Organization, 3) Provide clear separation between business units while maintaining centralized governance, 4) Enable separate dev/prod Projects within each business unit Folder. Folders are specifically designed for grouping Projects and applying hierarchical policies.",wrongExplanations:{1:"Creating separate Organizations would prevent centralized billing and governance. Organizations are typically limited to one per company/domain, and managing multiple Organizations creates unnecessary complexity and prevents unified policy enforcement.",2:"Using labels within a single Project doesn't provide resource isolation or policy enforcement capabilities. Labels are metadata for organization and billing attribution, but cannot enforce security boundaries or apply organizational policies. This would create a security and operational management nightmare.",3:"While simpler initially, applying policies at the Project level lacks scalability and doesn't leverage the hierarchical policy inheritance that Folders provide. You'd need to manually configure policies for each of the six Projects, and adding new Projects would require manual policy configuration each time."}},{id:51,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Organizational policies",question:"Your company has a policy that all Compute Engine instances must be created only in us-central1 and europe-west1 regions for data residency compliance. You need to enforce this across all projects in your organization. What should you do?",options:["Create a custom IAM role that only grants compute.instances.create permission for us-central1 and europe-west1, and assign it to all users","Set an organizational policy constraint `constraints/gcp.resourceLocations` with allowed values of `in:us-central1-locations` and `in:europe-west1-locations` at the Organization level","Configure firewall rules to block traffic to VMs created outside the specified regions","Use Cloud Asset Inventory to monitor VM creation and automatically delete VMs created in other regions"],correct:1,explanation:"The `constraints/gcp.resourceLocations` organizational policy is the correct solution. This constraint allows you to define allowed or denied locations for resource creation across your entire organization. By setting it at the Organization level with allowed values for US-central1 and Europe-west1 locations, you preventively enforce compliance - no user can create resources outside these locations regardless of their IAM permissions.",wrongExplanations:{1:"IAM roles control WHO can perform actions, not WHERE resources can be created. While you could theoretically create complex conditional IAM policies, this approach is extremely difficult to maintain, doesn't scale, and can be bypassed by users with broader permissions.",2:"This is a reactive approach that allows policy violations to occur before detection. Cloud Asset Inventory is for monitoring and auditing, not enforcement. Additionally, automatically deleting resources could cause significant business disruption.",3:"Firewall rules control network traffic between resources, not where resources can be created. This approach doesn't address the core requirement and would only affect connectivity, not creation."}},{id:52,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Enabling APIs",question:"You are setting up a new project and need to deploy a Compute Engine instance that will read data from Cloud Storage and write results to BigQuery. When you try to create the instance, you receive an error: 'Access Not Configured. Compute Engine API has not been used in project.' What is the most efficient way to resolve this?",options:["Run `gcloud services enable compute.googleapis.com storage.googleapis.com bigquery.googleapis.com` to enable all required APIs at once","Delete the project and create a new one with APIs pre-enabled","Enable only Compute Engine API now, then enable other APIs later when you encounter errors","Contact Google Cloud support to request API enablement"],correct:0,explanation:"The most efficient approach is to enable all required APIs at once using the gcloud command. This prevents future errors and allows immediate use of all services.",wrongExplanations:{1:"This reactive approach is inefficient and disrupts development workflow. You'd encounter errors at each stage requiring you to stop and enable APIs.",2:"API enablement is a self-service operation that doesn't require support. This would cause unnecessary delays.",3:"Project deletion is wasteful and doesn't solve the problem. API enablement is the correct solution."}},{id:53,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Budgets and alerts",question:"Your team deployed a new application and costs are higher than expected. You want an email alert when monthly spend reaches $5,000, and another when it's forecasted to reach $7,000. How should you configure this?",options:["Set a billing hard cap at $7,000 with notification at $5,000","Create a budget of $7,000 with two thresholds: 71% actual spend and 100% forecasted spend","Create two separate budgets: $5,000 actual and $7,000 forecasted","Use Pub/Sub to monitor billing exports and trigger alerts"],correct:1,explanation:"A single budget with both actual and forecasted thresholds is most efficient. 71% of $7,000 equals $5,000.",wrongExplanations:{1:"Two budgets create unnecessary management overhead when one budget can handle both thresholds.",2:"Google Cloud doesn't support hard spending caps. Budgets only provide alerts, they don't stop spending.",3:"This over-engineers a solution that budgets handle natively."}},{id:54,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Machine types",question:"Your video transcoding application is CPU-intensive but doesn't require much memory. You're using n2-standard-8 (8 vCPUs, 32 GB RAM). How can you optimize costs?",options:["Keep n2-standard-8 and rely on sustained use discounts","Switch to n2-highmem-8 for better performance","Switch to n2-highcpu-8 with 8 vCPUs and only 8 GB RAM at lower cost","Switch to e2-medium to reduce costs"],correct:2,explanation:"The highcpu family is designed for compute-intensive workloads, providing the same vCPUs with less RAM at ~30% lower cost.",wrongExplanations:{1:"e2-medium only has 2 vCPUs, drastically reducing performance. Your jobs would take 4x longer.",2:"highmem instances have MORE RAM (64 GB), moving in the wrong direction and increasing costs.",3:"Sustained use discounts apply automatically but don't address paying for unused RAM."}},{id:55,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Spot VMs",question:"You have a batch processing job running nightly for 3-4 hours that can be restarted if interrupted. How can you minimize costs?",options:["Purchase 1-year committed use discounts","Switch to Spot VMs which offer up to 91% discount for interruptible workloads","Use autoscaling to scale to zero","Switch to e2-micro instances"],correct:1,explanation:"Spot VMs are perfect for fault-tolerant batch jobs, providing 60-91% discounts with 30-second preemption warnings.",wrongExplanations:{1:"CUDs require 24/7 commitment but your job runs only ~15% of the time. Spot VMs provide better savings.",2:"e2-micro would be insufficient for processing jobs and make them run much slower.",3:"Autoscaling addresses scheduling but doesn't reduce per-instance costs like Spot VMs do."}},{id:56,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud Storage classes",question:"Security camera footage must be retained for 7 years. It's accessed frequently for 30 days, occasionally for days 31-365, and rarely after. How should you configure storage?",options:["Use Coldline for all objects","Manually move files quarterly","Store everything in Archive from the beginning","Start in Standard, transition to Nearline after 30 days, then Archive after 365 days using lifecycle policies"],correct:3,explanation:"Lifecycle policies automatically transition between storage classes based on age, optimizing costs for each access pattern.",wrongExplanations:{1:"Archive has high retrieval costs and early deletion fees. Starting there would be expensive for frequent first-30-day access.",2:"Coldline doesn't optimize for the frequent early access or the very rare post-year access.",3:"Manual transitions are error-prone, don't scale, and delay cost optimization."}},{id:57,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud SQL HA",question:"Your MySQL database must survive a complete zone failure with automatic failover and minimal data loss. What should you configure?",options:["Create read replicas in multiple zones","Enable automated backups every hour","Manually manage instances in each zone","Enable high availability configuration with synchronous replication to a standby in another zone"],correct:3,explanation:"Cloud SQL HA provides automatic failover in 60-120 seconds with zero data loss via synchronous replication to a standby replica in a different zone.",wrongExplanations:{1:"Read replicas use asynchronous replication and don't automatically promote during failures. Manual intervention is required.",2:"Backups are for disaster recovery but cause downtime during restore. They don't provide automatic failover.",3:"Manual management is complex and doesn't leverage Google's automatic failover capabilities."}},{id:58,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Load balancer selection",question:"You need a global web application with HTTPS, URL path-based routing, and Cloud CDN integration. Which load balancer should you use?",options:["Network Load Balancer for TCP load balancing","External Application Load Balancer with URL maps for path-based routing","Regional External Application Load Balancer","Internal Application Load Balancer"],correct:1,explanation:"External Application Load Balancer provides Layer 7 HTTP(S) load balancing with global reach, URL-based routing, and native Cloud CDN integration.",wrongExplanations:{1:"Network Load Balancer operates at Layer 4 and can't perform URL routing or integrate with CDN.",2:"Internal Application Load Balancer is for private VPC traffic, not internet-facing applications.",3:"Regional load balancer lacks global reach and edge caching benefits."}},{id:59,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - GKE Autopilot",question:"Your team wants Kubernetes without managing nodes or cluster operations. The application has variable traffic. What should you use?",options:["GKE Autopilot where Google manages all cluster infrastructure and you only configure Pod resources","Managed instance groups with containers","GKE Standard with node auto-provisioning","GKE Standard with manual node pools"],correct:0,explanation:"GKE Autopilot provides fully managed Kubernetes - you define Pod requirements, Google handles all node provisioning, scaling, and updates.",wrongExplanations:{1:"Standard GKE requires managing nodes, pools, and cluster configuration even with auto-provisioning.",2:"Manual management has highest operational overhead.",3:"MIGs lack Kubernetes orchestration features and would require reinventing Kubernetes."}},{id:60,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - IAM roles",question:"You need to grant a data science team access to run BigQuery queries across multiple projects in a folder. They should NOT be able to create or delete datasets. What is the most appropriate approach?",options:["Grant Editor role at the folder level","Grant BigQuery Admin role with IAM conditions","Grant BigQuery User role at the folder level","Grant BigQuery Data Owner for each dataset"],correct:2,explanation:"BigQuery User role allows running queries and browsing datasets without modification permissions. At folder level, it applies to all projects automatically.",wrongExplanations:{1:"BigQuery Admin grants full control including creating/deleting datasets, violating least privilege even with conditions.",2:"Editor role grants broad permissions across ALL services, massively violating least privilege.",3:"Data Owner requires per-dataset management and doesn't scale well or provide query execution capabilities."}},{id:61,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Cost allocation",question:"Multiple teams share projects and Finance needs to allocate costs back to teams for chargeback. What is the most effective approach?",options:["Create separate billing accounts for each team","Apply labels to resources with team and cost-center keys, enable billing export to BigQuery, query costs by label","Manually review monthly invoices","Create separate projects for each team"],correct:1,explanation:"Labels provide flexible multi-dimensional cost attribution. Billing export to BigQuery captures label data for flexible reporting and chargeback automation.",wrongExplanations:{1:"Separate billing accounts create management overhead, lose volume discounts, and complicate cross-team resource sharing.",2:"Separate projects are inflexible and don't handle shared resources or multi-dimensional attribution (team AND app AND cost center).",3:"Manual allocation is labor-intensive, error-prone, not scalable, and delays chargeback processes."}},{id:62,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Firewall rules",question:"Users cannot access your web application on port 80. The app works when you curl localhost:80 from the VM. What is the most likely issue?",options:["Cloud NAT is not configured","VPC firewall rules don't allow ingress on port 80 - create an allow rule for tcp:80 from 0.0.0.0/0","Private Google Access is not enabled","Instances lack external IP addresses"],correct:1,explanation:"VPC firewall rules default to DENY all ingress. An ingress allow rule for tcp:80 is needed to permit external HTTP traffic to reach the instances.",wrongExplanations:{1:"Private Google Access is for VM-to-Google API connectivity, not for inbound user traffic.",2:"If IPs were missing, connection attempts would fail differently. The symptom suggests blocking, not routing failure.",3:"Cloud NAT provides outbound internet for private VMs, not inbound access for users."}},{id:63,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Private Google Access",question:"Your VMs without external IPs need to access Cloud Storage and BigQuery. What is the minimum configuration required?",options:["Set up Cloud VPN","Assign temporary external IPs","Enable Private Google Access on the subnet where instances are deployed","Configure Cloud NAT"],correct:2,explanation:"Private Google Access allows VMs with only internal IPs to reach Google APIs using Google's internal network without internet traversal.",wrongExplanations:{1:"Cloud NAT provides general internet access but is overkill and less secure for Google API access only.",2:"Temporary external IPs defeat security requirements and add operational complexity.",3:"Cloud VPN connects on-premises to Google Cloud, not relevant for VM-to-API connectivity within Google Cloud."}},{id:64,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Cloud NAT",question:"VMs without external IPs need to download software updates from the internet and call external APIs. What should you implement?",options:["Set up a proxy server on a VM with external IP","Assign temporary external IPs when needed","Create a Cloud Router, then create a Cloud NAT gateway associated with that router for all subnets","Enable Private Google Access which provides full internet access"],correct:2,explanation:"Cloud NAT provides managed outbound internet connectivity for private VMs with automatic HA, scalability, and no VM configuration needed.",wrongExplanations:{1:"Private Google Access only reaches Google APIs, not general internet or third-party services.",2:"Temporary IPs defeat security and create operational complexity managing assignments.",3:"Self-managed proxy creates single point of failure, requires maintenance, and doesn't scale automatically."}},{id:65,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Cloud Identity",question:"Your company has 500 employees and needs identity management for Google Cloud without Google Workspace. What should you implement to minimize costs?",options:["Purchase Google Workspace licenses for all employees","Set up Cloud Identity Free edition for your domain","Use external identity federation without Cloud Identity","Have employees create personal Gmail accounts"],correct:1,explanation:"Cloud Identity Free provides enterprise identity management, IAM integration, and organizational features without productivity apps or per-user costs.",wrongExplanations:{1:"Workspace adds $6/user/month ($36k/year for 500 users) for productivity tools you don't need.",2:"Personal Gmail accounts prevent centralized management, can't create Organizations, and violate security policies.",3:"Federation still requires Cloud Identity to establish Organization and manage configurations."}},{id:66,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"Your team reports they cannot create more than 8 VMs in us-central1-a. You need to allow up to 24 CPUs. What should you do?",options:["Navigate to IAM & Admin > Quotas, filter for Compute Engine CPU quota in us-central1, request increase to 24","Delete existing VMs and recreate with smaller machine types","Use gcloud command to update quota","Upgrade to premium support for higher quotas"],correct:0,explanation:"Quota increases are requested through the Quotas page by selecting the CPU quota and providing justification. Requests are typically approved quickly.",wrongExplanations:{1:"No gcloud command exists to modify quotas directly. Quotas must be requested through Console or API.",2:"Deleting VMs is disruptive and doesn't solve the need for more capacity. Request proper quota.",3:"Quota increases are independent of support plans. All customers can request increases based on need."}},{id:67,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Billing exports",question:"Finance needs detailed cost analysis with data updated throughout the day, queryable by project, service, SKU, and labels. What should you configure?",options:["Export to Cloud Storage CSV files daily","Give Billing Account Viewer role for Console access","Download monthly PDF invoices","Enable billing export to BigQuery with detailed usage cost data in a multi-region dataset"],correct:3,explanation:"BigQuery export provides detailed resource-level data with throughout-the-day updates, SQL query capabilities, and integration with BI tools.",wrongExplanations:{1:"PDF invoices are high-level summaries lacking granular data, labels, or SKU details, and are monthly only.",2:"CSV exports update daily (not throughout day), require manual processing, and lack native query capabilities.",3:"Console provides basic reports but no SQL queries, automation, or advanced analysis capabilities."}},{id:68,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - VPC creation",question:"You're setting up a VPC for production across three regions. You want full control over subnet IP ranges following best practices. What should you do?",options:["Create auto mode VPC which automatically creates subnets in all regions","Create custom mode VPC, then create subnets in required regions with non-overlapping RFC 1918 IP ranges","Create three separate VPCs and connect with VPC Peering","Use the default VPC and modify subnets"],correct:1,explanation:"Custom mode VPC provides full control over subnet creation, IP addressing, and follows production best practices with explicit configuration.",wrongExplanations:{1:"Auto mode creates subnets in ALL 40+ regions using predefined ranges that may conflict with on-premises networks.",2:"Three separate VPCs add unnecessary complexity. Single global VPC can span multiple regions natively.",3:"Default VPC is auto mode and may have experimental resources. Best practice is purpose-built custom VPCs for production."}},{id:69,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - BigQuery partitioning",question:"Your 50TB dataset contains 5 years of logs. Most queries analyze last 3 months. Queries scan entire table and are expensive. How can you optimize costs?",options:["Partition table by date and ensure queries include date filter in WHERE clause for partition pruning","Enable BigQuery flat-rate pricing","Export old data to Cloud Storage","Add clustering without partitioning"],correct:0,explanation:"Partitioning by date physically separates data. Queries with date filters only scan relevant partitions, reducing costs by ~95% when querying 3 of 60 months.",wrongExplanations:{1:"Exporting loses BigQuery query capabilities and creates operational complexity managing data in two places.",2:"Flat-rate pricing changes billing model but doesn't fix inefficient queries. Optimize query first.",3:"Clustering improves performance within partitions but doesn't reduce data scanned like partitioning does."}},{id:70,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Run",question:"You need to deploy a containerized API with highly variable traffic and long idle periods. You want to minimize costs and avoid infrastructure management. What should you use?",options:["GKE Autopilot with scale-to-zero","Cloud Functions instead","Cloud Run, which automatically scales to zero during idle periods and charges only for request processing time","Compute Engine with autoscaling to zero"],correct:2,explanation:"Cloud Run is designed for this use case: auto-scales to zero (no idle charges), fast startup, pay-per-request pricing, fully managed, supports any container.",wrongExplanations:{1:"GKE Autopilot can scale to zero but has Kubernetes overhead and complexity unnecessary for simple APIs.",2:"Compute Engine MIGs don't scale to zero - minimum is 1 instance, resulting in 24/7 VM costs.",3:"Cloud Functions works but requires specific language runtimes. The workload is already containerized, making Cloud Run the direct choice."}},{id:71,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud VPN",question:"You need encrypted connectivity from on-premises to Google Cloud over internet with 3 Gbps throughput and high availability. What should you implement?",options:["VPC Peering","HA VPN with two tunnels to two VPN gateways for 99.99% SLA and up to 3 Gbps per tunnel","Direct connection without encryption","Classic VPN with single tunnel"],correct:1,explanation:"HA VPN provides 99.99% SLA with two interfaces in different zones, supports up to 3 Gbps per tunnel, uses BGP for automatic failover.",wrongExplanations:{1:"Classic VPN is deprecated, has no SLA, single point of failure, and static routing only.",2:"Unencrypted connection violates security and compliance requirements for data in transit.",3:"VPC Peering connects VPCs within Google Cloud, not on-premises to cloud."}},{id:72,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - VM operations",question:"You need to perform monthly maintenance requiring a VM to be powered off while preserving all data. What is the correct procedure?",options:["Suspend the instance","Delete and recreate from snapshot","Reset the instance","Stop the instance, perform maintenance, then start it again - all disks and IPs are preserved"],correct:3,explanation:"Stop/start is the standard procedure for offline maintenance. VM state, disks, metadata, and IP addresses are all preserved during stop state.",wrongExplanations:{1:"Delete/recreate is complex, risky, may lose metadata and IPs, and risks configuration errors.",2:"Reset performs hard reboot without powering off - doesn't allow offline maintenance and may cause data loss.",3:"Suspend is specialized for preserving RAM state - simpler stop/start is sufficient for routine maintenance."}},{id:73,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Cloud Monitoring alerts",question:"Your app has intermittent latency spikes. You want alerts when average latency exceeds 500ms for 5 minutes. What should you configure?",options:["Create alerting policy with metric threshold condition for latency > 500ms with 5-minute evaluation window","Set up log-based metric querying every minute","Use Cloud Trace for manual monitoring","Configure uptime checks"],correct:0,explanation:"Cloud Monitoring alerting policies continuously evaluate metrics, trigger on conditions, and send notifications automatically with configurable thresholds and windows.",wrongExplanations:{1:"Log-based metrics are more complex, less efficient, and higher latency than native metric-based alerts.",2:"Cloud Trace is for distributed tracing, not threshold-based alerting on aggregate metrics.",3:"Uptime checks monitor availability, not aggregate application latency across all requests."}},{id:74,domain:"Configuring access and security",subdomain:"4.1 IAM - Service accounts for VMs",question:"Your Compute Engine application needs to upload files to Cloud Storage. Following security best practices, how should you configure authentication?",options:["Make bucket publicly writable","Create dedicated service account with Storage Object Creator role, attach to VM, use Application Default Credentials","Generate service account keys and store on VM disk","Use personal user account credentials"],correct:1,explanation:"Attaching service account to VM with ADC is secure best practice: no key management, automatic rotation, easy auditing, follows least privilege.",wrongExplanations:{1:"Service account keys create security risks: can be stolen, don't rotate automatically, require secure storage.",2:"Personal accounts tie permissions to individuals, break when users leave, violate separation of duties.",3:"Public buckets allow anyone to upload, creating severe security violations and unlimited cost exposure."}},{id:75,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Workload Identity",question:"Your GKE application needs Cloud SQL access without managing service account keys. What is the recommended approach?",options:["Configure Workload Identity to allow Pods to authenticate as Google service accounts without key files","Store service account keys in Kubernetes Secrets","Use Compute Engine default service account","Mount key JSON files as volumes"],correct:0,explanation:"Workload Identity is Google's recommended method: no key files, automatic rotation, fine-grained permissions per workload, eliminates key management.",wrongExplanations:{1:"Secrets are base64-encoded (not encrypted by default), anyone with access can extract keys, no automatic rotation.",2:"Default service account has Editor role - too broad, can't differentiate between applications, violates least privilege.",3:"Mounting key files has same security issues as Secrets plus risk of logging or accidental export."}},{id:76,domain:"Configuring access and security",subdomain:"4.3 Security - Cloud KMS CMEK",question:"You must store customer data with customer-controlled encryption keys and ability to revoke access independently of deletion. What should you implement?",options:["Customer-Managed Encryption Keys (CMEK) using Cloud KMS to manage keys while Google handles crypto operations","Default Google-managed encryption","Customer-Supplied Encryption Keys (CSEK)","Client-side encryption"],correct:0,explanation:"CMEK provides key control for compliance while Google handles operations. Disabling key revokes access without deleting objects. Balances control and manageability.",wrongExplanations:{1:"Default encryption doesn't give key control, can't revoke by disabling keys, doesn't meet compliance for customer-controlled keys.",2:"CSEK requires providing keys with every request, creates operational burden, if lost data is inaccessible.",3:"Client-side encryption requires application implementation, loses Google Cloud features, more complex and risky."}},{id:77,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Cloud Storage operations",question:"You need to grant a partner temporary 7-day read access to specific files without them having a Google account. What should you do?",options:["Download and email files","Generate signed URLs with 7-day expiration for specific objects and share URLs","Make entire bucket public for 7 days","Create Google account for partner"],correct:1,explanation:"Signed URLs provide time-limited access to specific objects without Google accounts using cryptographic signatures. Auto-expire, secure, auditable.",wrongExplanations:{1:"Public buckets expose ALL objects to internet, creating massive security risk even temporarily.",2:"Creating accounts adds overhead and doesn't automatically expire - signed URLs are simpler.",3:"Email has size limits, security concerns, no audit trail, manual process doesn't scale."}},{id:78,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"You need AI-powered analysis to identify overprovisioned VMs, unused disks, and security misconfigurations. What should you use?",options:["Cloud Asset Inventory only","Active Assist Recommender API only","Custom scripts analyzing gcloud outputs","Gemini Cloud Assist with Cloud Asset Inventory for AI-powered recommendations"],correct:3,explanation:"Gemini Cloud Assist (2025 feature) provides AI-powered analysis with natural language interaction, proactive recommendations, and comprehensive resource visibility.",wrongExplanations:{1:"Active Assist gives rule-based recommendations but lacks AI conversational interface and comprehensive analysis.",2:"Asset Inventory shows resources but doesn't provide optimization recommendations or analysis.",3:"Custom scripts require development, maintenance, and expertise - don't build what's provided as managed service."}},{id:79,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE troubleshooting",question:"Your GKE cluster can't pull images from Artifact Registry in the same project. What is the most likely issue?",options:["Need VPC peering to Artifact Registry","Node service account needs Artifact Registry Reader role","Artifact Registry API is not enabled","Repository needs to be public"],correct:1,explanation:"GKE nodes use their service account to pull images. Grant Artifact Registry Reader role to the node service account for authentication.",wrongExplanations:{1:"Disabled API produces different error. Authentication failure indicates permission issues.",2:"Making repositories public is security anti-pattern - fix authentication instead.",3:"VPC connectivity not required for Artifact Registry from GKE - this is authentication issue."}},{id:80,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud Storage lifecycle",question:"Application logs must be retained 7 years. Frequently accessed first 30 days, occasionally to day 365, rarely after. How to optimize costs?",options:["Store everything in Archive from start","Manually move files quarterly","Use Coldline for all objects","Use Standard class with lifecycle rules transitioning to Nearline at 30 days, Archive at 365 days"],correct:3,explanation:"Lifecycle policies automatically transition between classes optimizing for each access pattern: Standard (frequent), Nearline (occasional), Archive (rare).",wrongExplanations:{1:"Archive has high retrieval costs and early deletion fees - expensive for frequent first-30-day access.",2:"Coldline doesn't optimize for both frequent early access and very rare post-year access.",3:"Manual transitions are error-prone, don't scale, delay optimization."}},{id:81,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Instance types",question:"Your batch processing is CPU-bound and doesn't need much memory. Currently using n2-standard-8 (8 vCPUs, 32GB RAM). How to optimize costs?",options:["Keep n2-standard-8 for sustained use discounts","Switch to e2-medium","Switch to n2-highmem-8","Switch to n2-highcpu-8 (8 vCPUs, 8GB RAM) at ~30% lower cost"],correct:3,explanation:"highcpu family provides same vCPUs with less RAM (1:1 ratio vs 1:4) for compute-intensive workloads at significant cost savings.",wrongExplanations:{1:"e2-medium only has 2 vCPUs - would make jobs 4x slower.",2:"highmem has MORE RAM (64GB) - wrong direction, increases costs.",3:"Sustained use discounts apply automatically but don't address paying for unused RAM."}},{id:82,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Database Center",question:"You have Cloud SQL, AlloyDB, and Spanner instances. You need centralized database health monitoring and recommendations. What should you use?",options:["Use gcloud commands for each service separately","Set up separate monitoring tools","Database Center for unified interface managing all Google Cloud databases with AI recommendations","Create custom Cloud Monitoring dashboards for each type"],correct:2,explanation:"Database Center (2025 feature) provides unified interface for all Google Cloud databases with health monitoring, performance insights, and AI-powered optimization recommendations.",wrongExplanations:{1:"Custom dashboards require manual setup per database and don't provide unified insights and AI recommendations.",2:"gcloud requires scripting and manual aggregation, no centralized view or AI recommendations.",3:"Separate tools increase complexity, costs, and maintenance. Database Center provides native integration."}},{id:83,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud NGFW",question:"You need advanced network security with intrusion detection, threat prevention, and URL filtering for your VPC. What should you configure?",options:["VPC firewall rules with deny rules","Cloud Next Generation Firewall (Cloud NGFW) with threat prevention profiles","Cloud Armor security policies","Network tags with hierarchical policies"],correct:1,explanation:"Cloud NGFW (2025 feature) provides IDS/IPS, threat prevention, URL filtering, TLS inspection - advanced security not available in standard firewall rules.",wrongExplanations:{1:"VPC firewall rules provide basic allow/deny but lack intrusion detection and threat prevention.",2:"Cloud Armor protects against DDoS/web attacks at Layer 7 but doesn't provide network-level intrusion detection.",3:"Network tags improve organization but don't add threat detection capabilities."}},{id:84,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Fabric FAST",question:"Your enterprise needs multi-environment, multi-project GCP foundation following Google best practices. What tool should you use?",options:["Cloud Foundation Toolkit's Fabric FAST framework with pre-built patterns","Write custom Terraform from scratch","Deploy manually and export configs","Use gcloud scripts"],correct:0,explanation:"Fabric FAST is designed for enterprise GCP foundations with pre-built, tested patterns following Google best practices for multi-environment deployments.",wrongExplanations:{1:"Custom Terraform requires significant development and may not follow best practices.",2:"gcloud scripts are imperative and don't provide declarative infrastructure management needed for enterprise.",3:"Manual deployment is error-prone, not reproducible, doesn't scale, doesn't capture all settings."}},{id:85,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Ops Agent",question:"You need to collect metrics and logs from Compute Engine instances for Cloud Monitoring and Logging. What is the recommended approach?",options:["Use legacy Stackdriver agents","Write custom scripts pushing to Logging API","Install and configure Ops Agent on instances","Manually export to Cloud Storage"],correct:2,explanation:"Ops Agent is the recommended unified agent replacing legacy agents, providing optimized collection with better performance and easier configuration.",wrongExplanations:{1:"Legacy Stackdriver agents are deprecated. Ops Agent provides better performance and simpler configuration.",2:"Custom scripts add maintenance overhead and complexity. Ops Agent handles this automatically.",3:"Manual export is inefficient and loses real-time monitoring capabilities."}},{id:86,domain:"Configuring access and security",subdomain:"4.1 IAM - Least privilege",question:"A data analyst needs read-only BigQuery dataset access for queries but not dataset/table modification. What role should you grant?",options:["BigQuery Data Viewer at dataset level","BigQuery User at project level","BigQuery Admin with conditions","Project Viewer role"],correct:0,explanation:"BigQuery Data Viewer provides read-only access to table data and metadata at dataset level, following least privilege without modification permissions.",wrongExplanations:{1:"Project Viewer grants read access to ALL project resources, not just BigQuery - violates least privilege.",2:"BigQuery User allows creating jobs including INSERT/UPDATE/DELETE operations - more than needed.",3:"BigQuery Admin grants full permissions. IAM conditions don't effectively restrict to read-only."}},{id:87,domain:"Configuring access and security",subdomain:"4.1 IAM - Organization policies",question:"You need to prevent all users from creating external IPs on Compute Engine instances organization-wide. What should you do?",options:["Remove Compute Instance Admin role from all users","Manually review and delete external IPs daily","Use VPC firewall rules to block external traffic","Set organizational policy constraint 'constraints/compute.vmExternalIpAccess' to deny all at Organization level"],correct:3,explanation:"Organization policies provide centralized control. The vmExternalIpAccess constraint preventively blocks external IP creation across the organization.",wrongExplanations:{1:"Removing roles prevents any instance creation, not just external IPs - too restrictive.",2:"Firewall rules control traffic flow but don't prevent external IP assignment.",3:"Manual review is reactive, doesn't scale, allows windows where insecure configs exist."}},{id:88,domain:"Configuring access and security",subdomain:"4.1 IAM - Custom roles",question:"You need a role allowing starting/stopping VMs but not creating or deleting them. What should you do?",options:["Use Compute Viewer role","Use Compute Instance Admin role","Grant Compute Admin with IAM conditions","Create custom role with compute.instances.start and compute.instances.stop permissions only"],correct:3,explanation:"Custom roles allow bundling specific permissions for precise access control, following least privilege with exactly needed permissions.",wrongExplanations:{1:"Instance Admin includes create/delete permissions - violates requirements and least privilege.",2:"IAM conditions restrict WHERE/WHEN, not specific operations within a role.",3:"Viewer only allows reading, not starting/stopping instances."}},{id:89,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Query Insights",question:"Your Cloud SQL database has slow queries. You need to identify resource-consuming queries. What tool should you use?",options:["Enable general query logging and manually analyze","Run EXPLAIN on all queries manually","Query Insights to analyze performance and get optimization recommendations automatically","Use Cloud Monitoring CPU metrics"],correct:2,explanation:"Query Insights (2025 feature) automatically identifies slow queries, provides performance statistics, execution plans, and optimization recommendations without manual work.",wrongExplanations:{1:"General query logs don't provide performance analysis, rankings, or recommendations - requires manual analysis.",2:"CPU metrics show overall load but don't identify specific problematic queries.",3:"Manual EXPLAIN requires knowing which queries to investigate. Query Insights automatically finds problem queries."}},{id:90,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Cloud Run traffic splitting",question:"You deployed new Cloud Run version and want gradual traffic shift to test stability. What should you do?",options:["Use feature flags in application code","Deploy to separate service with load balancer","Use Cloud Run's traffic splitting feature to route percentage of traffic to new revision","Delete old revision immediately"],correct:2,explanation:"Cloud Run's built-in traffic splitting enables canary deployments with percentage-based routing between revisions and quick rollback capability.",wrongExplanations:{1:"Separate services add complexity. Cloud Run has native traffic management that's simpler.",2:"Deleting old revision removes quick rollback ability. Traffic splitting enables safe gradual rollout.",3:"Feature flags add app complexity. Infrastructure-level splitting is simpler and works regardless of code."}},{id:91,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Cloud Trace",question:"Your microservices app has high latency but you're unsure which service causes delays. What tool should you use?",options:["Cloud Logging error searches","Cloud Monitoring CPU dashboards","Cloud Profiler for CPU analysis","Cloud Trace to visualize request flow and identify latency bottlenecks across services"],correct:3,explanation:"Cloud Trace provides distributed tracing showing how requests flow through microservices, where time is spent, and which services contribute to latency.",wrongExplanations:{1:"CPU metrics show resource usage but don't reveal request flow timing or latency distribution.",2:"Logs show events but don't visualize request flow timing.",3:"Profiler analyzes code-level CPU/memory within a service, not request flow across multiple services."}},{id:92,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Key management",question:"You discovered a service account key in a public GitHub repository. What should you do immediately?",options:["Delete the service account key in GCP and rotate all credentials immediately","Contact GitHub to remove the file","Change the service account's IAM roles","Make the repository private"],correct:0,explanation:"Once exposed, keys must be immediately revoked. Delete key in GCP, rotate others, investigate access. Repository changes don't remove key from Git history.",wrongExplanations:{1:"Making repo private doesn't remove key from Git history. Anyone who cloned/forked already has it.",2:"Changing roles doesn't invalidate the exposed key which can still be used with original permissions.",3:"Contacting GitHub is too slow. Immediate key deletion is critical for security."}},{id:93,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Config Connector",question:"Your team manages Kubernetes resources and wants to manage GCP resources using same tools and workflows. What should you use?",options:["Manual GCP resource creation","Terraform with Kubernetes provider","Config Connector to manage GCP resources as Kubernetes Custom Resources","gcloud commands in Kubernetes Jobs"],correct:2,explanation:"Config Connector extends Kubernetes to manage GCP resources as CRDs, enabling kubectl and GitOps workflows for both Kubernetes and GCP infrastructure.",wrongExplanations:{1:"Terraform with Kubernetes provider manages Kubernetes from Terraform (reverse direction).",2:"gcloud in Jobs is imperative and doesn't integrate with Kubernetes' declarative model.",3:"Manual creation defeats infrastructure-as-code and creates operational inconsistency."}},{id:94,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Cloud Storage versioning",question:"You accidentally deleted critical files 4 days ago from Cloud Storage. How can you recover them?",options:["Recover from local backups only","Contact Google Cloud Support for restore","Check Cloud Storage trash folder","List object versions and restore previous version if Object Versioning is enabled"],correct:3,explanation:"If Object Versioning is enabled, deleted objects become non-current versions that can be restored. Without versioning, deletions are permanent after grace period.",wrongExplanations:{1:"Google Support cannot restore deleted Cloud Storage objects. Storage is customer-managed.",2:"Cloud Storage doesn't have a trash folder. Deletions are permanent without versioning.",3:"Without Object Versioning, files cannot be recovered from Cloud Storage."}},{id:95,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - BigQuery optimization",question:"Your BigQuery query returns 'Resources exceeded during query execution'. What should you try first?",options:["Add WHERE clauses limiting data scanned or partition the table","Request BigQuery quota increase","Switch to Dataflow for processing","Export to Cloud Storage and process locally"],correct:0,explanation:"Limiting scanned data through WHERE clauses or partitioning reduces resource usage. Partitioned tables enable BigQuery to prune unnecessary data, dramatically improving performance.",wrongExplanations:{1:"Quota increases don't fix poorly optimized queries scanning too much data unnecessarily.",2:"Dataflow is overkill for query optimization. BigQuery handles large datasets efficiently with proper design.",3:"Exporting loses BigQuery's power and creates complexity. Fix query/table structure instead."}},{id:96,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE CrashLoopBackOff",question:"A Pod in GKE is stuck in 'CrashLoopBackOff'. What's the first diagnostic step?",options:["Scale deployment to zero and back","Restart the entire cluster","Delete the Pod and recreate it","Check Pod logs using 'kubectl logs <pod-name>' to see application errors"],correct:3,explanation:"Pod logs contain application output and error messages explaining why container is crashing. Always first diagnostic step for CrashLoopBackOff.",wrongExplanations:{1:"Deleting Pod might mask issue temporarily but doesn't solve underlying problem. New Pod will likely crash same way.",2:"Scaling down doesn't provide diagnostic information - just different way of deleting Pods. Investigate first.",3:"Restarting cluster is extremely disruptive and won't fix application-level issues. CrashLoopBackOff indicates Pod problem."}},{id:97,domain:"Configuring access and security",subdomain:"4.1 IAM - Predefined roles",question:"A developer needs to deploy to App Engine but not modify IAM policies. What role should you grant?",options:["App Engine Admin role","Editor role at project level","App Engine Deployer role at project level","Project Owner role"],correct:2,explanation:"App Engine Deployer allows deploying applications without modifying App Engine settings or IAM, following least privilege for developer needs.",wrongExplanations:{1:"App Engine Admin can modify settings including IAM policies - violates requirements.",2:"Editor grants broad permissions including IAM modifications across many services - far exceeds needs.",3:"Owner includes full IAM control and billing - most privileged role, completely violates least privilege."}},{id:98,domain:"Configuring access and security",subdomain:"4.1 IAM - Resource hierarchy inheritance",question:"You granted Editor role at folder level. What access does this provide?",options:["Editor access only to folder itself, not projects","Read-only access to projects in folder","Editor access to entire organization","Editor access to all projects within that folder and its subfolders"],correct:3,explanation:"IAM permissions inherit down the resource hierarchy. Folder-level roles automatically apply to all projects and resources within that folder and subfolders.",wrongExplanations:{1:"Folders are containers. IAM roles on folders automatically apply to contained resources through inheritance.",2:"Folder-level roles don't grant organization-level access - scoped to folder and children.",3:"Editor grants modification permissions. Question specifically asked about Editor role."}},{id:99,domain:"Configuring access and security",subdomain:"4.2 VPC Service Controls",question:"You need to prevent data exfiltration from sensitive BigQuery datasets. What security control should you implement?",options:["VPC Service Controls to create security perimeter around BigQuery project","Cloud Armor security policies","IAM conditions limiting access by IP","VPC firewall rules"],correct:0,explanation:"VPC Service Controls create security perimeters preventing data from leaving defined GCP resources, even if someone has IAM permissions. Designed specifically for preventing exfiltration.",wrongExplanations:{1:"IAM conditions restrict access by attributes but don't prevent data copying to authorized locations.",2:"Cloud Armor protects against external web attacks but doesn't control data movement between GCP resources.",3:"VPC firewall rules control network traffic to Compute Engine, not API-level BigQuery access or data exfiltration."}},{id:100,domain:"Configuring access and security",subdomain:"4.2 Binary Authorization",question:"You want to ensure only container images from your approved registry can deploy to GKE. What should you implement?",options:["Container Analysis API for scanning","Binary Authorization with policies requiring images from specific registries","IAM policies restricting deployers","VPC Service Controls around GKE"],correct:1,explanation:"Binary Authorization enforces deployment policies requiring images from specific registries, be signed, or pass vulnerability scans before GKE deployment.",wrongExplanations:{1:"Container Analysis scans for vulnerabilities but doesn't enforce source or prevent unapproved deployments.",2:"VPC Service Controls limit data exfiltration but don't control which container images can deploy.",3:"IAM controls who can deploy but not what they can deploy. Authorized users could still deploy unapproved images."}},{id:101,domain:"Configuring access and security",subdomain:"4.1 IAM - Conditions",question:"You need to grant contractor temporary access to Cloud Storage that auto-expires after 30 days. How should you configure this?",options:["Set calendar reminder to remove permissions","Use service account you'll delete after 30 days","Grant IAM role with condition that expires after 30 days using temporal constraints","Manually revoke access after 30 days"],correct:2,explanation:"IAM conditions support temporal constraints with automatic expiration on specified date without manual intervention. Reliable and automated.",wrongExplanations:{1:"Manual revocation is error-prone. People forget, get busy, or leave company. Automated expiration is more reliable.",2:"Service account deletion is disruptive if being used. IAM conditions allow fine-grained automatic expiration.",3:"Calendar reminders depend on humans. IAM conditions provide automated, enforced expiration not relying on manual processes."}},{id:102,domain:"Configuring access and security",subdomain:"4.3 Data encryption - CMEK",question:"Compliance requires control over encryption keys for Cloud Storage with ability to revoke access independently. What should you use?",options:["Client-side encryption","Default Google-managed keys","Customer-Supplied Encryption Keys (CSEK)","Customer-Managed Encryption Keys (CMEK) using Cloud KMS"],correct:3,explanation:"CMEK with Cloud KMS gives control over key management while Google handles cryptographic operations. Disabling key revokes access without deleting objects. Balances security, compliance, and operational simplicity.",wrongExplanations:{1:"Google-managed keys don't give control. Can't revoke by disabling keys or meet compliance for customer-controlled keys.",2:"CSEK requires providing keys with every operation, managing storage yourself, and creates operational burden.",3:"Client-side encryption adds app complexity, loses Google Cloud features, has performance overhead, riskier if implemented incorrectly."}},{id:103,domain:"Configuring access and security",subdomain:"4.4 Audit logging",question:"You need to track all IAM policy changes organization-wide for security auditing. What should you use?",options:["Set up custom Cloud Logging sinks","Admin Activity audit logs which are always enabled and log IAM changes","Enable Data Access audit logs","Use Cloud Asset Inventory"],correct:1,explanation:"Admin Activity logs automatically track administrative actions including IAM policy changes. Enabled by default, don't incur charges, provide detailed IAM change tracking.",wrongExplanations:{1:"Custom logging solutions are unnecessary when Admin Activity logs already capture IAM changes.",2:"Data Access logs track who accessed data, not who changed IAM policies.",3:"Cloud Asset Inventory tracks resource state but isn't designed for real-time audit logging."}},{id:104,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Error Reporting",question:"Your Cloud Run application has intermittent errors. You want automatic grouping and notifications. What should you use?",options:["Query Cloud Logging manually","Error Reporting to automatically group and track errors with Cloud Logging integration","Use Cloud Trace for errors","Set up custom log-based metrics"],correct:1,explanation:"Error Reporting automatically groups similar errors from Cloud Logging, provides real-time notifications, shows trends, and integrates with alerting - perfect for tracking application errors.",wrongExplanations:{1:"Log-based metrics require manual configuration and don't provide automatic error grouping or detailed analysis.",2:"Manual querying is time-consuming, reactive, doesn't provide automatic grouping or notifications.",3:"Cloud Trace is for distributed tracing and latency analysis, not error detection and grouping."}},{id:105,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Secrets management",question:"Your Cloud Run service needs database password. How should you securely provide this credential?",options:["Hard-code password in container image","Store password in Secret Manager and mount as environment variable in Cloud Run","Pass password as query parameter in URL","Store in Cloud Storage bucket"],correct:1,explanation:"Secret Manager provides secure storage with encryption, versioning, and audit logging. Cloud Run can directly access secrets as environment variables or volume mounts.",wrongExplanations:{1:"Hard-coding in images exposes passwords to anyone with image access. Images should never contain secrets.",2:"Query parameters are logged and visible in URLs - extremely insecure for sensitive data.",3:"Cloud Storage lacks specialized secret management features like rotation, versioning, and audit logging that Secret Manager offers."}},{id:106,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Compute Engine - Maintenance",question:"You need to perform maintenance on a Compute Engine instance without losing its ephemeral IP. What should you do?",options:["Use live migration","Take snapshot and restore to new instance","Stop the instance, perform maintenance, then start it - IPs are preserved","Delete and recreate with same configuration"],correct:2,explanation:"Stopping and starting preserves ephemeral IP addresses. Instance retains same internal and external IPs when restarted.",wrongExplanations:{1:"Deleting and recreating assigns new IP addresses. Ephemeral IP is released when instance deleted.",2:"Live migration is for Google's maintenance, not user-initiated. Can't trigger manually.",3:"Restoring to new instance creates different instance with different IPs. Original IPs aren't transferred."}},{id:107,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Live migration",question:"Google announces maintenance for your VM's zone. What happens by default?",options:["Instance is moved to another zone","Instance is automatically terminated","Nothing - you must manually migrate","Instance is live-migrated to another host in same zone with no downtime"],correct:3,explanation:"Compute Engine live migration moves running instances to different physical hosts during maintenance automatically with no downtime for most instance types.",wrongExplanations:{1:"Instances aren't terminated by default during maintenance. Only if maintenance policy set to 'TERMINATE'.",2:"Instances stay in same zone during live migration. Cross-zone would change internal IP and isn't done automatically.",3:"Live migration is automatic for most instances. No action needed unless using types that don't support live migration."}},{id:108,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE node pools",question:"You need to run CPU-intensive and memory-intensive workloads on GKE. How should you configure your cluster?",options:["Create separate clusters for each workload type","Use Autopilot which doesn't allow custom configuration","Create multiple node pools with different machine types optimized for each workload, use node selectors","Use single node pool with largest available machine type"],correct:2,explanation:"Multiple node pools allow workload-optimized machine types. Node selectors/affinity ensure pods schedule on appropriate nodes, optimizing cost and performance.",wrongExplanations:{1:"Single large machine type wastes resources. CPU-intensive workloads don't need excessive memory and vice versa, increasing costs.",2:"Separate clusters add management overhead, cost more (multiple control planes), and complicate cross-workload communication.",3:"Autopilot does allow resource specification through Pod requests. However, question is about Standard GKE where multiple node pools is correct approach."}},{id:109,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring - Uptime checks",question:"You want to monitor if your web application is accessible from different global locations. What should you configure?",options:["Use Cloud Trace for availability","Write Cloud Function to ping application every minute","Set up Compute Engine instances worldwide to test","Cloud Monitoring uptime checks from multiple geographic locations"],correct:3,explanation:"Uptime checks are designed for monitoring endpoint availability from multiple global locations. Integrate with alerting and are included with Cloud Monitoring.",wrongExplanations:{1:"Custom Cloud Functions add complexity and costs. Uptime checks provide this functionality natively with better integration.",2:"Compute Engine instances for monitoring are expensive and complex to manage. Uptime checks provide same functionality at lower cost.",3:"Cloud Trace is for distributed tracing and latency analysis, not availability monitoring or uptime status."}},{id:110,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Log sinks",question:"You need to retain audit logs for 5 years for compliance but Cloud Logging default retention is 30 days. What should you do?",options:["Manually export logs monthly","Use BigQuery for all logging","Create log sink to export logs to Cloud Storage with appropriate retention policies","Increase Cloud Logging retention to 5 years"],correct:2,explanation:"Log sinks export logs to Cloud Storage, BigQuery, or Pub/Sub. Cloud Storage with bucket retention policies provides cost-effective long-term retention for compliance.",wrongExplanations:{1:"Cloud Logging supports custom retention (up to 10 years) but it's more expensive than Cloud Storage for long-term retention.",2:"Manual export is unreliable and may miss logs during outages or human error. Automated log sinks ensure continuous export.",3:"BigQuery is optimized for queries, not cold storage. Cloud Storage is more cost-effective for compliance retention with infrequent access."}},{id:111,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Budget best practices",question:"You set up billing budget with 100% threshold alert but still exceed budget. What's the issue?",options:["Google Cloud doesn't enforce budgets","Alert wasn't properly configured","Budget alerts only notify - they don't stop spending. You need automated responses or manual intervention","Budget limits automatically stop resource creation"],correct:2,explanation:"Billing budgets are informational only - send alerts but don't prevent spending. You must take action (manual or automated via Cloud Functions/Pub/Sub) to control costs.",wrongExplanations:{1:"While misconfiguration is possible, more common issue is expecting budgets to enforce limits. Budgets alert, they don't stop spending.",2:"Budget limits don't automatically stop anything. Google Cloud will continue billing regardless of budget settings.",3:"Google tracks spending and sends budget alerts, but budgets are advisory, not enforced limits. You remain responsible for managing costs."}},{id:112,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Shared VPC",question:"Your organization has multiple projects needing private communication and shared network resources. What networking architecture should you implement?",options:["Shared VPC where host project shares VPC networks with service projects","Public IPs for cross-project communication","VPC Network Peering between all projects","Separate VPCs with Cloud VPN connections"],correct:0,explanation:"Shared VPC allows centralized network administration where service projects use networks from host project. Simplifies management and maintains private connectivity.",wrongExplanations:{1:"VPC Peering works but creates mesh topology that becomes complex with many projects. Shared VPC provides better centralized management.",2:"Cloud VPN is for connecting to on-premises or other clouds, not internal GCP project connectivity. Adds unnecessary complexity.",3:"Public IPs expose services to internet and incur egress charges. Private connectivity through Shared VPC is more secure and cost-effective."}},{id:113,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Terraform state",question:"You're using Terraform for production infrastructure. Where should you store the state file?",options:["Cloud Storage bucket with versioning enabled and appropriate IAM controls","Local file on workstation","Store in Cloud SQL database","Commit to Git repository"],correct:0,explanation:"Cloud Storage with versioning provides durable, shared state storage with history. IAM controls limit access. Enables team collaboration and disaster recovery.",wrongExplanations:{1:"Local state files don't allow team collaboration and can be lost. Only suitable for personal testing, never production.",2:"State files contain sensitive information. Committing to Git exposes data and creates merge conflicts with team collaboration.",3:"Cloud SQL adds unnecessary complexity for Terraform state. Cloud Storage is recommended backend with built-in versioning and locking support."}},{id:114,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - SSL certificates",question:"You're setting up HTTPS load balancer and need SSL certificates with automatic renewal. What should you use?",options:["Third-party CA certificates","Google-managed SSL certificates which automatically provision and renew via Let's Encrypt","Self-signed certificates","Manual Let's Encrypt certificates"],correct:1,explanation:"Google-managed certificates automatically provision and renew SSL certificates for your domains. Eliminates manual certificate management overhead.",wrongExplanations:{1:"Self-signed certificates trigger browser warnings and aren't trusted by clients. Only suitable for testing, never production.",2:"Manual Let's Encrypt certificates require renewal every 90 days. Google-managed certificates automate this completely.",3:"Third-party certificates cost money and require manual renewal and upload. Google-managed certificates are free and fully automated."}},{id:115,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Database selection for MySQL",question:"You're migrating MySQL database supporting financial analytics with complex queries requiring strong consistency and ACID transactions. Which database should you choose?",options:["BigQuery with streaming inserts","Firestore in Datastore mode","Cloud Spanner with PostgreSQL interface","Cloud SQL for PostgreSQL with high availability configuration"],correct:3,explanation:"Cloud SQL for PostgreSQL provides managed PostgreSQL service with ACID transactions, strong consistency, full SQL support. HA configuration ensures reliability.",wrongExplanations:{1:"Firestore is NoSQL database that doesn't support complex SQL queries or PostgreSQL compatibility. Designed for document-based, real-time applications.",2:"BigQuery is analytics data warehouse, not OLTP database. Optimized for analytical queries, not transactional workloads with frequent updates.",3:"Spanner is for globally distributed, horizontally scalable workloads. More expensive and complex than needed for PostgreSQL migration."}},{id:116,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - VPC design for on-premises",question:"You need to connect on-premises data center to GCP with predictable bandwidth and low latency without traversing public internet. What should you use?",options:["Cloud VPN with high-bandwidth tunnels","Public internet with VPN encryption","Direct peering with Google","Dedicated Interconnect or Partner Interconnect depending on proximity to Google facilities"],correct:3,explanation:"Dedicated or Partner Interconnect provides private, high-bandwidth, low-latency connections between on-premises and GCP without public internet. Choose based on proximity to Google facilities.",wrongExplanations:{1:"Cloud VPN traverses public internet (encrypted), doesn't meet 'not traverse public internet' requirement and has variable latency.",2:"Direct peering is for accessing Google services, not private connections to your VPC. Doesn't provide needed private connectivity.",3:"Public internet explicitly violates requirement. Even with VPN encryption, traffic still goes over public internet with variable performance."}},{id:117,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Lifecycle management",question:"You have log files in Cloud Storage that must be retained 7 years but are rarely accessed after 90 days. How should you optimize costs?",options:["Create lifecycle policy to move objects to Archive storage after 90 days","Manually move old files to Archive quarterly","Keep everything in Standard storage","Delete files after 90 days and restore from backups if needed"],correct:0,explanation:"Lifecycle policies automatically transition objects between storage classes. Archive storage provides lowest cost for long-term retention while maintaining compliance.",wrongExplanations:{1:"Manual processes are error-prone, don't scale, and may miss files. Lifecycle policies automate transitions reliably.",2:"Different storage classes have same durability and compliance. Standard storage is much more expensive for rarely-accessed data.",3:"Deleting files violates compliance requirements. Archive storage maintains data for required retention at much lower cost."}},{id:118,domain:"Configuring access and security",subdomain:"4.1 IAM - Best practices",question:"Your Cloud Function needs to write to BigQuery. Following best practices, how should you configure access?",options:["Use default App Engine service account","Grant function Owner role for simplicity","Use your personal user account","Create dedicated service account with only BigQuery Data Editor role and assign to function"],correct:3,explanation:"Dedicated service accounts with minimal required permissions (least privilege) are best practice. BigQuery Data Editor allows writing data without unnecessary permissions.",wrongExplanations:{1:"Default App Engine service account has Editor permissions across project - far more than needed. Violates least privilege.",2:"Personal user accounts shouldn't be used for services. Breaks automation and creates security/auditing issues.",3:"Owner role grants full control over all resources - massive security risk. Always follow least privilege."}},{id:119,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Best practices",question:"Your Cloud Function needs to write to Storage and publish to Pub/Sub. How should you configure the service account?",options:["Use default App Engine service account","Use personal account for deployment","Create service account with Owner role","Create dedicated service account with only Storage Object Creator and Pub/Sub Publisher roles"],correct:3,explanation:"Dedicated service account with only required permissions follows least privilege and provides clear audit trails for function's actions.",wrongExplanations:{1:"Default App Engine service account has Editor-level permissions - far more than needed. Violates least privilege.",2:"Owner role grants full control creating massive security risks. If compromised, attackers have complete project access.",3:"Personal accounts break automation, make auditing difficult, tie permissions to person rather than service requirement."}},{id:120,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Project creation",question:"Your company is launching a new product line and needs an isolated environment. You want to ensure proper billing attribution and separate IAM policies. What is the best approach?",options:["Create a new project within your existing organization for the product line","Create a new organization for the product line","Use the existing project with labels to separate resources","Create a separate billing account only"],correct:0,explanation:"Creating a new project within the existing organization provides proper isolation while maintaining centralized governance. Projects are the primary isolation boundary for resources, IAM policies, and billing attribution. The organization provides centralized policy control while projects enable separation of resources and billing.",wrongExplanations:{1:"Organizations are typically limited to one per company domain and are meant for the entire company, not individual product lines. Creating separate organizations prevents centralized governance and adds unnecessary complexity.",2:"Using labels within a single project doesn't provide IAM isolation or true resource separation. Labels are for organization and cost attribution but don't enforce security boundaries or separate IAM policies.",3:"Separate billing account alone doesn't provide resource or IAM isolation. Projects are needed for proper separation. Billing accounts are for payment and cost management, not resource isolation."}},{id:121,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - API management",question:"You need to disable the Compute Engine API for a project to prevent new VM creation while keeping existing VMs running. What happens when you disable the API?",options:["New VMs cannot be created, but existing VMs continue running normally","All existing VMs are immediately terminated","The API cannot be disabled if VMs exist","Existing VMs stop but are not deleted"],correct:0,explanation:"Disabling an API prevents new resource creation using that API but does not affect existing resources. Existing VMs continue to run, can be accessed via SSH, and can be managed through gcloud commands for start/stop operations. This is useful for preventing new resource creation while maintaining existing workloads.",wrongExplanations:{1:"Google Cloud never terminates existing resources when an API is disabled. This would cause major production outages and is not how API management works. Disabling APIs only prevents new resource creation.",2:"APIs can be disabled even if resources exist. Google Cloud allows API disablement at any time and simply prevents new resource creation via that API.",3:"Disabling an API doesn't affect running resources. VMs remain in their current state (running or stopped) and continue operating normally."}},{id:122,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Committed use discounts",question:"Your production workload runs 24/7 on n2-standard-8 instances in us-central1. You want to reduce costs. What pricing model should you choose?",options:["Purchase 1-year or 3-year committed use discounts for predictable 24/7 workloads","Use Spot VMs for production workloads","Use on-demand pricing and rely on sustained use discounts","Purchase reservations for peak usage only"],correct:0,explanation:"Committed use discounts (CUDs) provide up to 57% discount for 1-year or 70% for 3-year commitments on compute resources. They're ideal for steady-state, predictable workloads running continuously. You commit to a certain amount of vCPU and memory in a region and get significant discounts. This is perfect for 24/7 production workloads with predictable resource needs.",wrongExplanations:{1:"Spot VMs can be preempted with 30-second notice and are unsuitable for production workloads requiring high availability. While they offer 60-91% discount, the interruption risk makes them inappropriate for 24/7 production systems.",2:"Sustained use discounts apply automatically but only provide up to 30% discount. For 24/7 workloads, CUDs offer much better savings (up to 57-70%) and are specifically designed for predictable, continuous usage.",3:"Reservations ensure capacity availability in specific zones but don't provide the same cost savings as CUDs. Reservations are for guaranteeing capacity during high-demand periods, while CUDs are for cost optimization on consistent usage."}},{id:123,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Subnet expansion",question:"Your subnet is running out of IP addresses. The current range is 10.0.1.0/24 (256 IPs). You need to expand it to accommodate 500 IPs. What should you do?",options:["Expand the subnet to 10.0.1.0/23 which provides 512 IPs","Delete the subnet and create a new one with larger range","Create a second subnet in the same region","Use alias IP ranges for additional addresses"],correct:0,explanation:"Google Cloud allows expanding subnets by modifying the CIDR mask to be smaller (encompassing more IPs). You can expand 10.0.1.0/24 to 10.0.1.0/23 which doubles the available IPs to 512. Expansion is done in place without downtime or resource recreation. Command: 'gcloud compute networks subnets expand-ip-range SUBNET --prefix-length=23'. This is non-disruptive and maintains all existing resources.",wrongExplanations:{1:"Deleting subnet would require recreating all resources (VMs, forwarding rules, etc.) in that subnet. Subnet expansion can be done in place without disruption. Never delete subnets when expansion is possible.",2:"Creating a second subnet doesn't help existing resources that are bound to the first subnet. Resources can't automatically move between subnets. Expanding the existing subnet is the correct solution.",3:"Alias IP ranges are for assigning multiple IPs to a single VM interface (useful for containers/pods), not for expanding overall subnet capacity. They consume IPs from the same subnet range."}},{id:124,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Instance groups",question:"You need to deploy a stateless web application across multiple zones in a region with automatic scaling based on CPU utilization. What should you use?",options:["Regional managed instance group with autoscaling based on CPU metrics","Zonal managed instance group in single zone","Unmanaged instance group","Multiple single VM instances"],correct:0,explanation:"Regional managed instance groups (MIGs) automatically distribute instances across multiple zones in a region, providing high availability and automatic zone failover. With autoscaling, the MIG automatically adds or removes instances based on CPU utilization. This provides both availability (multi-zone) and scalability (autoscaling) for stateless applications. Regional MIGs are Google's recommended approach for production applications.",wrongExplanations:{1:"Zonal MIG only deploys in a single zone, creating single point of failure for zone outages. For production applications requiring high availability, regional MIGs are preferred as they distribute across multiple zones automatically.",2:"Unmanaged instance groups are just collections of VMs without autoscaling, health checking, or automatic instance recreation. They require manual management and don't provide the automation needed for scaling based on CPU.",3:"Multiple single VMs require manual management, don't autoscale, have no automated health checking, and require custom load balancing configuration. Managed instance groups provide all these features automatically."}},{id:125,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud Spanner",question:"Your global application needs a SQL database with ACID transactions, strong consistency, and ability to scale horizontally. The application serves users across US, Europe, and Asia with single-digit millisecond latency. What should you use?",options:["Cloud Spanner configured as a multi-region instance spanning the required continents","Cloud SQL with read replicas in each region","Multiple regional Cloud SQL instances with application-level replication","Firestore in Datastore mode"],correct:0,explanation:"Cloud Spanner is Google's globally distributed, horizontally scalable relational database designed exactly for this use case. It provides: 1) Full SQL support with ACID transactions, 2) Strong consistency across regions, 3) Automatic sharding and horizontal scaling, 4) Single-digit millisecond latency through strategic multi-region placement, 5) 99.999% availability SLA. Multi-region configuration automatically replicates data across continents while maintaining consistency.",wrongExplanations:{1:"Cloud SQL read replicas use asynchronous replication causing potential data inconsistency. They don't provide the strong consistency requirement. Additionally, Cloud SQL doesn't scale horizontally like Spanner - it's limited to vertical scaling within a single primary instance.",2:"Multiple Cloud SQL instances with application-level replication creates enormous complexity in maintaining consistency, handling conflicts, and ensuring ACID properties across regions. This essentially means building a distributed database yourself, which is what Spanner provides as a managed service.",3:"Firestore is a NoSQL document database that doesn't support SQL or traditional relational schemas. While it can scale globally, it doesn't meet the SQL and relational database requirements."}},{id:126,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Persistent disks",question:"Your application requires high IOPS for database workloads. You need sustained performance of 30,000 IOPS. What disk type should you choose?",options:["SSD persistent disk which provides up to 100,000 IOPS per disk","Standard persistent disk for cost savings","Local SSD for maximum performance","Balanced persistent disk as a compromise"],correct:0,explanation:"SSD persistent disks provide high IOPS (up to 100,000 read IOPS and 100,000 write IOPS per disk) and are designed for high-performance database workloads. They offer the best balance of performance, durability, and features. SSDs provide persistent storage with snapshots, encryption, and the ability to resize or attach to different VMs. For 30,000 IOPS requirement, SSD persistent disk is the appropriate choice providing headroom for growth.",wrongExplanations:{1:"Standard persistent disks are backed by hard drives and provide much lower IOPS (around 3,000 IOPS max). They're designed for throughput-oriented workloads, not IOPS-intensive database operations requiring 30,000 IOPS.",2:"Local SSDs provide highest IOPS (up to 900,000) but are ephemeral - data is lost when VM is stopped or deleted. For database workloads requiring data persistence, local SSDs are inappropriate. They're for temporary caching, not database storage.",3:"Balanced persistent disks offer moderate performance (up to 80,000 IOPS) and cost between standard and SSD. While cheaper than SSD, they don't provide the consistent high performance needed for demanding database workloads requiring 30,000+ IOPS."}},{id:127,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Network tiers",question:"Your global web application needs to minimize latency for users worldwide by using Google's global network. What should you configure?",options:["Premium Network Tier which routes traffic through Google's global network","Standard Network Tier for cost savings","Configure Cloud CDN only","Use multiple regional load balancers"],correct:0,explanation:"Premium Network Tier routes traffic through Google's global private network, providing: 1) Lower latency by avoiding public internet hops, 2) Better reliability and performance, 3) Single global IP address (anycast), 4) Traffic enters Google network at nearest point of presence to user, 5) Optimized routing within Google's network. This is Google's recommended tier for latency-sensitive global applications and is default for most services.",wrongExplanations:{1:"Standard Network Tier routes traffic over public internet, resulting in higher latency and less predictable performance. While cheaper, it doesn't meet the requirement of minimizing latency through Google's global network. Standard tier uses regular internet routing, not Google's private network.",2:"Cloud CDN alone caches static content at edge locations but doesn't address routing for dynamic content or application logic. While CDN helps with static assets, Premium Tier is needed for overall application traffic to use Google's global network.",3:"Multiple regional load balancers create management complexity and don't provide the global network benefits. Premium Tier with global load balancer provides single global IP and automatic routing through Google's network to nearest backend."}},{id:128,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Snapshots",question:"You need to create backup of a persistent disk attached to a running VM. The disk contains a database with active writes. What is the best approach?",options:["Create a snapshot while the VM is running - snapshots are crash-consistent by default","Stop the VM, create snapshot, then restart","Detach the disk, create snapshot, reattach disk","Copy data to Cloud Storage instead"],correct:0,explanation:"Google Cloud snapshots can be created from attached disks while VMs are running. Snapshots are crash-consistent by default, meaning they capture disk state as if the system crashed. For databases, this is often acceptable as databases are designed to recover from crashes. For production databases, you can flush writes to disk before snapshot or use database-specific backup tools. Creating snapshots from running VMs is standard practice and causes no downtime.",wrongExplanations:{1:"Stopping VM creates unnecessary downtime. Snapshots can be created from running VMs without stopping. While stopping ensures no active writes, it's not required and introduces service interruption for most workloads.",2:"Detaching disk requires stopping VM (disks can't be detached from running instances for boot disks). This creates even more downtime than stopping the VM. Snapshots are designed to work with attached disks.",3:"Copying to Cloud Storage is manual process that doesn't leverage Compute Engine's integrated snapshot features. Snapshots are incremental, compressed, and provide fast restore to persistent disks. Manual copying loses these benefits."}},{id:129,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Object lifecycle",question:"Your application generates reports stored in Cloud Storage. Reports are accessed daily for 30 days, then rarely. After 365 days, they can be deleted. How should you configure this?",options:["Create lifecycle policy with age condition to transition to Nearline at 30 days and delete at 365 days","Manually archive files monthly","Use Cloud Scheduler to delete old files","Store in Archive class from the start"],correct:0,explanation:"Object Lifecycle Management policies automate storage class transitions and deletions based on age, creation date, or other conditions. Configuration: 1) Transition to Nearline storage after 30 days (occasional access, lower storage cost), 2) Delete objects after 365 days. This optimizes costs automatically without manual intervention. Lifecycle policies are free and execute automatically across all objects matching conditions.",wrongExplanations:{1:"Manual archival doesn't scale, is error-prone, requires ongoing operational effort, and may miss files. Lifecycle policies handle this automatically and reliably across millions of objects without human intervention.",2:"Cloud Scheduler triggering deletion adds complexity and requires custom implementation. Lifecycle policies provide this functionality natively with better integration and reliability. Don't build what's provided as a managed feature.",3:"Archive storage has high retrieval costs and 365-day minimum storage duration. Storing reports there from start would incur expensive retrieval costs for the frequent first-30-day access period."}},{id:130,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Managing networking - Static IPs",question:"Your VM needs to retain its external IP address permanently, even after stop/start cycles or deletion/recreation. What should you configure?",options:["Reserve a static external IP address and assign it to the VM","Use ephemeral IP address which persists through stop/start","Configure DNS to always point to the VM","Use Cloud NAT for persistent addressing"],correct:0,explanation:"Static (reserved) external IP addresses remain associated with your project until explicitly released. They persist through VM stop/start cycles and can be reassigned to different VMs. Reserve with 'gcloud compute addresses create' then assign to VM. Static IPs incur small hourly charge when not attached to running resources but provide persistent public addressing.",wrongExplanations:{1:"Ephemeral external IPs persist through stop/start of the SAME VM but are released when VM is deleted. They're not truly permanent and can't be moved between VMs. For permanent addressing, static IPs are required.",2:"DNS pointing doesn't make the underlying IP address persistent. If VM is deleted or IP changes, DNS will point to wrong address. You need static IP for the address itself to be permanent.",3:"Cloud NAT provides outbound connectivity for VMs without external IPs. It doesn't provide persistent inbound addressing or external IPs for VMs."}},{id:131,domain:"Configuring access and security",subdomain:"4.1 IAM - Policy troubleshooting",question:"A user reports they cannot create VMs despite having Compute Instance Admin role at project level. What should you check first?",options:["Check if organizational policies restrict VM creation or resource locations","Grant them Owner role instead","Verify their Google account is active","Check VPC firewall rules"],correct:0,explanation:"Organizational policies can override IAM permissions. Policies like 'constraints/compute.vmExternalIpAccess' or 'constraints/gcp.resourceLocations' can prevent VM creation even with proper IAM roles. Always check organizational policies when users have appropriate IAM but can't perform actions. Use 'gcloud resource-manager org-policies list' to view active policies.",wrongExplanations:{1:"Owner role doesn't solve organizational policy restrictions. Policies apply even to Owners. Granting excessive permissions doesn't fix policy-based denials and violates least privilege.",2:"If account was inactive, user couldn't log in at all. Since they're reporting inability to create VMs specifically, account is clearly active. Account status wouldn't cause this specific issue.",3:"Firewall rules control network traffic between resources, not VM creation permissions. Firewall rules have nothing to do with IAM or ability to create VMs."}},{id:132,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - GPU instances",question:"Your machine learning workload requires GPUs for training models. What must you do before creating GPU-attached VMs?",options:["Request GPU quota increase for your project in the specific region and zone","Enable the GPU API","Create a custom machine type","Install GPU drivers before attaching GPU"],correct:0,explanation:"GPU quota is separate from CPU quota and defaults to 0 in most projects/regions. You must request GPU quota increase through IAM & Admin > Quotas page, specifying GPU type (K80, P4, T4, V100, A100, etc.) and region/zone. Once granted, you can create GPU-attached VMs. GPU availability also varies by zone, so check availability for your target zone.",wrongExplanations:{1:"There's no separate 'GPU API' to enable. GPUs are part of Compute Engine API. If you can create VMs, the necessary APIs are already enabled.",2:"Custom machine types are for CPU/memory combinations. GPU attachment is independent of machine type customization. You can attach GPUs to predefined or custom machine types.",3:"GPU drivers are installed AFTER VM creation, not before. You create the VM with GPU attached, then install appropriate drivers (NVIDIA CUDA, etc.) in the OS. Google provides GPU-optimized images with drivers pre-installed."}},{id:133,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud SQL backup",question:"Your Cloud SQL instance needs automated backups with ability to restore to any point in time within the last 7 days. What should you configure?",options:["Enable automated backups and enable point-in-time recovery (binary logging)","Create manual snapshots daily via cron job","Export database to Cloud Storage daily","Enable Cloud SQL high availability only"],correct:0,explanation:"Cloud SQL automated backups + point-in-time recovery (PITR) provide: 1) Automatic daily backups, 2) Binary logs (MySQL) or write-ahead logs (PostgreSQL) enabling restore to any second within retention period, 3) 7-day retention by default (configurable up to 365 days), 4) Automated and reliable. PITR requires additional storage for logs but provides granular recovery not possible with daily backups alone.",wrongExplanations:{1:"Manual snapshots via cron require custom implementation, can fail without alerting, don't provide point-in-time recovery between snapshots, and require ongoing maintenance. Cloud SQL's native backup solution is superior.",2:"Exporting to Cloud Storage provides data copy but requires custom restore process, doesn't integrate with Cloud SQL restore features, and doesn't provide point-in-time recovery capability.",3:"High availability prevents downtime from zone failures but doesn't provide backup/restore capabilities. HA is for availability, not disaster recovery. You need both HA and backups for complete protection."}},{id:134,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Custom metrics",question:"Your application tracks business metrics (orders per minute, revenue) that you want to monitor and alert on. How should you send these metrics to Cloud Monitoring?",options:["Use Cloud Monitoring API or client libraries to write custom metrics from your application","Write metrics to Cloud Logging and create log-based metrics","Store metrics in BigQuery and query periodically","Use Cloud Functions to scrape application endpoints"],correct:0,explanation:"Cloud Monitoring API and client libraries (available in Python, Java, Node.js, Go, etc.) allow applications to write custom metrics directly. Create custom metric descriptors, then write time series data points. Custom metrics integrate with all Cloud Monitoring features: dashboards, alerting, anomaly detection. This is the recommended approach for application-specific business metrics.",wrongExplanations:{1:"Log-based metrics work but are less efficient than native custom metrics. They require parsing log entries and have higher latency. Custom metrics via API are more performant and have better integration with monitoring features.",2:"BigQuery is for data warehousing and analysis, not real-time monitoring. Querying BigQuery periodically adds latency, complexity, and cost. Cloud Monitoring is purpose-built for metrics collection and alerting.",3:"Cloud Functions scraping adds latency, requires custom implementation, creates scaling challenges, and is less reliable than direct instrumentation. Applications should push metrics directly to Cloud Monitoring."}},{id:135,domain:"Configuring access and security",subdomain:"4.3 Security - Secret Manager",question:"Your application needs to access database credentials, API keys, and certificates. What is the recommended approach for managing these secrets?",options:["Store secrets in Secret Manager and access them programmatically using client libraries with automatic encryption and audit logging","Store secrets in environment variables","Commit secrets to version control encrypted with GPG","Store secrets in Cloud Storage with encryption"],correct:0,explanation:"Secret Manager is Google Cloud's purpose-built service for managing sensitive data: 1) Automatic encryption at rest and in transit, 2) Versioning of secrets, 3) Audit logging of all access, 4) IAM integration for access control, 5) Automatic rotation support, 6) Replication options. Access via API or client libraries. This is the recommended Google Cloud approach for centralized secret management.",wrongExplanations:{1:"Environment variables are visible to anyone with access to the VM/container and can be accidentally logged or exposed. They lack encryption, versioning, audit logging, and rotation capabilities that Secret Manager provides.",2:"Version control, even with encryption, is inappropriate for secrets: 1) Secrets remain in Git history permanently, 2) Anyone with repo access can decrypt, 3) No audit trail of secret access, 4) Difficult to rotate secrets, 5) Version control is for code, not secrets.",3:"Cloud Storage lacks specialized secret management features: no native versioning, requires custom access patterns, doesn't provide automatic audit logging of secret access, no built-in rotation support. Use purpose-built Secret Manager instead."}},{id:136,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Container options",question:"You have a containerized application that needs to scale to zero when idle, startup in under 1 second, and you want minimal operational overhead. What should you use?",options:["Cloud Run for fully managed container execution with automatic scale-to-zero","GKE Autopilot with custom scaling configuration","Cloud Functions repackaging your container","Compute Engine with container-optimized OS"],correct:0,explanation:"Cloud Run is specifically designed for this use case: 1) Fully managed (no infrastructure management), 2) Automatic scale-to-zero (no charges when idle), 3) Fast cold starts (typically under 1 second), 4) Supports any container, 5) Pay only for request processing time, 6) Built-in HTTPS endpoints, 7) Request-based autoscaling. Cloud Run is the simplest option for stateless containerized applications.",wrongExplanations:{1:"GKE Autopilot can scale to zero but requires more complex configuration and has Kubernetes overhead. For simple containerized apps, Cloud Run provides simpler solution with same scale-to-zero benefits.",2:"Cloud Functions requires specific language runtimes and doesn't directly support existing containers. You'd need to repackage your application, losing the benefit of existing container images.",3:"Compute Engine with containers requires managing VMs, doesn't scale to zero automatically, requires manual autoscaling configuration, and has much longer startup times than Cloud Run."}},{id:137,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Custom images",question:"You need to deploy 100 identical VMs with pre-installed software and configurations. What is the most efficient approach?",options:["Create a custom image from a configured VM, then create new VMs from that image","Use startup scripts to install software on each VM","Manually configure each VM after creation","Use Cloud Deployment Manager with inline scripts"],correct:0,explanation:"Custom images capture entire disk state including OS, software, and configurations. Create base VM, install software, create image, then use image to create new VMs. Benefits: 1) Fastest deployment (no installation time per VM), 2) Guaranteed consistency across all VMs, 3) Reduced bandwidth (no repeated downloads), 4) Lower error rate than repeated installations. Command: 'gcloud compute images create IMAGE --source-disk=SOURCE_DISK'.",wrongExplanations:{1:"Startup scripts install software on every VM boot, increasing deployment time significantly. For 100 VMs, this means 100x downloads and installations. Slower, consumes more bandwidth, and risks installation failures on some VMs.",2:"Manual configuration of 100 VMs is time-consuming, error-prone, not reproducible, and doesn't scale. Custom images eliminate manual work and ensure consistency.",3:"Deployment Manager with scripts still requires running installation on each VM. While automated, it's slower than using pre-configured images and consumes more resources."}},{id:138,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Load balancer SSL policies",question:"Your HTTPS load balancer needs to support only modern browsers with TLS 1.2+ and strong cipher suites for security compliance. What should you configure?",options:["Create and apply an SSL policy with MODERN or RESTRICTED profile to the load balancer","SSL policies are automatic and cannot be customized","Configure SSL policies on backend VMs","Use Cloud Armor for SSL configuration"],correct:0,explanation:"SSL policies define minimum TLS version and cipher suites for HTTPS load balancers. Google provides profiles: COMPATIBLE (broadest support, TLS 1.0+), MODERN (TLS 1.2+ with secure ciphers), RESTRICTED (TLS 1.2+ with most secure ciphers only), CUSTOM (define specific ciphers). Create policy: 'gcloud compute ssl-policies create POLICY --profile MODERN' then apply to load balancer. This enforces TLS requirements at load balancer level before reaching backends.",wrongExplanations:{1:"SSL policies are customizable and should be configured to meet security requirements. Default policy may allow older TLS versions not meeting compliance requirements. Explicit policy configuration is needed.",2:"SSL policies apply to load balancer (SSL termination point), not backend VMs. Load balancer handles SSL/TLS negotiation with clients, backends typically receive HTTP (unencrypted) traffic from load balancer.",3:"Cloud Armor provides DDoS protection and WAF features, not SSL/TLS configuration. SSL policies are separate feature in load balancing configuration."}},{id:139,domain:"Configuring access and security",subdomain:"4.1 IAM - Service account keys",question:"Your on-premises application needs to access Google Cloud APIs. What is the recommended authentication approach?",options:["Create service account key file, download JSON, and use in application with Application Default Credentials","Use user account credentials with OAuth","Create API key in Cloud Console","Use no authentication for internal APIs"],correct:0,explanation:"For applications running outside Google Cloud (on-premises, other clouds), service account keys are the recommended authentication method. Download JSON key file, store securely, set GOOGLE_APPLICATION_CREDENTIALS environment variable pointing to key file, use client libraries with Application Default Credentials. While keys require careful management, they're the standard approach for external applications. Consider Workload Identity Federation for even better security.",wrongExplanations:{1:"User account credentials tie authentication to individual humans, not services. This breaks when user leaves, makes auditing difficult, and violates principle of service identity. Services should use service accounts.",2:"API keys provide only identification, not authentication or authorization. They're for quota management and shouldn't be used for accessing protected resources. Service accounts provide proper authentication and IAM integration.",3:"All Google Cloud API access requires authentication. 'Internal' APIs still need proper authentication to enforce IAM policies and provide audit trails. Never leave APIs unauthenticated."}},{id:140,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - DNS",question:"You need to host your company's DNS zone (example.com) in Google Cloud with automatic DNSSEC. What should you use?",options:["Cloud DNS managed zone with DNSSEC enabled","Compute Engine VM running BIND","Use third-party DNS only","Cloud Load Balancer for DNS"],correct:0,explanation:"Cloud DNS is Google's managed, authoritative DNS service providing: 1) High availability (100% SLA), 2) Low latency via global anycast, 3) Automatic DNSSEC support, 4) Support for public and private zones, 5) Geo-routing and weighted routing, 6) API-driven for automation, 7) Integration with other Google Cloud services. Create managed zone: 'gcloud dns managed-zones create ZONE --dns-name=example.com --dnssec-state=on'.",wrongExplanations:{1:"Self-managed DNS on Compute Engine requires: VM management, software updates, HA configuration, backup strategy, monitoring, and security hardening. Cloud DNS eliminates all operational overhead while providing better reliability and performance.",2:"Third-party DNS providers work but don't integrate with Google Cloud features and may have higher latency for users accessing Cloud resources. Cloud DNS provides native integration and Google's network benefits.",3:"Load balancers distribute traffic to backends, they don't provide DNS hosting services. Cloud DNS and load balancers serve different purposes - DNS resolves names to IPs, load balancers distribute traffic."}},{id:141,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Transfer Service",question:"You need to migrate 500TB of data from AWS S3 to Cloud Storage. What is the recommended approach for large-scale transfer?",options:["Use Storage Transfer Service which handles large-scale transfers efficiently between cloud providers","Use gsutil rsync command from local workstation","Write custom multi-threaded application","Download to local disk then upload to GCS"],correct:0,explanation:"Storage Transfer Service is designed for large-scale data transfers: 1) Optimized for cloud-to-cloud transfers (AWS S3, Azure Blob, Google Cloud Storage), 2) Parallel transfers for high throughput, 3) Handles retries automatically, 4) Scheduled and one-time transfers, 5) Filters by filename, prefix, date, 6) Integrity verification, 7) Optional deletion of source objects after transfer. For 500TB, this is far more reliable and efficient than manual methods.",wrongExplanations:{1:"gsutil from local workstation limits transfer speed to your internet bandwidth. For 500TB, this could take weeks or months. Additionally, local workstation reliability issues could interrupt transfer. Storage Transfer Service uses Google's network backbone for much faster transfer.",2:"Custom applications require significant development, testing, error handling, resumption logic, and monitoring. Storage Transfer Service provides all this as managed service. Never build what's available as managed service.",3:"Downloading 500TB to local disk then uploading doubles transfer time and requires massive local storage. Storage Transfer Service performs direct cloud-to-cloud transfer using high-bandwidth cloud networks, avoiding local bottleneck entirely."}},{id:142,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your application running on GKE needs shared file system (NFS) accessible by multiple pods simultaneously for read/write. What should you use?",options:["Cloud Filestore which provides managed NFS file shares accessible from GKE","Persistent volumes with ReadWriteOnce access mode","Cloud Storage FUSE mount","Host path volumes"],correct:0,explanation:"Cloud Filestore is Google's managed NFS service providing: 1) POSIX-compliant file system, 2) Multiple concurrent readers/writers (ReadWriteMany in K8s), 3) Low latency suitable for application data, 4) Snapshots and backups, 5) Native integration with GKE. Create Filestore instance, mount as PersistentVolume with ReadWriteMany access mode. Perfect for shared application state, home directories, or content management systems.",wrongExplanations:{1:"ReadWriteOnce persistent volumes can only be attached to one node at a time. Multiple pods on same node can access it, but not across nodes. This doesn't meet requirement for multiple pods across cluster to access simultaneously.",2:"Cloud Storage FUSE provides object storage semantics, not true file system. Has limitations: high latency, eventual consistency for some operations, limited POSIX compliance. Not suitable for applications requiring true shared file system with concurrent writes.",3:"Host path volumes are node-local storage, not shared across nodes. Each node has different host path content. Can't be used for shared data across multiple pods on different nodes."}},{id:143,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Impersonation",question:"You need to grant a developer temporary ability to act as a service account for testing without giving them the service account key. What should you do?",options:["Grant the developer Service Account Token Creator role to impersonate the service account","Give developer the service account key file","Add developer's user account to the service account","Create new service account for developer"],correct:0,explanation:"Service account impersonation allows users to temporarily assume service account identity without keys. Grant 'roles/iam.serviceAccountTokenCreator' on the service account. Developer can then use 'gcloud --impersonate-service-account' or API calls to act as the service account. Benefits: 1) No key files to manage, 2) Auditable (logs show who impersonated), 3) Can be revoked instantly, 4) Time-limited tokens. This is much more secure than distributing keys.",wrongExplanations:{1:"Distributing key files creates security risks: keys can be stolen, don't rotate automatically, remain valid until deleted, harder to audit. Impersonation is more secure as it uses short-lived tokens and maintains audit trail.",2:"You cannot add user accounts 'to' service accounts. Service accounts are separate identities. Impersonation is the mechanism to allow users to act as service accounts.",3:"Creating new service account per developer defeats purpose of service account (representing service, not individual). Multiplies IAM management overhead and doesn't solve testing problem of using actual service account identity."}},{id:144,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Sole-tenant nodes",question:"Your workload has regulatory requirements that VMs must run on dedicated physical servers not shared with other Google customers. What should you use?",options:["Sole-tenant nodes which provide dedicated physical servers for your VMs","Preemptible VMs for isolation","Custom machine types","Shielded VMs for security"],correct:0,explanation:"Sole-tenant nodes are physical Compute Engine servers dedicated exclusively to your project. Provides: 1) Physical isolation from other customers, 2) Ability to group VMs on same hardware, 3) Access to same machine types as regular VMs, 4) Meet regulatory requirements for dedicated hardware, 5) Useful for BYOL scenarios requiring host affinity. Create node group, then create VMs specifying node group. Costs more but provides required physical isolation.",wrongExplanations:{1:"Preemptible VMs are for cost savings, not isolation. They run on shared infrastructure alongside other customers' VMs, just like regular VMs. They provide no additional physical isolation.",2:"Custom machine types affect CPU/memory configuration but don't provide dedicated hardware. Custom machines still run on shared physical infrastructure with other customers.",3:"Shielded VMs provide boot security features (Secure Boot, vTPM, integrity monitoring) but run on shared infrastructure. They protect against boot-level attacks, not provide physical isolation from other customers."}},{id:145,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - BigQuery costs",question:"Your BigQuery costs are high due to analysts running ad-hoc queries scanning large amounts of data. How can you provide cost controls while maintaining query access?",options:["Create custom cost controls by setting query cost limits per user and implementing query quotas","Switch to flat-rate pricing for everyone","Disable BigQuery access","Only allow scheduled queries"],correct:0,explanation:"BigQuery provides cost control features: 1) Custom query quotas per user/project, 2) Maximum bytes billed settings, 3) Required table partitioning, 4) Query costs preview before execution, 5) Cost controls in Data Studio/Looker, 6) IAM policies limiting query capabilities. Set project-level or user-level quotas: 'bq update --project_id=PROJECT --max_bytes_billed=BYTES'. Analysts can still query but within cost boundaries.",wrongExplanations:{1:"Flat-rate pricing ($40k-$170k/year minimum) only makes sense for very high query volumes. For teams with analysts running ad-hoc queries, on-demand pricing with quotas is more cost-effective and flexible.",2:"Disabling access defeats the purpose of having BigQuery. Analytics teams need query access. The solution is controlling costs, not eliminating access.",3:"Scheduled queries are useful but don't address ad-hoc analysis needs. Analysts need interactive query capability for exploration and investigation. Cost controls allow both scheduled and ad-hoc queries within budget."}},{id:146,domain:"Configuring access and security",subdomain:"4.3 Security - Cloud Armor",question:"Your public-facing application needs protection against DDoS attacks and application-layer attacks. What should you configure?",options:["Cloud Armor security policies on your load balancer with rate limiting and WAF rules","VPC firewall rules only","Cloud IDS for intrusion detection","Binary Authorization for container security"],correct:0,explanation:"Cloud Armor provides: 1) DDoS protection (L3/L4 and L7), 2) WAF rules for OWASP Top 10 protection, 3) Rate limiting per IP/region, 4) Geo-based access control, 5) Custom rules based on request attributes, 6) Bot management, 7) Adaptive protection using machine learning. Applied to HTTP(S) load balancers. Essential for protecting public applications from malicious traffic.",wrongExplanations:{1:"VPC firewall rules provide basic L4 filtering but lack L7 application awareness, rate limiting, WAF rules, or DDoS mitigation capabilities. They can't inspect HTTP requests or detect application-layer attacks.",2:"Cloud IDS provides network intrusion detection but is designed for internal traffic inspection, not DDoS protection or WAF for public applications. It detects intrusions but doesn't actively block them like Cloud Armor.",3:"Binary Authorization ensures only approved containers run in GKE. It's for supply chain security, not runtime protection against network attacks or malicious users. Completely different security layer."}},{id:147,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Profiler",question:"Your application has high CPU usage and you need to identify which functions are consuming most CPU time. What tool should you use?",options:["Cloud Profiler to analyze CPU and memory usage at the function level with flame graphs","Cloud Monitoring CPU metrics only","Cloud Trace for latency analysis","Cloud Logging for application logs"],correct:0,explanation:"Cloud Profiler is a continuous profiling tool that: 1) Shows which functions consume CPU and memory, 2) Provides flame graphs visualizing call stacks, 3) Low overhead (safe for production), 4) Compares performance across versions, 5) Works with multiple languages (Java, Go, Python, Node.js), 6) Identifies hotspots and optimization opportunities. Install profiler agent in application code. Perfect for optimizing application performance at code level.",wrongExplanations:{1:"Cloud Monitoring CPU metrics show overall resource usage but don't identify which code/functions are responsible. You see symptoms (high CPU) but not root cause (which functions). Profiler provides the function-level detail needed for optimization.",2:"Cloud Trace shows request flow and latency between services but doesn't profile CPU usage within individual functions. Trace is for distributed system latency, Profiler is for CPU/memory optimization.",3:"Cloud Logging captures application output but logs don't show CPU profiling data unless you instrument code to log profiling info (complex and high overhead). Profiler is purpose-built for performance analysis."}},{id:148,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Confidential VMs",question:"Your workload processes sensitive data and requires memory encryption. What type of VM should you use?",options:["Confidential VMs which provide memory encryption using AMD SEV","Shielded VMs for boot security","Standard VMs with encrypted persistent disks","Sole-tenant nodes for isolation"],correct:0,explanation:"Confidential VMs provide: 1) Memory encryption using AMD SEV (Secure Encrypted Virtualization), 2) Encryption keys generated in hardware, not accessible to Google, 3) Protection of data in use (in memory), 4) Built on N2D machine types with AMD EPYC processors, 5) Minimal performance overhead. Create with '--confidential-compute' flag. Complements encryption at rest (disks) and in transit (network) with encryption of data in use.",wrongExplanations:{1:"Shielded VMs provide boot security (Secure Boot, vTPM, integrity monitoring) but don't encrypt memory. They protect boot process and detect malware at boot, but data in memory is not encrypted.",2:"Encrypted persistent disks protect data at rest but not data in memory while being processed. Disk encryption and memory encryption solve different problems and are complementary.",3:"Sole-tenant nodes provide physical isolation but don't encrypt memory. VMs on sole-tenant nodes process data in clear memory just like regular VMs. Physical isolation  memory encryption."}},{id:149,domain:"Configuring access and security",subdomain:"4.4 Compliance - Cloud Asset Inventory",question:"You need to track all resources in your organization for security audits and compliance. What tool provides inventory and history of all resources?",options:["Cloud Asset Inventory which tracks resource configurations and changes over time","Cloud Monitoring for resource tracking","Cloud Logging export to BigQuery","Manual spreadsheet of resources"],correct:0,explanation:"Cloud Asset Inventory: 1) Maintains inventory of all GCP resources, 2) Tracks IAM policies, 3) Provides resource history and change tracking, 4) Supports querying and searching across organization, 5) Exports to BigQuery for analysis, 6) Real-time feed of asset changes, 7) Essential for compliance, security analysis, and auditing. Query with 'gcloud asset search-all-resources'.",wrongExplanations:{1:"Cloud Monitoring tracks metrics and logs but isn't designed for comprehensive resource inventory. It shows operational state but not complete resource catalog with configurations and policies.",2:"Cloud Logging captures events but doesn't maintain comprehensive current inventory of all resources. You'd need complex queries to reconstruct resource inventory from logs, and historical log data may not be retained.",3:"Manual spreadsheets can't keep pace with dynamic cloud environments. Resources are created and deleted programmatically. Manual tracking is outdated immediately and prone to errors. Cloud Asset Inventory provides automated, always-current inventory."}},{id:150,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your web application needs automatic scaling, zero server management, and supports Python. Traffic varies from 10 to 10,000 requests per second. What should you use?",options:["App Engine Standard with automatic scaling","App Engine Flexible with manual scaling","Compute Engine with autoscaling","Cloud Functions"],correct:0,explanation:"App Engine Standard provides true zero-ops with automatic scaling from 0 to thousands of instances within seconds, built-in load balancing, and pay-per-use pricing.",wrongExplanations:{1:"Flexible with manual scaling doesn't autoscale and requires specifying instance count, defeating the purpose for variable traffic.",2:"Compute Engine requires managing VMs, configuring autoscaling, and load balancers - much more overhead than App Engine Standard.",3:"Cloud Functions is for event-driven workloads and short executions, not full web applications needing longer request handling."}},{id:151,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"Your application needs sub-millisecond latency for cache lookups with Redis compatibility. What should you use?",options:["Memorystore for Redis providing fully managed Redis with high availability","Cloud SQL with query caching","BigQuery with BI Engine","Persistent disk for cache storage"],correct:0,explanation:"Memorystore for Redis is Google's managed Redis service providing sub-millisecond latency, automatic failover, backup/restore, and Redis compatibility.",wrongExplanations:{1:"Cloud SQL is a relational database, not an in-memory cache. Query times are milliseconds to seconds, not sub-millisecond.",2:"BigQuery is for analytics, not application caching. BI Engine accelerates queries but isn't a general-purpose cache.",3:"Persistent disks have millisecond latencies, not sub-millisecond. Disk-based storage can't match in-memory cache performance."}},{id:152,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Private Service Connect",question:"You need to access a Google API privately without traffic leaving your VPC to the internet. What should you configure?",options:["Private Service Connect endpoint for Google APIs","Cloud NAT for API access","VPC Peering with Google","Public internet with SSL"],correct:0,explanation:"Private Service Connect allows accessing Google APIs through internal IP addresses in your VPC without internet traversal, providing security and lower latency.",wrongExplanations:{1:"Cloud NAT provides internet egress but traffic still goes to public Google API endpoints, not staying within VPC network.",2:"You cannot VPC peer with Google's production networks. Private Service Connect is the mechanism for private API access.",3:"Public internet access doesn't meet the requirement of keeping traffic within VPC network."}},{id:153,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Metadata server",question:"Your application running on Compute Engine needs to retrieve its own external IP address. What is the recommended method?",options:["Query the metadata server at http://metadata.google.internal/computeMetadata/v1/","Parse output of ifconfig command","Hard-code the IP in application configuration","Use external service to detect IP"],correct:0,explanation:"Metadata server provides instance information including external IP, zone, tags, service accounts. Query with 'Metadata-Flavor: Google' header. Standard method for VMs to discover their configuration.",wrongExplanations:{1:"ifconfig shows internal network interfaces, not external IP assigned by Google Cloud.",2:"Hard-coding IPs breaks when VMs are recreated or IPs change. Metadata server provides current values dynamically.",3:"External services add latency and dependency. Metadata server is local and reliable."}},{id:154,domain:"Configuring access and security",subdomain:"4.1 IAM - Resource hierarchy permissions",question:"A user has Editor role at organization level and Viewer role at project level. What effective permissions does the user have in the project?",options:["Editor permissions - roles granted at higher levels override lower level restrictions","Viewer permissions - lower level takes precedence","No access - conflicting roles cancel out","Custom combination of both roles"],correct:0,explanation:"IAM uses union of permissions - more permissive role wins. Organization-level Editor includes all Viewer permissions and more. Permissions inherit down and accumulate, cannot be reduced at lower levels.",wrongExplanations:{1:"Lower levels cannot restrict permissions granted at higher levels. Organization-level Editor provides Editor access everywhere below.",2:"IAM doesn't cancel permissions. If conflicting roles grant different access, the union of all permissions applies.",3:"Roles combine additively, not as custom mix. User gets maximum permissions granted at any level."}},{id:155,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Bigtable",question:"Your IoT application generates 1 million writes per second with low-latency read requirements. Data is time-series sensor readings. What database should you use?",options:["Cloud Bigtable designed for high-throughput, low-latency time-series data","Cloud SQL for relational data","Cloud Storage for object storage","Firestore for document storage"],correct:0,explanation:"Bigtable handles millions of writes per second with single-digit millisecond latency. Perfect for time-series data, IoT, analytics. Scales horizontally, integrates with data processing tools.",wrongExplanations:{1:"Cloud SQL maxes at thousands of writes per second, not millions. Designed for OLTP, not massive time-series ingestion.",2:"Cloud Storage is for objects/files, not high-throughput database operations with low-latency reads.",3:"Firestore handles thousands of writes per second, not millions. Better for mobile/web apps than IoT time-series at this scale."}},{id:156,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Object holds",question:"You must prevent deletion of critical objects in Cloud Storage for legal reasons even by users with delete permissions. What should you configure?",options:["Object holds (event-based or temporary) prevent deletion regardless of IAM permissions","Remove delete permissions from all users","Make bucket requester pays","Enable versioning only"],correct:0,explanation:"Object holds override IAM permissions. Event-based holds prevent deletion until hold is removed. Temporary holds last until specific date. Critical for legal/compliance requirements where even admins can't delete.",wrongExplanations:{1:"Removing delete permissions doesn't work if users regain permissions or use elevated accounts. Holds enforce retention regardless of permissions.",2:"Requester pays affects billing for access, not deletion protection. Completely unrelated to preventing deletion.",3:"Versioning retains deleted object versions but doesn't prevent deletion. Objects can still be deleted, they just remain as non-current versions."}},{id:157,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Batch workloads",question:"You have nightly data processing jobs taking 6 hours. Jobs can be paused and resumed. How should you minimize costs?",options:["Use Spot VMs with checkpointing to handle interruptions and achieve 60-91% cost savings","Use committed use discounts for 24/7 capacity","Run on App Engine Flexible","Use Cloud Functions with maximum timeout"],correct:0,explanation:"Spot VMs perfect for batch jobs that can tolerate interruptions. Implement checkpointing to save progress. 60-91% discount vs on-demand. For 6-hour nightly jobs, massive savings with acceptable interruption risk.",wrongExplanations:{1:"CUDs require 24/7 commitment but jobs only run 6 hours daily (25% utilization). Spot VMs are cheaper for intermittent workloads.",2:"App Engine Flexible isn't designed for long-running batch processing. More expensive and complex than Compute Engine for this use case.",3:"Cloud Functions has 60-minute maximum timeout. Can't run 6-hour jobs. Wrong tool for long batch processing."}},{id:158,domain:"Configuring access and security",subdomain:"4.3 Security - VPC Service Controls",question:"You need to prevent data exfiltration from BigQuery to external locations. What should you implement?",options:["VPC Service Controls perimeter around BigQuery project allowing only authorized access","IAM deny policies for external destinations","Firewall rules blocking egress","Remove all user permissions"],correct:0,explanation:"VPC Service Controls create security perimeter preventing data leaving defined resources even with IAM permissions. Blocks copying to external projects, on-premises, or internet. Primary defense against exfiltration.",wrongExplanations:{1:"IAM controls who can access but can't prevent authorized users from copying data to external locations. VPC Service Controls enforce perimeter restrictions.",2:"Firewall rules apply to Compute Engine network traffic, not API-level data access like BigQuery queries or exports.",3:"Removing permissions breaks legitimate use. VPC Service Controls allow normal operations while preventing data leaving perimeter."}},{id:159,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - SLOs",question:"You want to track that 99.9% of requests complete in under 500ms. What Cloud Monitoring feature should you use?",options:["Create Service Level Objective (SLO) with latency SLI and error budget tracking","Create alerting policy only","Use uptime checks","Export metrics to spreadsheet"],correct:0,explanation:"SLOs in Cloud Monitoring track service reliability against targets. Define SLI (latency <500ms), set target (99.9%), monitor error budget. Provides visibility into service health and reliability trends.",wrongExplanations:{1:"Alerts notify when threshold crossed but don't track reliability over time or error budget consumption. SLOs provide comprehensive reliability tracking.",2:"Uptime checks monitor availability, not request latency percentiles. Can't track 99.9% of requests meeting latency target.",3:"Manual spreadsheet tracking is error-prone and doesn't provide real-time monitoring or error budget alerts."}},{id:160,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud Interconnect",question:"You need 10 Gbps private connection to Google Cloud that doesn't traverse internet. Your office is 500 miles from nearest Google facility. What should you use?",options:["Partner Interconnect which provides 10 Gbps through supported service provider","Dedicated Interconnect requiring colocation","Cloud VPN maxing at 3 Gbps","Public internet connection"],correct:0,explanation:"Partner Interconnect provides enterprise-grade connectivity through service provider network when you can't colocate. Supports 50 Mbps to 50 Gbps. Doesn't require Google facility proximity. Private connection without internet traversal.",wrongExplanations:{1:"Dedicated Interconnect requires physical connection in Google colocation facility. At 500 miles away, not practical. Partner Interconnect solves distance issue.",2:"Cloud VPN maxes at 3 Gbps per tunnel and traverses internet. Doesn't meet 10 Gbps or private connection requirements.",3:"Public internet doesn't provide private connection or bandwidth guarantees. Violates requirements."}},{id:161,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Short-lived credentials",question:"Your application needs service account token valid for only 5 minutes. How should you generate it?",options:["Use Service Account Credentials API to generate short-lived access token with custom lifetime","Create service account key and rotate every 5 minutes","Use user credentials instead","No way to create tokens shorter than 1 hour"],correct:0,explanation:"Service Account Credentials API generates OAuth access tokens with configurable lifetime (as short as 5 minutes). Use 'iam.serviceAccounts.generateAccessToken'. Much more secure than long-lived keys. Automatic expiration.",wrongExplanations:{1:"Rotating keys every 5 minutes is operationally complex and unnecessary. Keys are meant to be long-lived. Short-lived tokens are the proper solution.",2:"User credentials tie authentication to individuals, not services. Doesn't solve need for short-lived service tokens.",3:"Service Account Credentials API explicitly supports creating tokens with custom expiration as short as 1 second."}},{id:162,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Routes",question:"Your VMs need to route traffic to on-premises 10.1.0.0/16 network through Cloud VPN. What should you configure?",options:["Custom route with destination 10.1.0.0/16 and next-hop as VPN tunnel","Firewall rule allowing 10.1.0.0/16","NAT rule for translation","VPC Peering configuration"],correct:0,explanation:"Routes control traffic forwarding. Create custom route: 'gcloud compute routes create vpn-route --destination-range=10.1.0.0/16 --next-hop-vpn-tunnel=TUNNEL'. BGP can exchange routes automatically, but static routes work too.",wrongExplanations:{1:"Firewall rules permit/deny traffic but don't control routing. Even with allow rule, traffic won't reach on-premises without route.",2:"NAT translates IP addresses, doesn't route traffic. VPN connectivity requires proper routing, not NAT.",3:"VPC Peering connects two VPCs in Google Cloud. Can't peer with on-premises networks. VPN requires route configuration."}},{id:163,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Serial console",question:"Your VM is unresponsive to SSH. How can you troubleshoot if it's booting correctly?",options:["Access serial console output to view boot messages and system logs","Delete and recreate the VM","Wait for automatic recovery","Check Cloud Monitoring only"],correct:0,explanation:"Serial console provides access to VM's serial port output showing boot messages, kernel logs, login prompts. Essential for troubleshooting when SSH unavailable. View with 'gcloud compute instances get-serial-port-output'.",wrongExplanations:{1:"Deleting VM loses ability to diagnose issue and may destroy evidence of root cause. Serial console enables troubleshooting without destruction.",2:"VMs don't automatically recover from many issues. Serial console helps identify problem so you can fix it.",3:"Monitoring shows metrics but not boot logs or system errors visible in serial console."}},{id:164,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Transfer Appliance",question:"You need to transfer 500TB to Google Cloud but have limited bandwidth (10 Mbps). What is the most practical approach?",options:["Request Transfer Appliance, ship physical device with your data to Google","Use gsutil over internet connection","Use Storage Transfer Service","Mail hard drives directly"],correct:0,explanation:"Transfer Appliance is physical device Google ships for large offline data transfer. Load data locally at high speed, ship back to Google. Practical for multi-TB transfers with limited bandwidth. 10 Mbps would take years for 500TB.",wrongExplanations:{1:"gsutil over 10 Mbps for 500TB would take approximately 12 years. Completely impractical. Physical transfer is only viable option.",2:"Storage Transfer Service requires internet connectivity and is subject to same bandwidth limitations. No benefit over gsutil for this scenario.",3:"Google doesn't accept random hard drives. Transfer Appliance is the official physical transfer solution with encryption, tracking, and support."}},{id:165,domain:"Configuring access and security",subdomain:"4.1 IAM - Domain restricted sharing",question:"You need to ensure resources can only be shared with users from your organization's domain. What should you configure?",options:["Organization policy constraint 'iam.allowedPolicyMemberDomains' limiting to your domain","Manually review all IAM bindings","Remove all external users","Enable Cloud Identity only"],correct:0,explanation:"The 'iam.allowedPolicyMemberDomains' organizational policy prevents sharing resources with users outside specified domains. Enforces domain restriction across organization. Preventive control.",wrongExplanations:{1:"Manual review doesn't prevent future violations and doesn't scale. Organizational policy enforces rule automatically.",2:"Removing external users is reactive and one-time. New external users could be added. Policy prevents addition in first place.",3:"Cloud Identity provides user management but doesn't restrict resource sharing. Policy enforcement needed."}},{id:166,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Regional replication",question:"Your Cloud Storage bucket must replicate data to another region for disaster recovery. What should you configure?",options:["Use dual-region or multi-region bucket for automatic replication","Create script to copy objects between regions","Use gsutil rsync scheduled job","Enable versioning only"],correct:0,explanation:"Dual-region buckets automatically replicate between two specific regions. Multi-region buckets replicate across multiple regions. Provides geo-redundancy automatically. Can't convert single-region to dual-region - must create new bucket.",wrongExplanations:{1:"Scripts require maintenance, can fail, have replication lag, and require monitoring. Dual/multi-region provides automatic, reliable replication.",2:"gsutil rsync works but is manual, has lag, costs more (egress charges), and requires automation infrastructure. Dual-region is simpler.",3:"Versioning preserves object history but doesn't replicate to other regions. Different feature solving different problem."}},{id:167,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Windows licensing",question:"You want to use your existing Windows Server licenses on Compute Engine. What should you do?",options:["Use sole-tenant nodes with Bring Your Own License (BYOL) for Windows Server","Use regular VMs with Google-provided licenses","Install Windows without license","Use App Engine for Windows workloads"],correct:0,explanation:"BYOL requires sole-tenant nodes for licensing compliance. Allows using existing licenses, avoiding Google's Windows licensing costs. Must have license mobility rights. Configure with '--node-group' specifying sole-tenant group.",wrongExplanations:{1:"Regular VMs include Windows licensing in per-second pricing. Can't bring your own licenses to shared infrastructure. Must use sole-tenant for BYOL.",2:"Running unlicensed Windows violates licensing terms and creates legal/compliance issues. Not a valid option.",3:"App Engine doesn't support Windows. It's for web applications in specific languages, not Windows Server workloads."}},{id:168,domain:"Configuring access and security",subdomain:"4.3 Security - Shielded VMs",question:"You need to ensure VMs boot only with verified firmware and kernel. What feature should you enable?",options:["Shielded VM with Secure Boot, vTPM, and integrity monitoring","Confidential VM for encryption","Sole-tenant nodes for isolation","Custom image with hardening"],correct:0,explanation:"Shielded VMs provide: 1) Secure Boot (verified bootloader/kernel), 2) vTPM (virtual Trusted Platform Module), 3) Integrity monitoring (detect boot-level changes). Protects against rootkits and bootkits. Enable with '--shielded-secure-boot'.",wrongExplanations:{1:"Confidential VMs encrypt memory but don't verify boot integrity. Different security layer for data in use, not boot security.",2:"Sole-tenant nodes provide physical isolation but don't verify boot integrity. VMs on sole-tenant nodes can still have compromised bootloaders.",3:"Custom hardening helps but doesn't provide hardware-backed boot verification that Shielded VMs offer through Secure Boot."}},{id:169,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Notification channels",question:"Your alerting policy needs to notify multiple teams via different methods (email, Slack, PagerDuty). How should you configure this?",options:["Add multiple notification channels to the alerting policy","Create separate alerting policies for each team","Use Cloud Functions to fan out notifications","Send emails only and let teams forward"],correct:0,explanation:"Alerting policies support multiple notification channels. Configure email, Slack, PagerDuty, SMS, webhooks, Pub/Sub. Each channel delivers to different team/system. Simple configuration, no custom code.",wrongExplanations:{1:"Separate policies for same condition creates management overhead. One policy can notify multiple channels. Duplicating policies causes confusion.",2:"Cloud Functions add unnecessary complexity. Cloud Monitoring natively supports multiple notification channels without custom code.",3:"Manual forwarding is unreliable and defeats purpose of automated alerting. Direct notification ensures timely delivery."}},{id:170,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - AlloyDB",question:"You need PostgreSQL-compatible database with 4x faster transaction performance than standard PostgreSQL. What should you use?",options:["AlloyDB for PostgreSQL which provides high-performance PostgreSQL with Google innovations","Cloud SQL for PostgreSQL","Cloud Spanner with PostgreSQL interface","Self-managed PostgreSQL on Compute Engine"],correct:0,explanation:"AlloyDB is Google's fully managed PostgreSQL-compatible database offering 4x faster transactions, 100x faster analytics queries, built-in caching, and column store. Combines transactional and analytical workloads. More advanced than Cloud SQL.",wrongExplanations:{1:"Cloud SQL PostgreSQL offers standard performance. AlloyDB provides significantly better performance through proprietary optimizations.",2:"Spanner PostgreSQL interface provides global scale but doesn't offer the same transaction performance or PostgreSQL compatibility as AlloyDB.",3:"Self-managed PostgreSQL on Compute Engine provides standard PostgreSQL performance without AlloyDB's proprietary optimizations and managed features."}},{id:171,domain:"Configuring access and security",subdomain:"4.1 IAM - Deny policies",question:"You need to prevent all users including Owners from deleting production VMs. What should you configure?",options:["IAM deny policy blocking compute.instances.delete for all principals on production VMs","Remove delete permissions from all users","Organization policy only","Firewall rules"],correct:0,explanation:"IAM deny policies override allow policies. Can block specific actions even for Owners. Apply to resources with tags/labels. 'gcloud iam policies create' with deny rules for compute.instances.delete. Enforces constraints that allow policies can't.",wrongExplanations:{1:"Removing permissions doesn't prevent Owners who have all permissions by default. Deny policies can override even Owner role.",2:"Organization policies provide constraints but IAM deny policies offer more granular control over specific actions. Both can work together.",3:"Firewall rules control network traffic, not IAM permissions for VM deletion. Completely different security layer."}},{id:172,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Instance templates",question:"You need to standardize VM configurations for development teams to ensure consistency. What should you create?",options:["Instance template defining machine type, boot disk, network, metadata for standardized VM creation","Custom image only","Documentation with manual steps","Startup script"],correct:0,explanation:"Instance templates are immutable VM configuration blueprints. Define all VM properties: machine type, image, disks, network, service account, labels. Used to create individual VMs or instance groups. Ensures consistency, simplifies deployment.",wrongExplanations:{1:"Custom images define disk content but not VM configuration (machine type, networking, etc.). Templates include image plus infrastructure configuration.",2:"Documentation requires manual following, error-prone, not enforced. Templates programmatically enforce standard configurations.",3:"Startup scripts configure software after boot but don't define VM infrastructure settings. Templates define complete VM specification."}},{id:173,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud CDN",question:"Your global website has static assets (images, CSS, JavaScript) accessed by users worldwide. How can you reduce latency?",options:["Enable Cloud CDN on your load balancer to cache content at Google edge locations globally","Deploy VMs in every region","Use Cloud Storage only","Enable Premium Network Tier only"],correct:0,explanation:"Cloud CDN caches content at Google's 100+ edge locations worldwide. Dramatically reduces latency for static content. Enable on HTTP(S) load balancer with cache-mode. Supports custom cache keys, TTLs, cache invalidation.",wrongExplanations:{1:"Deploying VMs globally is expensive and complex. CDN provides caching without infrastructure in every location. VMs still needed for origin content.",2:"Cloud Storage alone doesn't provide edge caching. CDN caches Storage content at edge locations, reducing latency and Storage egress costs.",3:"Premium Tier improves network routing but doesn't cache content at edge. CDN and Premium Tier complement each other."}},{id:174,domain:"Configuring access and security",subdomain:"4.4 Compliance - Data residency",question:"Regulations require customer data stay within EU. How do you enforce this in Google Cloud?",options:["Set organization policy 'gcp.resourceLocations' restricting resources to EU regions only","Train developers to use EU regions only","Manually audit resources monthly","Use IAM policies"],correct:0,explanation:"Organization policy 'constraints/gcp.resourceLocations' enforces data residency by limiting resource creation to specified regions. Set to 'in:eu-locations' to restrict to EU. Preventive control, not reactive.",wrongExplanations:{1:"Training is good but not enforceable. Developers can make mistakes. Policy prevents creation outside EU automatically.",2:"Manual audit is reactive and allows violations to exist for up to a month. Policy prevents violations from occurring.",3:"IAM controls who can do what, not where resources can be created. Location restrictions require organizational policies."}},{id:175,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Bucket locking",question:"You need to prevent anyone including admins from reducing Cloud Storage bucket retention period once set. What should you enable?",options:["Retention policy with bucket lock to make retention period immutable","Object versioning","Lifecycle management","IAM policy only"],correct:0,explanation:"Bucket lock makes retention policy immutable. Once locked, retention period can only be increased, never decreased or removed. Critical for regulatory compliance. Lock is permanent and irreversible.",wrongExplanations:{1:"Versioning preserves object history but doesn't prevent retention policy changes. Different feature.",2:"Lifecycle management automates object lifecycle but doesn't lock retention policies. Can be modified or removed.",3:"IAM policies can be changed by admins. Bucket lock prevents even project owners from reducing retention."}},{id:176,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Accelerators",question:"Your video transcoding workload could benefit from hardware acceleration. What should you attach to VMs?",options:["GPU (T4, P4) for video encoding/decoding acceleration","TPU for machine learning","More vCPUs only","Local SSD for speed"],correct:0,explanation:"GPUs like NVIDIA T4 have hardware video encoders/decoders (NVENC/NVDEC) dramatically accelerating transcoding. Much faster and more power-efficient than CPU transcoding. Attach GPU to VM and use encoding software supporting hardware acceleration.",wrongExplanations:{1:"TPUs are specifically for machine learning tensor operations, not video transcoding. Wrong accelerator type.",2:"More vCPUs help but can't match GPU hardware encoders' efficiency. GPU transcoding is 10-100x faster than CPU.",3:"Local SSD speeds up storage I/O but doesn't accelerate actual encoding/decoding computations. Need GPU for video acceleration."}},{id:177,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Automatic creation",question:"You enable Compute Engine API and notice a service account was automatically created. What is this account?",options:["Default Compute Engine service account used by VMs when no custom service account specified","Your user account","Google's service account","Random account requiring deletion"],correct:0,explanation:"Google Cloud automatically creates default service accounts when enabling certain APIs. Compute Engine default SA has Editor role. VMs use it by default. Best practice: create custom service accounts with minimal permissions instead.",wrongExplanations:{1:"Default service accounts are separate from user accounts. They're for resources, not humans.",2:"These aren't Google's operational accounts. They belong to your project for resource use.",3:"Default service accounts are intentional, not errors. Don't delete unless you've assigned custom accounts to all resources using them."}},{id:178,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Dashboards",question:"You need to create custom dashboard showing VM CPU, memory, and application metrics together. What should you use?",options:["Cloud Monitoring dashboard with widgets displaying different metric types","BigQuery for visualization","Spreadsheet with manual data entry","Cloud Logging only"],correct:0,explanation:"Cloud Monitoring dashboards support multiple widget types: line charts, stacked area, heatmaps, gauges. Combine infrastructure and custom metrics. Share dashboards across team. Real-time auto-refresh.",wrongExplanations:{1:"BigQuery is for data analysis, not real-time operational monitoring. Dashboards provide real-time visibility.",2:"Manual spreadsheets can't show real-time data, don't auto-refresh, require constant updating. Monitoring dashboards are purpose-built solution.",3:"Cloud Logging shows logs, not metric visualizations. Monitoring provides metric dashboards and charts."}},{id:179,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Pub/Sub",question:"Your application components need asynchronous messaging with guaranteed delivery. What should you use?",options:["Pub/Sub for reliable, scalable messaging between independent components","Cloud Storage for message files","Direct HTTP calls between services","Shared database for queues"],correct:0,explanation:"Pub/Sub provides: 1) At-least-once delivery guarantee, 2) Automatic scaling, 3) Message retention (7 days), 4) Topic and subscription model, 5) Push and pull delivery, 6) Decouples publishers and subscribers. Perfect for event-driven architectures.",wrongExplanations:{1:"Cloud Storage isn't a message queue. No delivery guarantees, subscriber notification, or message ordering. Wrong tool for messaging.",2:"Direct HTTP calls couple services tightly, require handling failures/retries, and don't buffer when subscriber unavailable. Pub/Sub provides reliable async messaging.",3:"Shared database queues are DIY solution requiring complex implementation. Pub/Sub provides managed, scalable messaging without custom queue management."}},{id:180,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to process uploaded images automatically when they arrive in a Cloud Storage bucket. The processing takes 2-5 seconds per image and happens infrequently (a few times per hour). What is the most cost-effective solution?",options:["Run a Compute Engine instance continuously polling the bucket","Deploy a Cloud Function (2nd gen) triggered by Cloud Storage events","Deploy a GKE cluster with a CronJob checking the bucket every minute","Use Cloud Run with scheduled Cloud Scheduler checking the bucket"],correct:1,explanation:"Cloud Functions 2nd gen with Cloud Storage triggers provide event-driven processing with no infrastructure management. You only pay for execution time (seconds of processing), making it extremely cost-effective for infrequent, short-duration tasks. The function automatically scales to zero when not in use.",wrongExplanations:{1:"A continuously running Compute Engine instance wastes resources and costs money 24/7, even when no images are uploaded. This is far more expensive than serverless functions that only run on-demand.",2:"GKE is overkill for simple image processing. It requires cluster management, continuous node running costs, and is designed for complex containerized applications, not simple event-driven tasks.",3:"Cloud Scheduler with Cloud Run requires polling the bucket at regular intervals, processing whether files exist or not. Cloud Functions with storage triggers are more efficient and respond immediately to uploads."}},{id:181,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your Cloud Function needs to call multiple Google Cloud APIs and access Cloud SQL. The function execution time is unpredictable and can take up to 15 minutes. Which Cloud Functions generation should you use?",options:["Split the work into multiple 1st gen functions chained together","Cloud Functions 2nd gen is not suitable; use Cloud Run instead","Cloud Functions 1st gen with maximum 9-minute timeout","Cloud Functions 2nd gen, which supports up to 60-minute timeouts and better networking"],correct:3,explanation:"Cloud Functions 2nd gen (built on Cloud Run) supports execution timeouts up to 60 minutes, compared to 9 minutes for 1st gen. It also provides better VPC networking capabilities, making it ideal for database connections and long-running processes.",wrongExplanations:{1:"1st gen functions have a maximum timeout of 9 minutes, which is insufficient for this 15-minute workload. The function would be terminated before completing.",2:"While Cloud Run would work, Cloud Functions 2nd gen is built on Cloud Run infrastructure and provides the same capabilities with simpler deployment for function-style code. There's no need to avoid 2nd gen functions here.",3:"Chaining multiple functions adds unnecessary complexity, requires managing state between functions, and introduces potential failure points. 2nd gen functions handle this workload directly."}},{id:182,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You're deploying a Cloud Function that processes sensitive customer data. The function must not be publicly accessible and should only be invoked by authenticated services within your organization. What should you configure?",options:["Use VPC Service Controls without authentication","Deploy the function with --allow-unauthenticated flag and use API keys","Set the function to require authentication and grant roles/cloudfunctions.invoker to specific service accounts","Put the function behind Cloud Armor for protection"],correct:2,explanation:"Setting authentication requirements and granting roles/cloudfunctions.invoker only to specific service accounts follows the principle of least privilege. This ensures only authorized services can invoke the function, using Google Cloud's IAM for secure authentication.",wrongExplanations:{1:"The --allow-unauthenticated flag makes the function publicly accessible to anyone on the internet. API keys alone are not sufficient security for sensitive data and can be leaked or stolen.",2:"Cloud Armor protects against DDoS and web attacks but doesn't provide authentication or authorization. It's designed for load balancers, not for controlling function access.",3:"VPC Service Controls provide network perimeter security but don't replace authentication. You still need IAM-based authentication to control who can invoke the function."}},{id:183,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Cloud Functions",question:"Your Cloud Function is experiencing cold start latency issues, causing timeouts for the first request after periods of inactivity. Users report 5-10 second delays. What can you do to minimize cold starts?",options:["Configure minimum instances to keep at least one instance warm","Increase the function's memory allocation to speed up cold starts","Add retry logic in the client application","Switch to 1st gen Cloud Functions which have faster cold starts"],correct:0,explanation:"Setting minimum instances (available in Cloud Functions 2nd gen) keeps instances warm and ready to handle requests, eliminating cold start delays. This incurs a small cost for the idle instances but ensures consistent performance.",wrongExplanations:{1:"While more memory can slightly reduce cold start time, it doesn't eliminate cold starts entirely. You'll still experience delays when all instances have scaled to zero.",2:"1st gen Cloud Functions generally have longer cold starts than 2nd gen. Switching generations doesn't solve the cold start problem and moves you to older technology.",3:"Retry logic helps handle failures but doesn't prevent cold starts. Users would still experience the initial 5-10 second delay, followed by a retry with another potential cold start."}},{id:184,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to deploy a function that processes Pub/Sub messages containing order data. The function must handle up to 1000 messages per second during peak hours. What should you configure?",options:["Deploy multiple separate functions to distribute the load","Deploy Cloud Function 1st gen with default settings","Deploy Cloud Function 2nd gen with Pub/Sub trigger and configure max instances to handle peak load","Use Compute Engine with a Pub/Sub pull subscription instead"],correct:2,explanation:"Cloud Functions 2nd gen with Pub/Sub triggers automatically scale to handle message volume. Configuring max instances prevents runaway costs while ensuring capacity for 1000 messages/second. Each function instance can handle concurrent requests, efficiently processing the message stream.",wrongExplanations:{1:"1st gen functions have more limited concurrency and scaling capabilities compared to 2nd gen. For high-throughput scenarios like 1000 messages/second, 2nd gen provides better performance and concurrency handling.",2:"Deploying multiple functions adds unnecessary complexity and requires custom load distribution logic. Cloud Functions automatically scale horizontally to handle load without manual distribution.",3:"Compute Engine requires managing infrastructure, autoscaling configuration, and deployment complexity. Cloud Functions provide automatic scaling and simpler management for event-driven workloads."}},{id:185,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Cloud Functions",question:"Your Cloud Function needs to write data to BigQuery and read secrets from Secret Manager. Following least privilege, what should you do?",options:["Store BigQuery and Secret Manager credentials in environment variables","Create a dedicated service account with roles/bigquery.dataEditor and roles/secretmanager.secretAccessor, assign it to the function","Use the default App Engine service account which has broad permissions","Grant roles/owner to the function's service account"],correct:1,explanation:"Creating a dedicated service account with only the specific roles needed (BigQuery data editor and Secret Manager accessor) follows the principle of least privilege. This limits the blast radius if the function is compromised.",wrongExplanations:{1:"The default App Engine service account has roles/editor at the project level, which is far too broad. This violates least privilege and could allow the function to modify resources it shouldn't access.",2:"roles/owner grants full control over the project, including ability to delete resources, modify IAM policies, and access all data. This is the opposite of least privilege and represents a significant security risk.",3:"Environment variables are not secure for storing credentials. Secret Manager is specifically designed for this purpose. Also, you can't store service account credentials for BigQuery in environment variables - you need to use IAM service account authentication."}},{id:186,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to deploy a Cloud Function that connects to a Cloud SQL instance in a private VPC. The function should not be accessible from the internet. What configuration is required?",options:["Deploy the function in GKE instead to access the VPC","Deploy Cloud Functions 2nd gen with VPC connector, configure Cloud SQL private IP, and disable public access to the function","Deploy Cloud Functions 1st gen with Cloud SQL proxy","Use public IP for Cloud SQL and restrict by IP allowlist"],correct:1,explanation:"Cloud Functions 2nd gen supports VPC connectors for private networking, allowing direct connection to Cloud SQL private IP addresses. Disabling public access to the function ensures it can only be invoked by authenticated callers, not from the internet.",wrongExplanations:{1:"While Cloud SQL proxy works with 1st gen functions, 2nd gen functions with VPC connectors provide better VPC integration and networking capabilities, including support for private IPs without requiring the proxy.",2:"Using public IP for Cloud SQL exposes the database to the internet, requiring management of IP allowlists. This is less secure than private IP communication within a VPC.",3:"GKE is unnecessary overhead for a simple function. Cloud Functions 2nd gen with VPC connector provides the same private networking capabilities without the complexity of managing a Kubernetes cluster."}},{id:187,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring and logging - Cloud Functions",question:"Your Cloud Function is failing intermittently with timeout errors. You need to investigate the root cause. Where should you look first?",options:["Restart the function deployment","Check the function's source code for syntax errors","Check Cloud Logging for function logs and Cloud Trace for request latency breakdown","Increase the function timeout and memory allocation"],correct:2,explanation:"Cloud Logging captures function execution logs, including errors, warnings, and custom log messages. Cloud Trace shows detailed latency breakdown of function execution and downstream API calls, helping identify bottlenecks causing timeouts.",wrongExplanations:{1:"Increasing timeout and memory without understanding the root cause is treating symptoms, not the problem. The function might have an infinite loop, slow API calls, or inefficient code that needs fixing first.",2:"Restarting a function deployment doesn't fix intermittent timeout issues, which are typically caused by performance problems in the code or downstream dependencies, not deployment state.",3:"Syntax errors would cause the function to fail to deploy or fail immediately on every invocation, not intermittently. Timeouts suggest a performance or resource issue, not syntax problems."}},{id:188,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You have a Cloud Function triggered by HTTP requests that performs data validation. During load testing, you notice some requests are taking 30 seconds while others complete in 2 seconds. What is the most likely cause?",options:["Cold starts when new instances are created to handle increased load","Cloud Functions throttling your requests","The function code has a performance bug","Network latency to Google Cloud"],correct:0,explanation:"Cold starts occur when Cloud Functions creates new instances to handle load. The first request to a new instance experiences initialization latency (loading runtime, dependencies, code). Subsequent requests on warm instances are much faster. This explains the 30s vs 2s variance.",wrongExplanations:{1:"Network latency to Google Cloud is typically measured in milliseconds, not tens of seconds. Network issues wouldn't cause such dramatic variance between requests.",2:"While a performance bug could cause slow execution, it would affect most or all requests consistently. The dramatic difference (30s vs 2s) and occurrence during load testing specifically points to cold starts as instances scale up.",3:"Cloud Functions don't throttle by adding delay to requests. Throttling would result in rejected requests (429 errors) or rate limiting, not variable latency within successful requests."}},{id:189,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your team is debating between Cloud Functions and Cloud Run for a new microservice. The service processes webhook events, has predictable traffic patterns, and requires specific runtime dependencies. When would Cloud Functions be the better choice?",options:["When you need fine-grained control over container configuration","Cloud Run is always better than Cloud Functions","When you need custom Dockerfile for complex runtime dependencies","When the code follows simple function patterns and you want minimal configuration and deployment simplicity"],correct:3,explanation:"Cloud Functions excel for simple, function-based code with standard runtime dependencies (Node.js, Python, Go, Java, etc.). They offer the simplest deployment model with minimal configuration. For webhook processing with standard libraries, Functions provide easier development and deployment.",wrongExplanations:{1:"If you need custom Dockerfiles for complex dependencies, Cloud Run is the better choice. Cloud Functions support standard runtimes with requirements.txt or package.json for dependencies, but not custom container images.",2:"Cloud Run provides fine-grained control over container configuration, including CPU, memory, concurrency, and custom containers. Cloud Functions offer simpler deployment but less configuration control. This scenario favors Run, not Functions.",3:"This is false. Both services have valid use cases. Cloud Functions are better for simple event-driven functions, while Cloud Run is better for containerized applications, complex dependencies, or when you need more control."}},{id:190,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to deploy a Cloud Function that runs on a schedule to generate daily reports. The function takes 5 minutes to execute and runs every day at 2 AM. What is the correct approach?",options:["Configure Cloud Function with HTTP trigger and call it manually each day","Use GKE CronJob instead","Create a Cloud Scheduler job with Pub/Sub topic, configure Cloud Function with Pub/Sub trigger","Use a Compute Engine instance with crontab"],correct:2,explanation:"Cloud Scheduler publishes messages to Pub/Sub on a cron schedule. The Cloud Function with Pub/Sub trigger automatically executes when messages arrive. This is the serverless, managed approach for scheduled function execution with no infrastructure to manage.",wrongExplanations:{1:"Compute Engine with crontab requires running and managing a VM 24/7 just to execute a 5-minute daily task. This is wasteful and expensive compared to serverless Cloud Functions that only consume resources during execution.",2:"Manual execution defeats the purpose of automation and is error-prone. Cloud Scheduler provides reliable, automated scheduling without manual intervention.",3:"GKE CronJob requires managing a Kubernetes cluster for a simple scheduled task. This is significant overhead compared to Cloud Functions + Cloud Scheduler, which require no infrastructure management."}},{id:191,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Cloud Functions",question:"After deploying a new version of your Cloud Function, users report errors. You need to quickly roll back to the previous working version. What should you do?",options:["Use gcloud functions deploy with the previous version's code or redeploy from source control","Delete the function and create it again from scratch","Use gcloud functions rollback command","Wait for automatic rollback to occur"],correct:0,explanation:"Cloud Functions don't have built-in version management or automatic rollback. To roll back, you redeploy the previous version using 'gcloud functions deploy' with the old source code. Best practice is to maintain versioned source code in git for easy rollback.",wrongExplanations:{1:"Deleting and recreating the function causes downtime and loses configuration like environment variables, IAM bindings, and triggers. Redeploying the previous version maintains all configuration while only changing the code.",2:"Cloud Functions don't have automatic rollback capabilities. You must manually deploy the previous version. This is why version control and CI/CD practices are important for functions.",3:"There is no 'gcloud functions rollback' command. Cloud Functions don't maintain version history automatically. You must redeploy using the standard deploy command with previous source code."}},{id:192,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your Cloud Function needs to process large files (up to 500MB) uploaded to Cloud Storage. The processing involves reading the entire file into memory. What should you consider?",options:["Use 1st gen functions which have better file handling","Configure sufficient memory for the function (up to 32GB in 2nd gen) and consider streaming processing to reduce memory usage","Cloud Functions cannot handle files over 100MB","Split files before uploading using client-side logic"],correct:1,explanation:"Cloud Functions 2nd gen supports up to 32GB of memory, sufficient for 500MB files. However, best practice is to use streaming processing where possible to reduce memory footprint, improve performance, and reduce costs. Consider processing files in chunks rather than loading entirely into memory.",wrongExplanations:{1:"This is incorrect. Cloud Functions 2nd gen can handle files much larger than 100MB with appropriate memory configuration. The limit is based on available memory (up to 32GB), not file size.",2:"Splitting files adds complexity to both upload and processing logic. It's better to use the function's memory configuration and streaming processing. File splitting should be a last resort, not the first solution.",3:"1st gen functions have lower memory limits (up to 8GB) compared to 2nd gen (up to 32GB). For large file processing, 2nd gen provides better capabilities, not 1st gen."}},{id:193,domain:"Configuring access and security",subdomain:"4.2 Security best practices - Cloud Functions",question:"You need to provide a third-party service temporary access to invoke your Cloud Function for testing purposes. Following security best practices, what should you do?",options:["Share your personal Google account credentials","Create a service account with roles/cloudfunctions.invoker, generate a short-lived token, and revoke access after testing","Create an API key and share it with the third party","Make the function public with --allow-unauthenticated"],correct:1,explanation:"Creating a dedicated service account with only the necessary permission (cloudfunctions.invoker) follows least privilege. Using short-lived tokens and revoking access after testing limits the window of potential unauthorized access. Service accounts can be easily disabled or deleted.",wrongExplanations:{1:"Sharing personal credentials violates security best practices and may breach your organization's policies. Personal accounts often have broad permissions beyond just the function, creating security risks.",2:"Making the function public removes all access control, exposing it to anyone on the internet. This creates security vulnerabilities even if only temporarily, and you might forget to re-enable authentication.",3:"API keys alone don't provide sufficient security for Cloud Functions. They can be leaked, don't expire automatically, and don't integrate with IAM. Service accounts with short-lived tokens are more secure."}},{id:194,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your company processes payment transactions using Cloud Functions. The function must guarantee exactly-once processing of each transaction. How can you achieve this?",options:["Deploy multiple function instances for redundancy","Cloud Functions automatically guarantee exactly-once processing","Implement idempotency in the function code by tracking processed transaction IDs in Firestore or Cloud SQL","Use Pub/Sub with exactly-once delivery setting"],correct:2,explanation:"Exactly-once processing must be implemented at the application level through idempotency. Track processed transaction IDs in a database (Firestore, Cloud SQL, etc.) and check before processing. Cloud Functions themselves provide at-least-once guarantees, so your code must handle potential duplicates.",wrongExplanations:{1:"Cloud Functions provide at-least-once guarantees, not exactly-once. In failure scenarios, a function may be retried, causing duplicate invocations. Your application code must implement idempotency to handle this.",2:"Pub/Sub delivers messages at-least-once, not exactly-once. Even with Pub/Sub, your function code must implement idempotency checks to ensure transactions are processed only once.",3:"Multiple instances increase availability but don't guarantee exactly-once processing. In fact, they increase the chance of duplicate processing if requests are retried. Idempotency logic is still required."}},{id:195,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring and logging - Cloud Functions",question:"You want to monitor the error rate and execution time of your Cloud Functions to detect performance degradation. What should you set up?",options:["Read Cloud Logging logs manually every day","Cloud Functions don't provide performance metrics","Create Cloud Monitoring dashboards with function execution metrics and set up alerting policies for error rate and latency thresholds","Use third-party monitoring tools only"],correct:2,explanation:"Cloud Monitoring automatically collects Cloud Functions metrics including execution count, error count, execution time, and memory usage. Creating dashboards visualizes these metrics, and alerting policies proactively notify you when thresholds are exceeded, enabling quick response to issues.",wrongExplanations:{1:"Manual log review is time-consuming, error-prone, and reactive. You won't detect issues until you happen to check the logs. Cloud Monitoring provides automated, proactive alerting.",2:"While third-party tools can augment monitoring, Cloud Monitoring provides native integration with Cloud Functions metrics at no additional cost. It's best to use the built-in monitoring capabilities first.",3:"This is false. Cloud Functions automatically export detailed metrics to Cloud Monitoring, including execution times, error rates, instance counts, and resource usage."}},{id:196,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your Cloud Function needs to fan out work to multiple downstream services based on event data. For example, when an order is placed, you need to notify inventory, billing, and shipping services. What pattern should you use?",options:["Have the function publish to multiple Pub/Sub topics, with each downstream service subscribing to its relevant topic","Store events in a database and have services poll for new events","Make synchronous HTTP calls to all services from within the function","Deploy separate functions for each downstream service"],correct:0,explanation:"Publishing to multiple Pub/Sub topics decouples services and provides reliable, asynchronous delivery. Each service independently subscribes to relevant topics and processes messages at its own pace. This follows event-driven architecture best practices and allows services to scale independently.",wrongExplanations:{1:"Synchronous HTTP calls create tight coupling between services and increase function execution time. If any downstream service is slow or unavailable, the function is delayed or fails. This doesn't scale well and reduces reliability.",2:"Database polling is inefficient, adds latency (waiting for next poll cycle), and creates unnecessary database load. Pub/Sub provides immediate event delivery without polling overhead.",3:"Deploying separate functions for each downstream service creates unnecessary duplication of the fan-out logic. One function publishing to multiple topics is simpler and more maintainable."}},{id:197,domain:"Configuring access and security",subdomain:"4.3 Managing encryption - Cloud Functions",question:"Your Cloud Function processes sensitive healthcare data and must encrypt environment variables containing API keys. What is the recommended approach?",options:["Encrypt environment variables manually before deployment","Store secrets in Secret Manager and access them from the function code using the Secret Manager API","Use Cloud KMS to encrypt environment variables","Store secrets in function source code"],correct:1,explanation:"Secret Manager is designed specifically for storing and managing sensitive data like API keys. It provides encryption at rest, access logging, versioning, and fine-grained IAM control. Functions can retrieve secrets at runtime using the Secret Manager API with appropriate IAM permissions.",wrongExplanations:{1:"While you could manually encrypt environment variables, managing the encryption keys and decryption logic adds complexity. Secret Manager handles encryption, rotation, and access control automatically.",2:"Storing secrets in source code is a critical security violation. Source code is often committed to version control, shared among developers, and may be exposed in logs or error messages. Never store secrets in code.",3:"While Cloud KMS can encrypt data, Secret Manager is the preferred solution for secrets as it's purpose-built for this use case. It provides additional features like versioning, audit logging, and easier access patterns specifically designed for secrets."}},{id:198,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You're migrating a Python web application to Google Cloud. The application requires automatic scaling, zero server management, and support for background workers. Traffic is variable with occasional spikes. Which deployment option is most suitable?",options:["Compute Engine with managed instance groups","App Engine Standard Environment with automatic scaling and task queues for background jobs","App Engine Flexible Environment with manual scaling","GKE with manual scaling configuration"],correct:1,explanation:"App Engine Standard provides automatic scaling with zero instance management, scales to zero when idle (cost-effective), and integrates with Cloud Tasks for background workers. It's ideal for web applications with variable traffic patterns and supports Python natively.",wrongExplanations:{1:"Flexible Environment doesn't scale to zero and requires at least one instance running always, increasing costs. Manual scaling defeats the purpose of serverless benefits for variable traffic.",2:"Compute Engine requires managing VMs, load balancers, autoscaling configuration, and OS updates. This adds operational overhead compared to App Engine's fully managed approach.",3:"GKE requires managing a Kubernetes cluster, which is unnecessary complexity for a standard web application. App Engine provides simpler deployment and management for typical web apps."}},{id:199,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine application needs to install custom system libraries and run containers that require root access during initialization. Which App Engine environment should you use?",options:["Neither; use Cloud Run instead","App Engine Flexible Environment, which supports custom Dockerfiles and root access","App Engine Standard Environment is always the better choice","App Engine Standard Environment with buildpacks"],correct:1,explanation:"App Engine Flexible Environment runs your application in Docker containers on Compute Engine VMs, allowing custom Dockerfiles, system library installation, and root access during initialization. Standard Environment runs in a sandbox with limited system access.",wrongExplanations:{1:"App Engine Standard Environment runs in a secure sandbox that doesn't allow custom system libraries or root access. It's designed for applications using standard runtime dependencies.",2:"This is incorrect. Both environments have valid use cases. Standard is better for simpler apps with standard dependencies, while Flexible is better for apps requiring custom system libraries or container configuration.",3:"While Cloud Run supports custom containers, if you're already using App Engine features like traffic splitting, versions, and integrated services, Flexible Environment provides those benefits with custom container support."}},{id:200,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - App Engine",question:"You deployed a new version of your App Engine application and users report errors. You need to quickly route all traffic back to the previous working version. What should you do?",options:["Delete the new version","Redeploy the previous version code","App Engine automatically rolls back on errors","Use traffic splitting to route 100% of traffic to the previous version"],correct:3,explanation:"App Engine maintains multiple versions simultaneously. Traffic splitting allows instant migration between versions without redeployment. You can route 100% of traffic back to the working version immediately, then debug the new version before migrating traffic again.",wrongExplanations:{1:"Deleting the version doesn't immediately fix the issue for users currently being served. You need to actively route traffic to the working version first. Deletion should happen after confirming the rollback works.",2:"Redeploying creates a new version and takes time. Traffic splitting provides instant rollback by leveraging existing deployed versions. This is faster and safer.",3:"App Engine does not automatically roll back on errors. You must manually manage traffic between versions. This gives you control over when and how to roll back."}},{id:201,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine Standard application needs to process uploaded files that take 15 minutes per file. Standard Environment has a 10-minute request timeout. What is the best solution?",options:["Use Cloud Tasks to queue the file processing job and have a separate service process it asynchronously","Increase the timeout limit in App Engine Standard","Process files in the upload request handler","Switch to App Engine Flexible which has 60-minute timeout"],correct:0,explanation:"Cloud Tasks provides asynchronous job queuing, allowing the upload request to complete quickly while processing happens separately. The task handler can run for up to 10 minutes, but you can chain tasks or use Cloud Functions 2nd gen for longer processing. This decouples upload from processing.",wrongExplanations:{1:"While Flexible Environment does support longer timeouts, it's more expensive (doesn't scale to zero) and overkill for this use case. Asynchronous processing is a better architectural pattern.",2:"App Engine Standard timeout limits cannot be increased beyond 10 minutes. This is a fundamental constraint of the Standard Environment.",3:"Processing long-running jobs in the request handler creates poor user experience (15-minute wait for upload), ties up instances, and will hit timeout limits. Asynchronous processing is the correct pattern."}},{id:202,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You need to test a new version of your App Engine application with 10% of production traffic before full rollout. How can you achieve this?",options:["Deploy the new version without migrating traffic, then use traffic splitting to allocate 10% to the new version","Deploy to a separate project for testing","App Engine doesn't support partial traffic routing","Use Compute Engine for canary testing instead"],correct:0,explanation:"App Engine traffic splitting allows you to distribute traffic between versions by percentage, IP address, or cookie. Deploying with --no-promote keeps the new version available without receiving traffic, then you can gradually split traffic (10%, 50%, 100%) for canary testing.",wrongExplanations:{1:"Deploying to a separate project doesn't test with real production traffic and requires duplicate infrastructure. Traffic splitting within the same service provides true canary testing with production users.",2:"Compute Engine requires setting up load balancers and traffic management manually. App Engine provides built-in traffic splitting without infrastructure management.",3:"This is false. Traffic splitting is a core App Engine feature specifically designed for gradual rollouts, A/B testing, and canary deployments."}},{id:203,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - App Engine",question:"Your App Engine application needs to access Cloud SQL. Following security best practices, how should you configure authentication?",options:["Store Cloud SQL password in environment variables","Use the App Engine default service account with appropriate Cloud SQL IAM roles","Grant roles/owner to the App Engine service account","Use Cloud SQL proxy with hardcoded credentials"],correct:1,explanation:"App Engine applications automatically use the App Engine default service account for authentication. Grant this service account the Cloud SQL Client role (roles/cloudsql.client) for secure, password-free connection to Cloud SQL instances.",wrongExplanations:{1:"Storing passwords in environment variables exposes them in configuration and logs. IAM-based authentication with service accounts is more secure and doesn't require managing passwords.",2:"Hardcoded credentials in code or configuration are a critical security vulnerability. They can be exposed in version control, logs, or error messages. Always use IAM-based authentication.",3:"roles/owner grants far too many permissions beyond Cloud SQL access. This violates least privilege principle and could allow the application to modify or delete any project resources."}},{id:204,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - App Engine",question:"Your App Engine application experiences high latency during cold starts. Users report slow initial page loads. What can you do to minimize this issue?",options:["Switch to manual scaling","Cold starts are unavoidable in App Engine","Increase the instance class size","Configure minimum instances to keep instances warm and ready to serve requests"],correct:3,explanation:"Minimum instances (min_instances) keeps the specified number of instances running and warm, eliminating cold start latency for those instances. This incurs a cost for idle instances but ensures consistent performance. This is available in App Engine Standard.",wrongExplanations:{1:"Manual scaling keeps instances running but defeats autoscaling benefits. Automatic scaling with minimum instances provides the best of both worlds: warmth and scalability.",2:"Instance class affects performance but doesn't eliminate cold starts. Cold starts are caused by instance initialization, not resource constraints. Minimum instances solve this by keeping instances warm.",3:"While cold starts exist, they can be minimized through minimum instances, optimizing startup code, reducing dependencies, and using warmup requests. Multiple strategies exist to address cold starts."}},{id:205,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You have microservices architecture where different components need independent scaling and deployment. Each service has different resource requirements. How should you structure your App Engine deployment?",options:["Deploy all microservices in a single service with different versions","Use separate GCP projects for each microservice","Deploy each microservice as a separate App Engine service, allowing independent scaling and deployment","App Engine doesn't support microservices"],correct:2,explanation:"App Engine services (formerly called modules) allow you to structure your application as microservices. Each service can have its own scaling configuration, instance class, runtime, and versions. Services are independently deployable and scalable while sharing the same project.",wrongExplanations:{1:"Versions within a service share scaling configuration and are meant for different versions of the same code, not different microservices. This doesn't provide the independent scaling needed for microservices.",2:"Separate projects add complexity in networking, IAM, billing, and service communication. App Engine services provide microservices benefits within a single project.",3:"This is false. App Engine services are specifically designed for microservices architecture, allowing multiple independently deployable and scalable services within one application."}},{id:206,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine Standard application needs to connect to a Cloud Memorystore Redis instance for caching. Redis is only accessible via private IP. What configuration is required?",options:["Switch to App Engine Flexible Environment","App Engine Standard cannot connect to private IPs","Use public IP for Redis instance","Configure Serverless VPC Access connector for your App Engine service to connect to the VPC"],correct:3,explanation:"Serverless VPC Access connector enables App Engine Standard (and other serverless services) to connect to resources in your VPC via private IP addresses, including Cloud Memorystore, Compute Engine VMs, and other internal resources.",wrongExplanations:{1:"This is incorrect. App Engine Standard can connect to private IPs using Serverless VPC Access connectors. This is a supported configuration for accessing VPC resources.",2:"Memorystore doesn't support public IPs for security reasons. It's designed for private VPC access only. Serverless VPC Access connector is the correct solution.",3:"While Flexible Environment has VPC networking capabilities, Standard Environment with Serverless VPC Access connector is more cost-effective (scales to zero) and simpler for applications that otherwise fit Standard's model."}},{id:207,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring and logging - App Engine",question:"Your App Engine application is experiencing performance issues. You need to identify which handler functions are slowest and causing timeouts. What should you use?",options:["Cloud Trace to analyze request latency and identify slow operations in your application","Cloud Logging to read logs manually","Cloud Monitoring for CPU metrics only","App Engine doesn't provide performance profiling"],correct:0,explanation:"Cloud Trace automatically collects latency data from App Engine applications, showing detailed breakdowns of request processing time, including time spent in different functions, API calls, and database queries. This pinpoints performance bottlenecks.",wrongExplanations:{1:"While logs contain some timing information, manually reading logs is inefficient for performance analysis. Cloud Trace provides automated latency analysis and visualization specifically designed for identifying bottlenecks.",2:"CPU metrics show resource utilization but don't identify which specific code paths or operations are slow. Cloud Trace provides request-level detail showing where time is spent.",3:"This is false. App Engine integrates with Cloud Trace and Cloud Profiler for detailed performance analysis, latency tracking, and code-level profiling."}},{id:208,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine application uses environment variables for configuration. You need to update a configuration value without redeploying the application. Is this possible?",options:["Yes, environment variables auto-sync from Secret Manager","Yes, use gcloud app update-env command","No, environment variables are set at deployment time and require redeployment to change","Yes, update them in the Cloud Console"],correct:2,explanation:"App Engine environment variables are part of the version configuration and are baked in at deployment time. To change them, you must redeploy. For runtime-updatable configuration, use Secret Manager, Cloud Storage, or Firestore to store configuration that your app reads at runtime.",wrongExplanations:{1:"This command doesn't exist. App Engine environment variables cannot be updated without redeployment. Consider using external configuration sources for values that change frequently.",2:"Environment variables cannot be updated through the Console without creating a new version. They're part of the immutable version configuration.",3:"Environment variables don't auto-sync with Secret Manager. While Secret Manager is recommended for sensitive configuration, your application code must explicitly fetch secrets at runtime using the API."}},{id:209,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You need to deploy a Node.js application that requires a specific Node.js version not available in the current App Engine Standard runtimes. What should you do?",options:["Deploy to Compute Engine instead","App Engine Standard supports all Node.js versions","Use App Engine Flexible Environment with a custom runtime specified in Dockerfile","Downgrade your application to use an available Node.js version"],correct:2,explanation:"App Engine Flexible Environment allows custom runtimes using Dockerfiles, enabling you to specify any Node.js version. Standard Environment only supports specific runtime versions. Flexible provides flexibility for custom runtime requirements while maintaining App Engine benefits.",wrongExplanations:{1:"Downgrading may not be feasible if your application depends on features or security patches in newer Node.js versions. Using Flexible Environment maintains your version requirements.",2:"This is false. App Engine Standard provides specific, managed runtime versions. While Google regularly updates them, not all versions are available. Flexible Environment supports custom versions.",3:"Compute Engine requires managing VMs, load balancing, autoscaling, and deployments manually. App Engine Flexible provides these features while supporting custom runtimes."}},{id:210,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine application serves static assets (images, CSS, JavaScript). These assets don't change frequently but are requested on every page load. How can you optimize delivery and reduce costs?",options:["Store static files in Cloud Storage and link directly","Use Cloud Functions to serve static files","Configure app.yaml to serve static files directly and use Cloud CDN for caching","Serve all files through application handlers"],correct:2,explanation:"App Engine's app.yaml allows defining static file handlers that serve files directly without invoking application code, reducing latency and costs. Enabling Cloud CDN caches these assets at edge locations globally, further improving performance and reducing origin requests.",wrongExplanations:{1:"Serving static files through application handlers wastes instance resources and increases latency. Static handlers bypass the runtime, serving files directly more efficiently.",2:"While Cloud Storage is good for static files, App Engine's static handlers with CDN provide integrated deployment and caching. For simple static assets bundled with your app, static handlers are simpler.",3:"Cloud Functions for static file serving adds unnecessary complexity and cost. App Engine's static handlers are specifically designed for this use case and are more efficient."}},{id:211,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - App Engine",question:"You need to restrict access to your App Engine application so only users in your organization can access it. What should you configure?",options:["Add firewall rules to block external IPs","Enable Identity-Aware Proxy (IAP) and configure access policies to allow only your organization's users","Configure Cloud Armor security policies","Use VPC Service Controls"],correct:1,explanation:"Identity-Aware Proxy (IAP) provides application-level access control based on user identity and context. You can restrict access to users in your organization using Google Workspace or Cloud Identity, without requiring VPN or complex network configuration.",wrongExplanations:{1:"App Engine Standard doesn't support VPC firewall rules in the traditional sense. It's a serverless platform where network security is managed differently. IAP provides the application-level access control needed.",2:"Cloud Armor protects against DDoS and web attacks but doesn't provide user authentication or organization-based access control. IAP provides identity-based access control.",3:"VPC Service Controls create security perimeters for GCP resources but are designed for preventing data exfiltration, not user authentication. IAP provides the user-level access control needed here."}},{id:212,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - App Engine",question:"Your App Engine application has automatic scaling enabled. You notice instances are being created and destroyed frequently during traffic fluctuations, causing cold starts. How can you optimize this behavior?",options:["Switch to basic scaling instead","Increase max_instances limit","Configure target CPU utilization and max concurrent requests to tune scaling behavior, and set min_idle_instances","Disable autoscaling entirely"],correct:2,explanation:"Automatic scaling in App Engine can be tuned using target_cpu_utilization, target_throughput_utilization, max_concurrent_requests, and min_idle_instances. These settings control when new instances are created and how many idle instances are kept warm, balancing cost and performance.",wrongExplanations:{1:"Basic scaling doesn't provide automatic scaling based on load. It creates instances on-demand and shuts them down after idle time. This doesn't solve the cold start problem for traffic fluctuations.",2:"Disabling autoscaling defeats the purpose of serverless benefits and cost optimization. The goal is to tune autoscaling, not disable it.",3:"Increasing max_instances doesn't prevent frequent creation/destruction of instances based on traffic. It just sets an upper limit. Tuning scaling thresholds and keeping minimum idle instances addresses the cold start issue."}},{id:213,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You need to process streaming data from Pub/Sub, perform windowed aggregations, and write results to BigQuery in real-time. The processing logic requires complex transformations. What should you use?",options:["Cloud Functions triggered by Pub/Sub","BigQuery scheduled queries","Cloud Dataflow with Apache Beam for streaming ETL pipeline","Dataproc with Spark Streaming"],correct:2,explanation:"Cloud Dataflow is Google's fully managed service for stream and batch processing using Apache Beam. It excels at streaming data processing with windowing, complex transformations, and has native Pub/Sub and BigQuery connectors. It autoscales and requires no cluster management.",wrongExplanations:{1:"BigQuery scheduled queries work on data already in BigQuery, not streaming data from Pub/Sub. They can't perform real-time processing of incoming events.",2:"Cloud Functions are designed for simple event processing, not complex streaming analytics with windowing and aggregations. They lack the streaming primitives (windows, triggers, watermarks) needed for sophisticated stream processing.",3:"While Dataproc with Spark Streaming can handle this, it requires managing clusters. Dataflow is serverless, autoscaling, and purpose-built for this use case with better BigQuery integration."}},{id:214,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"Your team has existing Spark and Hadoop jobs that need to run in Google Cloud. The jobs run periodically (hourly) and complete within 20-30 minutes. What is the most cost-effective approach?",options:["Use Compute Engine with manual Hadoop installation","Use Dataproc with ephemeral clusters created per job, then deleted after completion","Rewrite all jobs to use Dataflow","Run a persistent Dataproc cluster 24/7"],correct:1,explanation:"Dataproc supports ephemeral clusters that spin up for a job and delete automatically afterward. For periodic workloads, this is highly cost-effective as you only pay for cluster time during job execution. Dataproc clusters start in 90 seconds, making this pattern practical.",wrongExplanations:{1:"A persistent cluster running 24/7 for jobs that only run hourly for 30 minutes wastes resources. You'd pay for 23.5 hours of idle time per day. Ephemeral clusters eliminate this waste.",2:"Rewriting existing Spark/Hadoop jobs to Dataflow requires significant development effort and may not be necessary. Dataproc provides a lift-and-shift path for existing Spark/Hadoop workloads.",3:"Manual Hadoop installation on Compute Engine requires managing cluster configuration, software updates, and scaling manually. Dataproc provides a fully managed Hadoop/Spark environment."}},{id:215,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your Dataflow pipeline processes batch data from Cloud Storage. You need to minimize costs while accepting longer processing times. What should you configure?",options:["Use Dataflow Shuffle service and enable FlexRS (Flexible Resource Scheduling) for batch jobs","Use the smallest machine type available","Process data manually with Cloud Functions","Reduce the number of workers to 1"],correct:0,explanation:"FlexRS (Flexible Resource Scheduling) provides advanced scheduling for batch jobs at reduced cost (up to 40% savings) by using a mix of normal and preemptible resources with intelligent retries. Dataflow Shuffle service (included with FlexRS) improves performance and reduces costs for large batch jobs.",wrongExplanations:{1:"Using the smallest machine type doesn't optimize costs effectively. It may actually increase costs if jobs run much longer. FlexRS provides better cost optimization through resource scheduling.",2:"Reducing workers to 1 severely limits parallelism, making jobs extremely slow. Dataflow's autoscaling with FlexRS provides better cost/performance balance.",3:"Cloud Functions are unsuitable for large-scale batch data processing. They have memory and execution time limits. Dataflow is purpose-built for large batch processing."}},{id:216,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You're running Apache Spark jobs on Dataproc that process data stored in Cloud Storage. The jobs frequently fail due to worker node failures. How can you improve reliability?",options:["Enable Enhanced Flexibility Mode (EFM) which uses preemptible VMs with automatic recovery","Switch to Dataflow instead","Increase the number of worker nodes","Disable preemptible workers entirely"],correct:0,explanation:"Enhanced Flexibility Mode (EFM) in Dataproc allows using a mix of preemptible and regular VMs with intelligent workload placement and automatic recovery. It provides cost savings from preemptible VMs while maintaining reliability through smart failure handling.",wrongExplanations:{1:"Disabling preemptible workers increases costs significantly. EFM allows safe use of preemptible workers with reliability through automatic recovery mechanisms.",2:"Simply increasing nodes doesn't address the root cause of node failures. EFM provides failure recovery mechanisms that make the cluster more resilient.",3:"While Dataflow is more resilient for some workloads, if you have existing Spark jobs, EFM provides a cost-effective way to improve Dataproc reliability without rewriting code."}},{id:217,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataprep",question:"Your data analysts need to clean and prepare messy CSV files for analysis in BigQuery, but they don't have programming skills. They need a visual interface to explore data and create transformation recipes. What should you use?",options:["Use Cloud Dataflow with Apache Beam","Use BigQuery SQL for data cleaning","Write Python scripts in Cloud Functions","Cloud Dataprep by Trifacta for visual data preparation and cleaning"],correct:3,explanation:"Cloud Dataprep provides a visual, no-code interface for data cleaning and preparation. Analysts can interactively explore data, detect anomalies, and build transformation recipes that are executed as Dataflow jobs. It's designed specifically for non-technical users.",wrongExplanations:{1:"Python scripts require programming skills, which the analysts lack. Dataprep provides a visual interface specifically designed for non-technical users.",2:"While BigQuery SQL can clean data, it's less interactive and doesn't provide the data profiling, anomaly detection, and recipe-building features that Dataprep offers for exploratory data cleaning.",3:"Cloud Dataflow requires programming with Apache Beam (Python or Java). It's powerful but not suitable for analysts without programming skills. Dataprep generates Dataflow jobs automatically."}},{id:218,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataflow",question:"Your Dataflow streaming pipeline is experiencing high latency. The pipeline reads from Pub/Sub, processes data, and writes to BigQuery. How can you identify the bottleneck?",options:["Restart the pipeline","Increase worker count immediately","Check the Dataflow monitoring UI for system lag, data freshness metrics, and per-step processing times","Dataflow doesn't provide performance metrics"],correct:2,explanation:"Dataflow's monitoring UI provides detailed metrics including system lag (backlog of unprocessed data), data freshness (age of oldest unprocessed data), and per-step execution time. These metrics pinpoint whether bottlenecks are in reading, processing, or writing stages.",wrongExplanations:{1:"Blindly increasing workers without identifying the bottleneck wastes resources. The bottleneck might be in a non-parallelizable step or external dependency, where more workers won't help.",2:"Restarting doesn't fix performance issues. It might even make things worse if there's backlog, as the pipeline needs to reprocess buffered data.",3:"This is false. Dataflow provides comprehensive monitoring including system lag, data freshness, throughput, worker utilization, and per-step metrics."}},{id:219,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You need to run a Dataproc cluster with HDFS for temporary storage during Spark jobs, but want to minimize costs. What storage strategy should you use?",options:["Use Cloud Filestore for HDFS replacement","Use large persistent disks for HDFS storage","Use only in-memory storage without any disk","Use Cloud Storage as primary storage and local HDFS only for shuffle/temp data with smaller persistent disk"],correct:3,explanation:"Cloud Storage provides durable, cost-effective storage for data with excellent integration with Dataproc (HDFS-compatible connector). Use HDFS on smaller local disks only for temporary shuffle data during jobs. This approach minimizes storage costs while maintaining performance.",wrongExplanations:{1:"Large persistent disks for HDFS are expensive and unnecessary when Cloud Storage provides durable, cheaper storage. Persistent disks should be sized for temp/shuffle data only.",2:"In-memory storage is limited by VM memory and ephemeral. Spark jobs need persistent storage for data and temp files. This approach won't work for most real workloads.",3:"Cloud Filestore is expensive network-attached storage meant for applications needing shared NFS. Cloud Storage with HDFS connector is more cost-effective for Hadoop/Spark workloads."}},{id:220,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your Dataflow pipeline needs to join streaming data from Pub/Sub with reference data in BigQuery. The reference data changes infrequently (once daily). What pattern should you use?",options:["Store reference data in Dataflow worker memory statically","Query BigQuery for each streaming record","Use Dataflow side input to periodically reload reference data from BigQuery and join in memory","Duplicate reference data in Pub/Sub messages"],correct:2,explanation:"Dataflow side inputs allow loading reference data periodically (e.g., hourly or daily) and making it available to all workers for in-memory joins. This is efficient for slowly changing reference data, avoiding per-record BigQuery queries while keeping data reasonably fresh.",wrongExplanations:{1:"Querying BigQuery for each streaming record creates massive overhead, latency, and BigQuery costs. For slowly changing reference data, periodic bulk loads with side inputs are far more efficient.",2:"Duplicating reference data in each Pub/Sub message wastes bandwidth and increases message size. It also creates consistency problems when reference data changes.",3:"Static worker memory doesn't update when reference data changes. Side inputs support periodic refreshes, keeping reference data current without pipeline restarts."}},{id:221,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You need to run both Spark and Presto workloads on the same dataset. Managing two separate clusters is operationally expensive. What Dataproc feature should you use?",options:["Use GKE with custom containers instead","Dataproc only supports Spark","Use Dataproc Component Gateway to install multiple big data components (Spark, Presto, Hive) on one cluster","Run separate Dataproc clusters for each component"],correct:2,explanation:"Dataproc supports optional components including Presto, Hive, HBase, Jupyter, and more through initialization actions and Component Gateway. You can run multiple components on a single cluster, reducing operational overhead and costs while providing web UIs for each component.",wrongExplanations:{1:"Running separate clusters increases costs and operational complexity. Dataproc's component support allows consolidation when workloads share data and resource requirements.",2:"This is false. Dataproc is based on Apache Hadoop and supports a full ecosystem including Spark, Hadoop MapReduce, Pig, Hive, Presto, Flink, and more through optional components.",3:"While GKE provides flexibility, managing big data components in Kubernetes adds significant complexity. Dataproc provides a fully managed Hadoop ecosystem with integrated component support."}},{id:222,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataproc",question:"Your Dataproc cluster job failed, but the cluster was configured to delete after job completion. You need to troubleshoot the failure. How can you access logs?",options:["Logs are permanently lost when cluster is deleted","SSH into the cluster master node","Check Cloud Logging which retains Dataproc logs even after cluster deletion","Logs are only available in HDFS on the cluster"],correct:2,explanation:"Dataproc automatically exports logs to Cloud Logging (formerly Stackdriver Logging). These logs persist after cluster deletion, allowing post-mortem troubleshooting of failed jobs on ephemeral clusters. Logs include job output, YARN logs, and system logs.",wrongExplanations:{1:"This is false. Dataproc integrates with Cloud Logging to preserve logs even after clusters are deleted. This is essential for ephemeral cluster troubleshooting.",2:"You can't SSH into a deleted cluster. Cloud Logging provides access to logs after cluster deletion without requiring cluster access.",3:"HDFS is local to the cluster and lost when cluster is deleted. Cloud Logging provides durable log storage independent of cluster lifecycle."}},{id:223,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You're building a Dataflow pipeline that processes financial transactions. You need to ensure exactly-once processing semantics when writing to BigQuery. What should you do?",options:["Use Dataflow's BigQueryIO with built-in exactly-once semantics and idempotent writes","Dataflow cannot guarantee exactly-once semantics","Implement deduplication logic in your pipeline code","Use Pub/Sub exactly-once delivery"],correct:0,explanation:"Dataflow's BigQueryIO connector provides exactly-once write semantics to BigQuery using streaming inserts with deduplication or using BigQuery Storage Write API. This is built into the connector and handles retries and failures transparently, ensuring financial transactions aren't duplicated.",wrongExplanations:{1:"While you can implement deduplication logic, BigQueryIO already provides this capability. Using the built-in feature is simpler, more reliable, and follows best practices.",2:"Pub/Sub provides at-least-once delivery, not exactly-once. Even with exactly-once delivery (where available), you still need idempotent sinks. BigQueryIO provides this for BigQuery writes.",3:"This is false. Dataflow with appropriate sinks (like BigQueryIO) provides exactly-once processing semantics, which is critical for financial and other sensitive data processing."}},{id:224,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You need to process data using Spark ML library for machine learning training. The training jobs require GPUs. Can you use Dataproc for this, and if so, how?",options:["No, Dataproc doesn't support GPUs; use Vertex AI instead","Yes, configure Dataproc clusters with GPU-enabled machine types and appropriate GPU drivers","No, Spark doesn't support GPU acceleration","Yes, but only on the master node"],correct:1,explanation:"Dataproc supports GPU-enabled machine types for worker nodes. You can configure clusters with NVIDIA GPUs and drivers for GPU-accelerated Spark workloads, including Spark ML training. This is useful for distributed GPU computing with existing Spark code.",wrongExplanations:{1:"While Vertex AI is excellent for managed ML training, Dataproc does support GPUs for users who want to run Spark-based ML workflows with GPU acceleration. Both services have valid use cases.",2:"GPUs can be attached to worker nodes where processing happens, not just the master node. Worker nodes need GPUs for distributed computation.",3:"This is false. Spark 3.x includes GPU support through Rapids accelerator. Dataproc can run GPU-accelerated Spark workloads for ETL and ML."}},{id:225,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your streaming Dataflow pipeline needs to handle late-arriving data (events that arrive hours after their event time). What Dataflow feature addresses this?",options:["Configure windowing with allowed lateness and triggers to handle late data appropriately","Late data cannot be handled in streaming pipelines","Increase the pipeline parallelism","Use batch processing instead"],correct:0,explanation:"Dataflow provides sophisticated late data handling through watermarks, allowed lateness, and triggers. You can specify how long to wait for late data, configure triggers to emit multiple results per window, and handle late data gracefully without losing events.",wrongExplanations:{1:"This is false. Handling late data is a core capability of Apache Beam and Dataflow. Watermarks and allowed lateness are designed specifically for this scenario.",2:"Parallelism affects throughput, not late data handling. Late data is handled through windowing and watermark configuration, independent of parallelism.",3:"Batch processing doesn't eliminate late data issues; it just changes the problem. Streaming with proper windowing and late data handling is often more appropriate for real-time use cases."}},{id:226,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataflow",question:"Your Dataflow pipeline autoscaling is not responding to load increases. Workers remain at minimum despite growing backlog. What might be the issue?",options:["Increase the worker machine type","Dataflow doesn't support autoscaling","The pipeline has a non-parallelizable bottleneck or maxNumWorkers is set too low","Autoscaling is disabled by default"],correct:2,explanation:"Dataflow autoscales based on backlog and work distribution. If a pipeline has operations that aren't parallelizable (e.g., global aggregations, single-threaded operations), autoscaling can't help. Also, check maxNumWorkers setting which caps scaling. Identify bottleneck steps in monitoring UI.",wrongExplanations:{1:"Autoscaling is enabled by default in Dataflow. If it's not working, there's typically a configuration issue or a non-parallelizable bottleneck, not that it's disabled.",2:"This is false. Autoscaling is a core Dataflow feature. Dataflow automatically scales workers based on backlog and resource utilization.",3:"Machine type affects per-worker capacity but doesn't address autoscaling issues. If workers aren't scaling out, adding more powerful machines doesn't solve the parallelization problem."}},{id:227,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You're migrating on-premises Hadoop workloads to Google Cloud. The workflows use custom Python scripts that interact with HDFS commands. What is the easiest migration path?",options:["Use Dataproc with Cloud Storage HDFS connector; most HDFS commands work with gs:// paths transparently","Use Filestore to replicate HDFS exactly","Rewrite all scripts to use Cloud Storage API directly","Keep HDFS on Dataproc persistent disks"],correct:0,explanation:"Dataproc includes the Cloud Storage connector that implements the HDFS interface for gs:// paths. Many HDFS commands and Hadoop filesystem API calls work with Cloud Storage transparently, enabling lift-and-shift migration with minimal code changes.",wrongExplanations:{1:"Rewriting scripts requires significant development effort. The Cloud Storage HDFS connector allows most scripts to work with minimal changes by supporting HDFS APIs.",2:"Filestore is expensive network storage meant for NFS use cases. It's not a suitable HDFS replacement. Cloud Storage with HDFS connector provides better cost and performance.",3:"Using HDFS on persistent disks increases costs and loses Cloud Storage benefits (durability, no storage management). Cloud Storage with HDFS connector is recommended for most workloads."}},{id:228,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You need to update a production Dataflow streaming pipeline with new transformation logic without losing in-flight data or creating downtime. What should you do?",options:["Run both pipelines in parallel, then switch","Use Dataflow's update feature to perform an in-place update of the running pipeline","Stop the current pipeline, then start a new one","Streaming pipelines cannot be updated"],correct:1,explanation:"Dataflow supports updating running streaming pipelines in-place using the --update flag. This preserves pipeline state, in-flight data, and subscription positions without downtime. Dataflow determines which transforms changed and migrates state appropriately.",wrongExplanations:{1:"Stopping and restarting loses in-flight data and resets subscription positions. You'd need to reprocess data or accept data loss. Dataflow updates provide a better solution.",2:"Running parallel pipelines wastes resources and creates complexity in managing two pipelines and avoiding duplicate processing. In-place updates are cleaner.",3:"This is false. Dataflow specifically supports updating streaming pipelines in-place, which is a major operational advantage for production systems."}},{id:229,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Dataproc",question:"Your Dataproc cluster needs to read data from Cloud Storage and write results to BigQuery. Following least privilege, what roles should you grant the cluster's service account?",options:["roles/storage.objectViewer for source bucket and roles/bigquery.dataEditor for destination dataset","No roles; Dataproc has automatic access","roles/editor at the project level","roles/owner for full access"],correct:0,explanation:"Dataproc clusters use a service account for accessing GCP resources. Grant only the minimum required permissions: objectViewer (or objectUser) for reading Cloud Storage, and dataEditor for writing to BigQuery. This follows the principle of least privilege.",wrongExplanations:{1:"roles/editor grants broad project-level permissions far beyond what's needed for reading storage and writing to BigQuery. This violates least privilege and creates security risks.",2:"roles/owner grants full control over the project, including IAM modifications and resource deletion. This is extremely excessive for a data processing cluster.",3:"Dataproc clusters use a service account that requires appropriate IAM roles. Without proper roles, the cluster cannot access Cloud Storage or BigQuery."}},{id:230,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your organization wants to use Dataflow but developers are more familiar with Python than Java. Can you use Dataflow with Python, and are there any limitations?",options:["Yes, Apache Beam Python SDK is fully supported in Dataflow with feature parity for most use cases","Yes, but Python pipelines can't use streaming","No, Dataflow only supports Java","Yes, but Python has no access to GCP connectors"],correct:0,explanation:"Dataflow fully supports Apache Beam Python SDK with connectors for Pub/Sub, BigQuery, Cloud Storage, and more. Both streaming and batch processing are supported. Python and Java SDKs have near feature parity for most use cases.",wrongExplanations:{1:"This is false. Dataflow supports Apache Beam Java, Python, and Go SDKs. Python is a first-class citizen with full support.",2:"This is false. Python SDK fully supports streaming pipelines. Many production streaming pipelines use Python with Dataflow.",3:"This is false. Python SDK includes I/O connectors for all major GCP services including BigQuery, Pub/Sub, Cloud Storage, Bigtable, and more."}},{id:231,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataproc",question:"You need to apply custom initialization scripts to install Python libraries on all Dataproc cluster nodes before jobs run. What should you use?",options:["Include installation commands in each job submission","Initialization scripts are not supported","SSH into each node manually after cluster creation","Use Dataproc initialization actions to run scripts during cluster creation"],correct:3,explanation:"Dataproc initialization actions are scripts that run on all nodes during cluster creation. They're used to install custom software, configure settings, or download data. Scripts can be stored in Cloud Storage and specified at cluster creation time.",wrongExplanations:{1:"Manual SSH doesn't scale and must be repeated for each cluster, especially problematic with autoscaling or ephemeral clusters. Initialization actions automate this.",2:"Including installation in each job wastes time and resources. Libraries should be installed once during cluster initialization, not repeatedly for each job.",3:"This is false. Initialization actions are a core Dataproc feature for customizing cluster configuration during creation."}},{id:232,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You need to migrate from batch to streaming processing for near real-time analytics. Your existing batch code is written in Apache Beam. How much code rewrite is required?",options:["Complete rewrite: batch and streaming are completely different programming models","Partial rewrite of 50% of the code","Minimal changes: update the pipeline to read from Pub/Sub instead of batch source and specify windowing strategy","Impossible: must use different technology for streaming"],correct:2,explanation:"Apache Beam's unified batch/streaming model allows most batch pipelines to convert to streaming with minimal changes: swap the source (e.g., Cloud Storage  Pub/Sub) and add windowing/triggering configuration. The core transformation logic often remains unchanged.",wrongExplanations:{1:"This is false. Apache Beam's key design principle is unified batch and streaming. The same transforms work for both, making batch-to-streaming migration much easier than with other frameworks.",2:"While some changes are needed (particularly around windowing), 50% is an overestimate. Most transformation logic remains the same; main changes are source, windowing, and possibly triggering.",3:"This is false. Apache Beam on Dataflow specifically enables using the same code for batch and streaming with configuration changes, not technology replacement."}},{id:233,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"Your application needs a low-latency caching layer for frequently accessed database queries. Cache hit rates are critical for performance. Which service should you use?",options:["BigQuery materialized views","Cloud Storage with lifecycle management","Cloud Memorystore for Redis as a managed in-memory cache","Persistent Disk attached to Compute Engine"],correct:2,explanation:"Cloud Memorystore for Redis provides fully managed, highly available Redis instances with sub-millisecond latency. It's specifically designed for caching use cases, session storage, and real-time analytics requiring fast in-memory data access.",wrongExplanations:{1:"Cloud Storage is object storage with milliseconds to seconds latency, not suitable for sub-millisecond caching requirements. It's designed for file storage, not in-memory caching.",2:"BigQuery materialized views cache query results but are designed for analytical queries in BigQuery, not application-level caching with sub-millisecond latency requirements.",3:"Persistent Disk is block storage for VMs with millisecond latency. For caching, in-memory systems like Memorystore provide much faster access (microseconds vs milliseconds)."}},{id:234,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"You need to deploy Memorystore for Redis with high availability across zones. What tier and configuration should you use?",options:["Basic Tier with multiple instances","Standard Tier with manual backups only","Standard Tier with automatic failover to a replica in a different zone","Basic Tier which provides built-in HA"],correct:2,explanation:"Memorystore Standard Tier provides a Redis instance with a replica in a different zone for high availability. Automatic failover ensures minimal downtime if the primary instance fails. Basic Tier doesn't provide replication or automatic failover.",wrongExplanations:{1:"Basic Tier is a single instance without replication or automatic failover. It's suitable for development/testing but not for production HA requirements.",2:"Standard Tier with only manual backups doesn't provide high availability during failures. HA requires automatic failover to a replica, not just backups.",3:"Running multiple Basic Tier instances doesn't provide automatic failover or data replication. Standard Tier with built-in replication is the correct solution for HA."}},{id:235,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Memorystore",question:"Your Memorystore Redis instance is approaching maximum memory capacity. What are your options to prevent out-of-memory errors?",options:["Add more instances and manually shard data","Delete the instance and create a larger one","Scale up the instance to a larger memory size or configure eviction policies for automatic key removal","Memorystore automatically scales memory"],correct:2,explanation:"Memorystore supports vertical scaling (increasing memory size) and configurable eviction policies (LRU, LFU, etc.) to automatically remove less-used keys when memory is full. Standard Tier supports scaling with minimal downtime.",wrongExplanations:{1:"Memorystore does not automatically scale memory. You must manually scale up the instance or configure eviction policies to manage memory usage.",2:"While you can create multiple instances, Memorystore doesn't provide automatic sharding. Manual sharding adds complexity. Scaling up or using eviction policies is simpler for most use cases.",3:"Deleting and recreating causes downtime and data loss. Standard Tier supports scaling up with minimal disruption. This should be the last resort, not the first option."}},{id:236,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"Your application running on GKE needs to connect to Memorystore Redis. Both are in the same project and region. What networking configuration is required?",options:["Use Memorystore's public IP endpoint","Configure Cloud VPN between GKE and Memorystore","Ensure GKE and Memorystore are in the same VPC; Memorystore is accessible via private IP","Deploy a proxy service in front of Memorystore"],correct:2,explanation:"Memorystore instances are created in a VPC with private IP addresses. GKE clusters in the same VPC can directly connect to Memorystore using the private IP. No additional networking configuration is needed.",wrongExplanations:{1:"Cloud VPN is for connecting different networks or on-premises to GCP. Within the same VPC, resources communicate directly via private IPs without VPN.",2:"Memorystore doesn't provide public IP endpoints for security reasons. It's only accessible via private IP within the VPC.",3:"A proxy is unnecessary. GKE pods can directly connect to Memorystore via private IP when in the same VPC. Adding a proxy increases complexity and latency."}},{id:237,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"You need to choose between Memorystore for Redis and Memorystore for Memcached. Your application requires data persistence and complex data structures (lists, sets). Which should you choose?",options:["Either one; they have identical features","Memorystore for Memcached, which is faster for all use cases","Memorystore for Redis, which supports persistence and rich data structures","Memorystore for Memcached with custom persistence logic"],correct:2,explanation:"Redis supports data persistence (RDB snapshots, AOF logs) and rich data structures (strings, lists, sets, sorted sets, hashes). Memcached is a pure in-memory cache with only key-value strings and no persistence. For complex data structures and persistence, Redis is the correct choice.",wrongExplanations:{1:"Memcached can be faster for simple key-value operations but doesn't support complex data structures or persistence. The question requires both features, making Redis the only viable option.",2:"This is false. Redis and Memcached have different capabilities. Redis supports complex data structures, persistence, pub/sub, and transactions. Memcached is simpler with only key-value caching.",3:"Memcached doesn't support persistence at all. Adding custom persistence logic defeats the purpose of using a managed caching service. Redis provides built-in persistence."}},{id:238,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your application running on multiple Compute Engine instances needs shared file storage with NFS protocol support. What should you use?",options:["Cloud Storage FUSE mount","Local SSD on each instance","Cloud Filestore for managed NFS file storage accessible by multiple instances","Persistent Disk attached to one instance and shared via custom NFS server"],correct:2,explanation:"Cloud Filestore provides fully managed NFS file shares that multiple Compute Engine instances (or GKE pods) can mount concurrently. It's designed for shared file storage use cases requiring NFS protocol.",wrongExplanations:{1:"While you can create an NFS server on Compute Engine, this requires managing the server, handling availability, and capacity planning. Filestore provides a fully managed solution.",2:"Cloud Storage FUSE provides object storage access via filesystem interface but has different semantics than NFS (eventual consistency, no file locking). For true NFS compatibility, Filestore is better.",3:"Local SSDs are attached to individual instances and can't be shared between instances. Each instance would have separate storage, not shared storage."}},{id:239,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your application requires high-throughput file storage for video rendering workloads. Which Filestore tier should you choose?",options:["Basic HDD tier for maximum capacity","Basic SSD tier is always sufficient","Filestore doesn't support high throughput workloads","High Scale or Enterprise tier for high throughput and IOPS requirements"],correct:3,explanation:"Filestore High Scale and Enterprise tiers provide high throughput (up to 1200+ MB/s) and IOPS for demanding workloads like video rendering, HPC, and analytics. Basic tiers have lower performance limits suitable for general file sharing.",wrongExplanations:{1:"Basic HDD tier provides maximum capacity but low throughput (100 MB/s). For high-throughput video rendering, this would be a bottleneck.",2:"Basic SSD tier (up to 480 MB/s) may be sufficient for some workloads but video rendering often requires higher throughput. High Scale/Enterprise tiers provide better performance.",3:"This is false. Filestore High Scale and Enterprise tiers are specifically designed for high-throughput workloads and can deliver over 1200 MB/s."}},{id:240,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Filestore",question:"Your Filestore instance is running out of capacity. How can you increase storage without downtime?",options:["Delete and recreate the instance with larger capacity","Create a new larger instance and manually copy data","Filestore capacity cannot be changed after creation","Scale up the instance capacity through the console or gcloud command; Filestore supports online resizing"],correct:3,explanation:"Filestore supports online capacity increases without downtime. You can scale up (but not down) the instance size through the console or gcloud commands. The resize happens while the instance remains available to clients.",wrongExplanations:{1:"Creating a new instance and copying data causes disruption to applications and requires coordinating the migration. Online resizing is simpler and maintains availability.",2:"This is false. Filestore supports scaling up capacity online without downtime. This is a key operational feature for production environments.",3:"Deleting and recreating causes downtime and data loss if not backed up. Online scaling is available and maintains continuous availability."}},{id:241,domain:"Configuring access and security",subdomain:"4.1 Managing access - Filestore",question:"You need to control which Compute Engine instances can mount your Filestore instance. What security mechanism should you configure?",options:["Configure Cloud Armor policies","Filestore is automatically secure; no configuration needed","Use IAM roles to control mount access","Configure VPC firewall rules and Filestore's IP-based access control"],correct:3,explanation:"Filestore uses IP-based access control. You can restrict which IP addresses/ranges can mount the NFS share. Combined with VPC firewall rules, this controls access at the network level. IAM controls who can manage the Filestore instance, not who can mount it.",wrongExplanations:{1:"IAM roles control who can create, delete, or modify Filestore instances in GCP, but don't control NFS mount access. NFS uses IP-based access control, not IAM authentication.",2:"Filestore requires explicit access control configuration. By default, any instance in the VPC can potentially access it if firewall rules allow. IP-based restrictions add security.",3:"Cloud Armor is for protecting load-balanced HTTP(S) services from DDoS and web attacks. It doesn't apply to NFS file shares."}},{id:242,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your GKE application needs persistent shared storage for multiple pods to read and write simultaneously. What combination should you use?",options:["Cloud Storage bucket with gcsfuse","Local storage in each pod","Cloud Filestore with ReadWriteMany (RWX) PersistentVolumeClaim in GKE","Persistent Disk with ReadWriteOnce (RWO)"],correct:2,explanation:"Filestore with ReadWriteMany (RWX) PersistentVolumeClaim allows multiple pods to mount the same volume simultaneously with read and write access. This is ideal for shared file storage in Kubernetes. Persistent Disk only supports RWO (one pod at a time).",wrongExplanations:{1:"Persistent Disk with ReadWriteOnce can only be mounted by one pod at a time. For simultaneous access by multiple pods, ReadWriteMany with Filestore is required.",2:"While gcsfuse can work, it provides object storage semantics, not true filesystem semantics. File locking and consistency guarantees differ from NFS. Filestore provides true shared filesystem.",3:"Local storage in each pod creates separate storage per pod, not shared storage. Changes in one pod aren't visible to others."}},{id:243,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Build",question:"You need to automatically build and deploy your application to GKE whenever code is pushed to your GitHub repository. What should you configure?",options:["Set up Cloud Build triggers connected to GitHub repository with build steps to build container and deploy to GKE","Cloud Build doesn't integrate with GitHub","Manually run gcloud commands after each code push","Use Jenkins on Compute Engine"],correct:0,explanation:"Cloud Build integrates with GitHub (and other repositories) through triggers. When code is pushed, triggers automatically start builds defined in cloudbuild.yaml. Build steps can build containers, push to Artifact Registry, and deploy to GKE - creating a complete CI/CD pipeline.",wrongExplanations:{1:"Manual commands don't provide automation, are error-prone, and don't scale. Cloud Build triggers provide fully automated CI/CD.",2:"While Jenkins can work, it requires managing infrastructure (VMs), updates, and plugins. Cloud Build is serverless and fully managed with native GCP integration.",3:"This is false. Cloud Build has first-class integrations with GitHub, GitLab, Bitbucket, and Cloud Source Repositories through build triggers."}},{id:244,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Build",question:"Your Cloud Build pipeline needs to access secrets (API keys, passwords) during builds. What is the recommended approach?",options:["Store secrets in Secret Manager and grant Cloud Build service account access to retrieve them during builds","Use Cloud KMS to encrypt secrets in source code","Store secrets in environment variables in the Cloud Build configuration","Hardcode secrets in cloudbuild.yaml"],correct:0,explanation:"Secret Manager is designed for storing sensitive data like API keys and passwords. Cloud Build can retrieve secrets from Secret Manager during builds using the secretEnv field or gcloud commands. This keeps secrets out of source code and build configurations.",wrongExplanations:{1:"Hardcoding secrets in configuration files is a critical security violation. These files are often committed to source control, exposing secrets to anyone with repository access.",2:"While you can use environment variables, storing secret values directly in Cloud Build configuration exposes them. Secret Manager integration allows referencing secrets without exposing values.",3:"Encrypting secrets in source code still requires managing decryption keys and exposes encrypted values in version control. Secret Manager provides better security and secret lifecycle management."}},{id:245,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing CI/CD - Cloud Build",question:"Your Cloud Build pipeline is failing with permission errors when trying to deploy to GKE. What is the likely cause?",options:["The Cloud Build service account lacks required roles (Kubernetes Engine Developer or Container Developer)","GKE is in a different project","The build configuration is invalid","Cloud Build cannot deploy to GKE"],correct:0,explanation:"Cloud Build uses a service account to interact with GCP services. To deploy to GKE, this service account needs appropriate roles like roles/container.developer or roles/container.clusterAdmin. Without these roles, deployment commands will fail with permission errors.",wrongExplanations:{1:"This is false. Cloud Build is commonly used for GKE deployments. It can run kubectl commands, deploy manifests, and manage Kubernetes resources.",2:"Cloud Build can deploy to GKE in different projects if the service account has cross-project permissions. The issue is likely missing roles, not cross-project deployment.",3:"Invalid build configuration would cause different errors (syntax errors, missing steps). Permission errors specifically indicate IAM/role issues."}},{id:246,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Artifact Registry",question:"You need to store Docker container images and Maven packages for your organization. What service should you use?",options:["Artifact Registry which supports multiple artifact formats (Docker, Maven, npm, Python, etc.)","Separate services for each artifact type","Cloud Storage for storing artifacts","Container Registry for all artifact types"],correct:0,explanation:"Artifact Registry is Google's recommended solution for storing and managing artifacts. It supports Docker containers, Maven, npm, Python, apt, and other formats in a single service. It's the successor to Container Registry with additional features.",wrongExplanations:{1:"Container Registry only supports Docker images. For Maven packages and other formats, you'd need different solutions. Artifact Registry consolidates all artifact types.",2:"While Cloud Storage can store files, it doesn't provide artifact-specific features like vulnerability scanning, versioning, access controls, or integration with build tools that Artifact Registry provides.",3:"Using separate services increases management complexity. Artifact Registry provides a unified solution for all artifact types with consistent IAM, scanning, and policies."}},{id:247,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Artifact Registry",question:"You need to allow Cloud Build to push images to Artifact Registry and GKE to pull images from it. What roles should you configure?",options:["Grant both service accounts roles/owner","Grant Cloud Build service account roles/artifactregistry.writer and GKE service account roles/artifactregistry.reader","Use public repository without authentication","Artifact Registry doesn't use IAM"],correct:1,explanation:"Artifact Registry uses IAM for access control. Cloud Build needs writer role to push artifacts, while GKE needs reader role to pull images. This follows least privilege by granting only necessary permissions.",wrongExplanations:{1:"roles/owner grants full project control, far exceeding what's needed for pushing/pulling artifacts. This violates least privilege and creates security risks.",2:"Public repositories remove access control, allowing anyone on the internet to pull images. This exposes your artifacts and creates security vulnerabilities.",3:"This is false. Artifact Registry uses IAM for fine-grained access control. You can control who can read, write, or manage artifacts using IAM roles."}},{id:248,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Deploy",question:"You need to deploy applications to multiple GKE environments (dev, staging, prod) with progressive rollouts and automated promotion. What GCP service simplifies this?",options:["Manually deploy to each environment using kubectl","Cloud Build alone handles multi-environment deployments","Use separate Cloud Build triggers for each environment","Cloud Deploy for managed continuous delivery with deployment pipelines and progressive delivery"],correct:3,explanation:"Cloud Deploy provides managed continuous delivery with support for deployment pipelines (dev  staging  prod), progressive delivery strategies (canary, blue-green), automated rollbacks, and approval workflows. It simplifies multi-environment deployments.",wrongExplanations:{1:"Cloud Build handles CI (building and testing) but doesn't provide CD pipeline features like progressive rollouts, deployment strategies, or approval workflows. Cloud Deploy adds these CD capabilities.",2:"Manual deployment doesn't scale, lacks automation, and is error-prone. It doesn't provide progressive delivery, rollback capabilities, or audit trails.",3:"While multiple triggers can deploy to different environments, this doesn't provide pipeline orchestration, approval workflows, or progressive delivery strategies that Cloud Deploy offers."}},{id:249,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing CI/CD - Cloud Build",question:"Your Cloud Build builds are taking too long. You notice builds are downloading dependencies from the internet on every run. How can you improve build times?",options:["Run builds in parallel","Increase build machine size","Build times cannot be optimized","Use Cloud Build's built-in caching or explicitly cache dependencies in Cloud Storage between builds"],correct:3,explanation:"Cloud Build supports caching dependencies between builds. You can use Kaniko cache for Docker layers or explicitly save/restore dependencies to Cloud Storage. This avoids re-downloading unchanged dependencies, significantly improving build times.",wrongExplanations:{1:"Larger machines provide more resources but don't avoid downloading dependencies. Caching eliminates redundant downloads, providing better speedup for dependency-heavy builds.",2:"Parallel builds help with concurrent builds but don't speed up individual builds that are slow due to dependency downloads. Caching addresses the root cause.",3:"This is false. Build times can be significantly optimized through caching, dependency optimization, multi-stage builds, and build configuration tuning."}},{id:250,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Build",question:"You need to build Docker images in Cloud Build without requiring Docker daemon. What builder should you use?",options:["Standard Docker builder is the only option","Use docker-in-docker","Kaniko or Cloud Native Buildpacks which build images without Docker daemon","Cloud Build doesn't support building Docker images"],correct:2,explanation:"Kaniko and Cloud Native Buildpacks (pack) build container images without requiring Docker daemon, making them ideal for Cloud Build's containerized build environment. They're more secure and efficient than docker-in-docker approaches.",wrongExplanations:{1:"Cloud Build supports multiple Docker builders. While the docker builder works, Kaniko and Buildpacks are recommended for better security and caching in Cloud Build's environment.",2:"This is false. Cloud Build is commonly used for building Docker images and provides multiple builder options including docker, Kaniko, and Buildpacks.",3:"Docker-in-docker runs Docker daemon inside a container, which has security implications and complexity. Kaniko and Buildpacks are purpose-built for daemonless image building."}},{id:251,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Migrate for Compute Engine",question:"You need to migrate on-premises VMware VMs to Google Cloud with minimal downtime. What service should you use?",options:["Rebuild VMs from scratch in GCP","Use gsutil to copy VM files","Migrate for Compute Engine (formerly Velostrata) for live migration with minimal downtime","Manually export VMs and import to GCP"],correct:2,explanation:"Migrate for Compute Engine enables live migration of VMs from on-premises, AWS, or Azure to Compute Engine with minimal downtime. It uses streaming technology to run workloads in GCP while data migrates in the background.",wrongExplanations:{1:"Manual export/import requires downtime during export, transfer, and import. It's time-consuming and causes significant service disruption compared to live migration.",2:"Rebuilding VMs requires reconfiguring applications, installing software, and migrating data manually. This takes weeks/months and is error-prone. Migration tools automate this process.",3:"gsutil is for transferring files to Cloud Storage, not for migrating VMs. VM migration requires tools that handle disk images, configurations, and minimize downtime."}},{id:252,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Database Migration Service",question:"You need to migrate your PostgreSQL database from on-premises to Cloud SQL with continuous replication and minimal downtime. What should you use?",options:["Database Migration Service for continuous replication and minimal downtime migration","Manually set up replication","pg_dump and restore to Cloud SQL","Export to CSV and import"],correct:0,explanation:"Database Migration Service provides serverless, easy-to-use database migration with continuous replication. It supports PostgreSQL and MySQL to Cloud SQL with minimal downtime by keeping databases in sync until cutover.",wrongExplanations:{1:"pg_dump requires downtime during export and restore. For large databases, this can be hours or days. Database Migration Service provides continuous replication for minimal downtime.",2:"Manual replication setup is complex, requires expertise in PostgreSQL replication, and is error-prone. Database Migration Service automates and simplifies the process.",3:"CSV export/import causes significant downtime, doesn't preserve database schema fully, and is inefficient for large databases. Database Migration Service handles schema and data migration properly."}},{id:253,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Transfer Service",question:"You need to transfer 100TB of data from AWS S3 to Cloud Storage. What is the most efficient approach?",options:["Manually copy files one by one","Use gsutil rsync","Download data locally and upload to Cloud Storage","Use Storage Transfer Service for automated, managed transfer from AWS S3 to Cloud Storage"],correct:3,explanation:"Storage Transfer Service is designed for large-scale data transfers from AWS S3, Azure Blob Storage, or HTTP/HTTPS sources to Cloud Storage. It provides parallel transfers, scheduling, filtering, and automatic retries - ideal for 100TB transfers.",wrongExplanations:{1:"Downloading locally requires massive bandwidth and local storage. For 100TB, this is impractical and slow. Storage Transfer Service transfers directly between clouds.",2:"While gsutil rsync can work, it runs from a single machine (limited bandwidth) and requires managing the transfer process. Storage Transfer Service provides managed, parallel transfers.",3:"Manual copying is completely impractical for 100TB. It would take months and be extremely error-prone. Automated transfer services are essential for large datasets."}},{id:254,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing migration - Migrate for Compute Engine",question:"During a migration using Migrate for Compute Engine, the migrated VM is experiencing performance issues. What could be the cause?",options:["Migrate for Compute Engine always reduces performance","Migration failed","The VM configuration is corrupted","The VM is still in streaming mode where some data is being read from source; performance improves after full data migration"],correct:3,explanation:"Migrate for Compute Engine uses streaming technology where VMs run in GCP while data migrates in background. During streaming, some disk reads may fetch from source, causing latency. Performance improves once migration completes and all data is local.",wrongExplanations:{1:"This is false. After migration completes, VMs run natively on Compute Engine with full performance. Temporary performance issues during streaming are expected and resolve post-migration.",2:"Configuration corruption would cause failures or errors, not just performance issues. Streaming mode specifically explains the performance characteristics during migration.",3:"If migration failed, the VM wouldn't be running. Performance issues during streaming mode are expected behavior, not failure."}},{id:255,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Transfer Appliance",question:"You need to transfer 1PB of data to Google Cloud but have limited internet bandwidth. What offline transfer option should you use?",options:["Storage Transfer Service over internet","Use gsutil over slow connection","Ship hard drives via regular mail","Transfer Appliance, a physical device shipped to you for loading data, then shipped to Google for upload"],correct:3,explanation:"Transfer Appliance is Google's solution for offline data transfer. Google ships a high-capacity storage device to your location. You load data locally (fast), ship the appliance back, and Google uploads data to your Cloud Storage bucket. Ideal for large datasets with limited bandwidth.",wrongExplanations:{1:"Storage Transfer Service requires internet connectivity. With limited bandwidth, transferring 1PB could take months or years. Transfer Appliance bypasses bandwidth limitations.",2:"Google doesn't support informal shipping of hard drives. Transfer Appliance is the official, secure offline transfer method with encryption and chain-of-custody tracking.",3:"gsutil over slow connection would take an impractical amount of time for 1PB. Transfer Appliance is specifically designed for large offline transfers."}},{id:256,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Assessment",question:"You're planning a large-scale migration to Google Cloud and need to discover and assess your current infrastructure. What tool should you use?",options:["Migrate everything immediately without assessment","Assessment is not necessary","Manually document all infrastructure","Use migration assessment tools like StratoZone or RVTools to discover infrastructure and plan migration"],correct:3,explanation:"Migration assessment tools automatically discover on-premises infrastructure, analyze dependencies, estimate costs, and recommend migration strategies. StratoZone (Google's tool) and similar products provide data-driven migration planning.",wrongExplanations:{1:"Manual documentation is time-consuming, incomplete, and quickly becomes outdated. Automated discovery tools provide comprehensive, accurate inventory faster.",2:"Migrating without assessment leads to cost overruns, architectural problems, and migration failures. Assessment identifies dependencies, sizing requirements, and potential issues.",3:"This is false. Assessment is a critical first phase of migration. It informs migration strategy, prioritization, timeline, and budget. Skipping it leads to failed migrations."}},{id:257,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Committed Use Discounts",question:"Your Compute Engine workloads run consistently 24/7 with predictable resource needs. How can you reduce costs?",options:["Purchase Committed Use Discounts (CUDs) for 1 or 3 years to get up to 57% discount","Use Spot VMs for production workloads","Scale down instances daily","No cost optimization is possible"],correct:0,explanation:"Committed Use Discounts provide significant savings (up to 57% for 3-year commitments) for predictable workloads. You commit to use a certain amount of vCPUs and memory in a region for 1 or 3 years. Ideal for steady-state production workloads.",wrongExplanations:{1:"Spot VMs can be preempted at any time, making them unsuitable for production workloads requiring 24/7 availability. CUDs provide discounts without preemption risk.",2:"Scaling down 24/7 workloads defeats the purpose and may impact availability. For always-on workloads, CUDs provide cost savings without operational changes.",3:"This is false. Multiple cost optimization options exist: CUDs, sustained use discounts, rightsizing, Spot VMs for fault-tolerant workloads, and custom machine types."}},{id:258,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Recommender",question:"You want to identify idle resources and rightsizing opportunities across your GCP organization. What should you use?",options:["GCP doesn't provide optimization recommendations","Wait for billing alerts","Active Assist Recommender which provides automated recommendations for cost optimization and resource efficiency","Manually review all resources in Cloud Console"],correct:2,explanation:"Active Assist Recommender analyzes resource usage and provides automated recommendations including idle resource deletion, VM rightsizing, committed use discounts, and more. It uses machine learning to identify optimization opportunities across your organization.",wrongExplanations:{1:"Manual review doesn't scale, misses patterns, and is time-consuming. Recommender uses ML to analyze usage patterns and identify opportunities you might miss.",2:"Billing alerts notify you of spending but don't identify specific optimization opportunities. Recommender proactively suggests cost-saving actions.",3:"This is false. Google Cloud provides comprehensive recommendations through Active Assist Recommender for cost, performance, security, and reliability improvements."}},{id:259,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cost optimization - Monitoring",question:"You notice unexpected cost increases in your billing. What tools should you use to investigate?",options:["Use Cloud Billing Reports with cost breakdown by service, SKU, and label; export to BigQuery for detailed analysis","Billing data is not available","Guess which service is causing costs","Wait for monthly invoice"],correct:0,explanation:"Cloud Billing Reports provide detailed cost breakdowns by project, service, SKU, label, and time period. Exporting to BigQuery enables custom analysis with SQL. This combination helps identify cost drivers and unexpected spending.",wrongExplanations:{1:"Monthly invoices provide totals but lack the granularity needed to investigate cost increases. Real-time billing reports enable proactive cost management.",2:"Guessing is ineffective and wasteful. Cloud Billing provides detailed data to identify exact sources of cost increases.",3:"This is false. GCP provides comprehensive billing data through Cloud Billing Reports, BigQuery export, budgets, and cost allocation using labels and projects."}},{id:260,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Rightsizing",question:"Your VM instances are consistently using only 20% of allocated CPU and memory. What should you do?",options:["Ignore utilization metrics","Add more VMs","Downsize to smaller machine types or use custom machine types to match actual resource needs","Keep overprovisioned to handle potential spikes"],correct:2,explanation:"Rightsizing reduces costs by matching instance size to actual usage. With 20% utilization, you're paying for 80% unused capacity. Downsizing or using custom machine types (exact vCPU/memory configuration) optimizes costs while meeting workload needs.",wrongExplanations:{1:"While some overhead is reasonable, 80% unused capacity is excessive. Autoscaling or load balancing can handle spikes more cost-effectively than constant overprovisioning.",2:"Adding VMs increases costs. The issue is oversized instances, not lack of instances. Rightsizing existing instances is the solution.",3:"Ignoring low utilization wastes money. Recommender provides rightsizing suggestions, and monitoring metrics guide optimization decisions."}},{id:261,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Spot VMs",question:"You have a batch processing job that can tolerate interruptions and restart from checkpoints. How can you minimize compute costs?",options:["Use the smallest instance type","Use Spot VMs which offer up to 90% discount but can be preempted when Google needs capacity","Use regular on-demand instances","Purchase committed use discounts"],correct:1,explanation:"Spot VMs (formerly preemptible VMs) offer significant discounts (up to 90%) for fault-tolerant, interruptible workloads. Since your job can checkpoint and restart, Spot VMs are ideal. Google can reclaim them with 30-second warning when capacity is needed.",wrongExplanations:{1:"On-demand instances cost more but provide no benefit for fault-tolerant workloads that can use Spot VMs. Spot VMs provide the same performance at much lower cost.",2:"CUDs provide discounts for committed usage but aren't as aggressive as Spot VM discounts. For batch jobs that tolerate interruption, Spot VMs offer better savings.",3:"The smallest instance might reduce costs but could make jobs run too slowly. Spot VMs provide the same instance types at 90% discount, giving better cost-performance balance."}},{id:262,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Storage classes",question:"You have log files that must be retained for compliance but are rarely accessed after 30 days. How can you reduce storage costs?",options:["Compress files manually","Keep all files in Standard storage class","Delete files after 30 days","Use Cloud Storage lifecycle policies to transition objects to Coldline or Archive storage after 30 days"],correct:3,explanation:"Cloud Storage lifecycle management automatically transitions objects between storage classes based on age or conditions. Coldline (30-90 day access) and Archive (yearly access) cost significantly less than Standard storage, perfect for compliance retention of rarely-accessed data.",wrongExplanations:{1:"Standard storage costs more than Coldline/Archive. Without lifecycle policies, you pay premium prices for rarely-accessed data, wasting money on retention.",2:"Deleting files violates compliance requirements. Lifecycle policies allow cost-effective retention by moving to cheaper storage classes, not deletion.",3:"While compression helps, it doesn't address storage class costs. Lifecycle policies to cheaper storage classes provide better cost reduction, and can be combined with compression."}},{id:263,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cost optimization - Budgets",question:"You want to automatically alert your team when project spending reaches 80% of monthly budget. What should you configure?",options:["Create a budget with threshold alert at 80% and configure email or Pub/Sub notification","Budgets cannot send alerts","Wait for budget overrun","Manually check billing daily"],correct:0,explanation:"Cloud Billing budgets allow setting spending thresholds with automated alerts. You can configure email notifications or Pub/Sub messages at specific thresholds (50%, 80%, 100%, etc.). This enables proactive cost management before budgets are exceeded.",wrongExplanations:{1:"Manual checking doesn't scale, can miss spending spikes, and is reactive. Automated budget alerts provide timely notifications without manual effort.",2:"Waiting for overrun means costs already exceeded budget. Proactive alerts at 80% allow time to investigate and take action before budget is fully consumed.",3:"This is false. Budget alerts are a core feature. You can configure multiple threshold alerts (percentage or absolute amounts) with email or Pub/Sub notifications."}},{id:264,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Labels",question:"Your organization has multiple teams and projects sharing a billing account. You need to track costs per team for chargeback. What should you implement?",options:["Cost allocation by team is not possible","Manually allocate costs in spreadsheets","Apply labels to resources with team names and use billing reports filtered by labels for cost allocation","Create separate billing accounts per team"],correct:2,explanation:"Labels are key-value pairs attached to resources (VMs, buckets, etc.). Billing exports include labels, enabling cost filtering and allocation. You can track costs by team, environment, cost center, or any dimension using labels. This enables accurate chargeback without separate billing accounts.",wrongExplanations:{1:"Separate billing accounts create administrative overhead and lose volume discounts. Labels provide cost allocation within a single billing account.",2:"Manual spreadsheet allocation is error-prone, time-consuming, and doesn't scale. Labels provide automated, accurate cost attribution in billing reports.",3:"This is false. Labels combined with billing reports and BigQuery export enable detailed cost allocation across any dimension you define."}},{id:265,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud NAT",question:"Your Compute Engine instances in a private subnet need to download updates from the internet but should not be directly accessible from the internet. What should you configure?",options:["Assign public IPs to all instances","Private instances cannot access the internet","Configure Cloud NAT to provide outbound internet access without assigning public IPs to instances","Use Cloud VPN for internet access"],correct:2,explanation:"Cloud NAT (Network Address Translation) provides managed outbound internet connectivity for private instances without exposing them to inbound internet traffic. Instances can download updates, access APIs, etc., while remaining private and secure.",wrongExplanations:{1:"Assigning public IPs exposes instances to inbound internet traffic, increasing attack surface. Cloud NAT provides outbound-only access, maintaining security.",2:"Cloud VPN connects to on-premises or other private networks, not the public internet. For internet access from private instances, Cloud NAT is the solution.",3:"This is false. Cloud NAT specifically enables private instances to access the internet for outbound connections while preventing inbound connections."}},{id:266,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - VPC Peering",question:"You have two VPC networks in the same organization and need to enable direct communication between resources in both VPCs. What should you configure?",options:["Use Cloud VPN to connect the VPCs","Set up VPC Network Peering to create direct network connectivity between the two VPCs","Assign public IPs and communicate over internet","VPCs cannot communicate"],correct:1,explanation:"VPC Network Peering creates direct, private connectivity between VPC networks within or across projects/organizations. Traffic stays on Google's network, providing low latency, high bandwidth, and security without internet or VPN gateways.",wrongExplanations:{1:"While Cloud VPN can connect VPCs, it adds latency and complexity. VPC Peering provides direct connectivity with better performance and simpler configuration for VPC-to-VPC connections.",2:"Using public IPs sends traffic over internet, increasing latency, costs (egress), and security risks. VPC Peering provides private, secure connectivity.",3:"This is false. VPC Peering is specifically designed for VPC-to-VPC connectivity within Google Cloud."}},{id:267,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Private Service Connect",question:"You need to access a Google-managed service (like BigQuery or Cloud SQL) from your VPC using private IP addresses without exposing traffic to the internet. What should you use?",options:["VPN is required for Google services","Use public IPs for all Google services","Private access to Google services is not possible","Private Service Connect or Private Google Access for private IP connectivity to Google services"],correct:3,explanation:"Private Service Connect enables private connectivity to Google APIs and services using internal IP addresses. Private Google Access allows VMs with only private IPs to reach Google services. Both keep traffic on Google's network without internet exposure.",wrongExplanations:{1:"Public IPs send traffic over internet, increasing latency and security risks. Private access keeps traffic on Google's private network.",2:"VPN is for connecting to on-premises or other external networks. Private Service Connect/Private Google Access provide direct private connectivity to Google services without VPN.",3:"This is false. Private Service Connect and Private Google Access specifically provide private IP connectivity to Google services."}},{id:268,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Shared VPC",question:"Your organization has multiple projects that need to share a common network while maintaining project-level isolation for billing and IAM. What should you implement?",options:["Use VPC Peering between all projects","Create identical VPC in each project","Use Shared VPC to share a VPC network across multiple projects with centralized network administration","Use a single project for all resources"],correct:2,explanation:"Shared VPC allows a central host project to share VPC networks with multiple service projects. Network admins manage networking centrally while project owners manage their resources. This provides centralized network control with project separation for billing and IAM.",wrongExplanations:{1:"Identical VPCs in each project create management overhead, inconsistency risk, and don't provide centralized control. Shared VPC centralizes network management.",2:"VPC Peering creates mesh connections between separate VPCs, requiring N(N-1)/2 connections. Shared VPC provides simpler hub-and-spoke model with centralized management.",3:"Single project loses project-level isolation for billing, IAM, and resource organization. Shared VPC provides network sharing while maintaining project separation."}},{id:269,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking troubleshooting - Connectivity Tests",question:"You need to verify that traffic can flow from a VM in one subnet to a VM in another subnet. What tool should you use to test connectivity?",options:["Connectivity testing is not available","Network Connectivity Tests (formerly Network Intelligence Center) to simulate traffic and identify connectivity issues","Guess if firewall rules are correct","Ping from the VM"],correct:1,explanation:"Network Connectivity Tests analyze your configuration (firewall rules, routes, VPC, etc.) and simulate traffic between endpoints without sending actual packets. It identifies misconfigurations blocking connectivity, providing detailed analysis of the network path.",wrongExplanations:{1:"Ping tests actual connectivity but doesn't explain why connection fails if it does. Connectivity Tests analyze configuration to identify issues before deploying or sending traffic.",2:"Guessing is inefficient and error-prone. Connectivity Tests provide automated analysis of firewall rules, routes, and network configuration.",3:"This is false. Network Connectivity Tests provide configuration analysis and traffic simulation to diagnose connectivity issues."}},{id:270,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud Interconnect",question:"You need high-bandwidth, low-latency connectivity between your on-premises datacenter and Google Cloud for production workloads. What should you use?",options:["Cloud VPN for all use cases","Public internet","Dedicated Interconnect (10 Gbps or 100 Gbps) or Partner Interconnect for high-bandwidth private connectivity","Interconnect is not available"],correct:2,explanation:"Cloud Interconnect provides private, high-bandwidth connectivity (10-100+ Gbps) with lower latency than internet. Dedicated Interconnect requires direct physical connection to Google. Partner Interconnect uses service provider connections. Both provide enterprise-grade connectivity for production workloads.",wrongExplanations:{1:"Cloud VPN provides up to 3 Gbps per tunnel over internet. For high-bandwidth requirements (10+ Gbps), Interconnect provides dedicated capacity and better performance.",2:"Public internet has variable latency, limited bandwidth, and security concerns. Interconnect provides private, dedicated connectivity with SLAs.",3:"This is false. Cloud Interconnect is Google's enterprise connectivity solution with Dedicated and Partner options for high-bandwidth private connectivity."}},{id:271,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud VPN",question:"You need to connect your on-premises network to Google Cloud VPC with automatic failover. What VPN configuration should you use?",options:["Multiple independent Classic VPNs","VPN doesn't support failover","HA VPN with two tunnels to two GCP gateways providing 99.99% SLA with automatic failover","Classic VPN with single tunnel"],correct:2,explanation:"HA VPN provides high availability with 99.99% SLA by using two VPN gateways and tunnels. Automatic failover occurs if one tunnel fails. Classic VPN offers 99.9% SLA with single gateway. HA VPN is recommended for production workloads requiring high availability.",wrongExplanations:{1:"Classic VPN with single tunnel provides 99.9% SLA without automatic failover. For HA requirements, HA VPN with redundant tunnels is the better choice.",2:"Multiple Classic VPNs can provide redundancy but require manual configuration and don't provide automatic failover. HA VPN provides built-in redundancy and automatic failover.",3:"This is false. HA VPN specifically provides automatic failover between redundant tunnels for high availability."}},{id:272,domain:"Configuring access and security",subdomain:"4.2 Network security - Firewall rules",question:"You need to allow SSH access to your Compute Engine instances only from your corporate IP range. What firewall rule should you create?",options:["Create ingress allow rule for TCP port 22 with source IP range matching your corporate IPs and target tags for your instances","SSH is automatically allowed","Allow port 22 from 0.0.0.0/0 (all IPs)","Firewall rules cannot filter by source IP"],correct:0,explanation:"VPC firewall rules can filter by source IP ranges. Creating an ingress allow rule for port 22 with your corporate IP range restricts SSH access to only your network. Using target tags applies the rule to specific instances, following least privilege.",wrongExplanations:{1:"Allowing SSH from all IPs (0.0.0.0/0) exposes instances to brute-force attacks and unauthorized access attempts. Restrict source IPs to known, trusted networks.",2:"This is false. Firewall rules support source IP filtering using CIDR ranges, making IP-based access control a core capability.",3:"SSH is not automatically allowed. Default VPC firewall rules allow some traffic, but custom VPCs require explicit rules. Even with default rules, restricting to corporate IPs improves security."}},{id:273,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud DNS",question:"Your application needs DNS resolution for services within your VPC using custom domain names. What should you configure?",options:["Use public DNS zones for internal names","Create a Cloud DNS private zone for internal DNS resolution within your VPC","Internal DNS is not supported","Manually edit /etc/hosts on each VM"],correct:1,explanation:"Cloud DNS private zones provide DNS resolution for resources within your VPC. They're not visible from the internet, making them ideal for internal service discovery and custom internal domain names. VMs in the VPC can resolve these names automatically.",wrongExplanations:{1:"Public DNS zones expose internal names to the internet, creating security risks. Private zones keep DNS resolution internal to your VPC.",2:"Manual /etc/hosts editing doesn't scale, is error-prone, and requires updating each VM when IPs change. Cloud DNS provides centralized, dynamic DNS management.",3:"This is false. Cloud DNS private zones specifically provide internal DNS resolution for VPC resources."}},{id:274,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Load balancing",question:"You need to load balance HTTPS traffic across multiple regions with automatic failover and content-based routing. What load balancer should you use?",options:["Manually distribute traffic with DNS","Internal Load Balancer","Global HTTP(S) Load Balancer with backend services in multiple regions and URL maps for content routing","Network Load Balancer"],correct:2,explanation:"Global HTTP(S) Load Balancer operates at L7, supports HTTPS, SSL termination, content-based routing (URL maps), and global distribution with automatic failover between regions. It routes users to the nearest healthy backend, providing optimal performance and availability.",wrongExplanations:{1:"Network Load Balancer operates at L4 (TCP/UDP), doesn't support HTTPS termination or content-based routing. For HTTP(S) with advanced routing, HTTP(S) Load Balancer is required.",2:"Internal Load Balancer is for VPC-internal traffic, not internet-facing global load balancing. Global HTTP(S) Load Balancer handles external traffic.",3:"Manual DNS-based distribution doesn't provide health checks, automatic failover, or content-based routing. Load balancers provide automated traffic distribution and failover."}},{id:275,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Flow Logs",question:"You need to troubleshoot network connectivity issues and analyze traffic patterns in your VPC. What should you enable?",options:["Firewall logs only","Network logging is not available","Cloud Logging only","VPC Flow Logs to capture IP traffic information for network monitoring and forensics"],correct:3,explanation:"VPC Flow Logs record samples of network flows sent/received by VM instances. They provide visibility into traffic patterns, help troubleshoot connectivity issues, perform security analysis, and optimize network usage. Logs can be analyzed in Cloud Logging or exported to BigQuery.",wrongExplanations:{1:"Cloud Logging captures application and system logs but doesn't capture network flow data. Flow Logs specifically provide network traffic visibility.",2:"Firewall logs show allowed/denied traffic based on rules but don't provide detailed flow information for all traffic. Flow Logs provide comprehensive traffic data.",3:"This is false. VPC Flow Logs specifically provide network traffic visibility for monitoring, troubleshooting, and security analysis."}},{id:276,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud CDN",question:"Your web application serves static content globally. Users far from your backend experience high latency. How can you improve performance?",options:["Increase backend instance size","Deploy VMs in every region","Content delivery optimization is not possible","Enable Cloud CDN with your HTTP(S) Load Balancer to cache content at Google's edge locations worldwide"],correct:3,explanation:"Cloud CDN caches content at Google's globally distributed edge locations. When enabled with HTTP(S) Load Balancer, static content is served from the edge location nearest to users, dramatically reducing latency and backend load. Cache headers control what's cached.",wrongExplanations:{1:"Deploying VMs in every region is expensive and complex. Cloud CDN provides global content distribution without managing infrastructure in multiple regions.",2:"Larger backend instances don't reduce network latency for distant users. CDN edge caching reduces latency by serving content close to users.",3:"This is false. Cloud CDN specifically optimizes content delivery by caching at edge locations worldwide."}},{id:277,domain:"Configuring access and security",subdomain:"4.2 Network security - Cloud Armor",question:"Your public-facing web application is experiencing DDoS attacks. What should you configure to protect it?",options:["Increase instance count","Use firewall rules only","DDoS protection is not available","Enable Cloud Armor with security policies on your HTTP(S) Load Balancer to block malicious traffic"],correct:3,explanation:"Cloud Armor provides DDoS protection, WAF capabilities, and customizable security policies for HTTP(S) Load Balancers. It can block traffic based on IP addresses, geolocation, request patterns, and more. Google's infrastructure absorbs attacks at the edge.",wrongExplanations:{1:"VPC firewall rules operate at L3/L4 and can't protect against application-layer (L7) attacks. Cloud Armor provides L7 protection for HTTP(S) traffic.",2:"Increasing instances helps with capacity but doesn't block attack traffic. Attackers can overwhelm any number of instances. Cloud Armor blocks malicious traffic before it reaches backends.",3:"This is false. Cloud Armor provides DDoS protection, WAF, and customizable security policies specifically for protecting HTTP(S) applications."}},{id:278,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Packet Mirroring",question:"You need to capture and analyze network traffic for security monitoring and compliance. What GCP feature should you use?",options:["Manual tcpdump on each instance","VPC Flow Logs only","Packet Mirroring to clone traffic from specified instances and send to monitoring tools","Packet capture is not supported"],correct:2,explanation:"Packet Mirroring clones traffic from specified instances and sends it to a collector instance running monitoring/security tools. This enables deep packet inspection, intrusion detection, and compliance monitoring without affecting production traffic.",wrongExplanations:{1:"Flow Logs provide metadata about traffic but not full packet capture. For deep packet inspection and protocol analysis, Packet Mirroring provides complete packet data.",2:"Manual tcpdump on instances requires installing software on each VM, affects performance, and doesn't scale. Packet Mirroring provides centralized, non-invasive capture.",3:"This is false. Packet Mirroring specifically provides network traffic cloning for security monitoring and analysis."}},{id:279,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Private Google Access",question:"Your VMs have only private IP addresses (no external IPs) but need to access Google Cloud services like Cloud Storage and BigQuery. What should you enable?",options:["Assign external IPs to all VMs","Use Cloud VPN","Private VMs cannot access Google services","Enable Private Google Access on the subnet to allow VMs with private IPs to reach Google services"],correct:3,explanation:"Private Google Access allows VMs with only private IPs to reach Google APIs and services (Cloud Storage, BigQuery, etc.) using internal routing. Traffic stays on Google's network without internet exposure. Enabled per subnet in VPC settings.",wrongExplanations:{1:"Assigning external IPs exposes VMs to internet, increasing security risks. Private Google Access provides service access while keeping VMs private.",2:"Cloud VPN is for connecting to on-premises networks, not for accessing Google Cloud services from VMs. Private Google Access is the solution for this use case.",3:"This is false. Private Google Access specifically enables private-only VMs to access Google Cloud services."}},{id:280,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - SSL Policies",question:"You need to ensure your HTTP(S) Load Balancer uses only strong TLS versions and cipher suites. What should you configure?",options:["Create and attach an SSL policy specifying minimum TLS version and allowed cipher suites","SSL configuration is automatic","TLS configuration is not available","Modify backend application code"],correct:0,explanation:"SSL policies define the TLS features (versions, cipher suites) that clients can use to connect to your load balancer. You can enforce minimum TLS 1.2, restrict weak ciphers, and meet compliance requirements. Policies attach to load balancer frontends.",wrongExplanations:{1:"While default SSL configuration is secure, custom requirements (compliance, security standards) often need specific TLS versions and ciphers. SSL policies provide this control.",2:"Backend applications don't handle SSL termination when using HTTP(S) Load Balancer. The load balancer terminates SSL, and SSL policies configure its behavior.",3:"This is false. SSL policies provide detailed control over TLS versions and cipher suites for HTTP(S) Load Balancers."}},{id:281,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Troubleshooting firewall",question:"You created a firewall rule to allow traffic but connections are still failing. What should you check?",options:["Verify rule priority, direction (ingress/egress), targets match your instances, and no higher-priority deny rules exist","Only check if rule exists","Firewall rules apply automatically","Delete all firewall rules"],correct:0,explanation:"Firewall rules are evaluated by priority (lower number = higher priority). A deny rule with higher priority overrides allow rules. Also verify direction (ingress/egress), source/destination filters, target tags/service accounts match your instances, and protocols/ports are correct.",wrongExplanations:{1:"Just checking if a rule exists isn't sufficient. Misconfigured priority, targets, or competing deny rules can prevent the allow rule from working.",2:"Firewall rules apply automatically when created, but misconfigurations can prevent them from working as expected. Systematic troubleshooting is needed.",3:"Deleting all rules creates default deny, making things worse. Systematic review of priority, targets, and rule logic identifies the issue."}},{id:282,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Service Directory",question:"You have microservices across multiple environments (GKE, Compute Engine, on-premises) and need unified service discovery. What should you use?",options:["Kubernetes service discovery only","Service Directory for centralized service registry across hybrid environments","Manual service endpoint management","Unified service discovery is not available"],correct:1,explanation:"Service Directory provides a unified service registry for hybrid and multi-cloud environments. Services from GKE, Compute Engine, Cloud Run, and on-premises can register, enabling centralized discovery and DNS-based resolution across environments.",wrongExplanations:{1:"Kubernetes service discovery only works within a K8s cluster. For services outside GKE or in multiple clusters, Service Directory provides cross-environment discovery.",2:"Manual endpoint management doesn't scale, is error-prone, and lacks automation. Service Directory provides automated registration and discovery.",3:"This is false. Service Directory specifically provides unified service discovery across hybrid and multi-cloud environments."}},{id:283,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud Router",question:"You're using Cloud VPN or Interconnect and need to dynamically exchange routes between your on-premises network and GCP VPC. What should you configure?",options:["Cloud Router with BGP to dynamically exchange routes between on-premises and VPC","Static routes only","Manual route updates","Dynamic routing is not supported"],correct:0,explanation:"Cloud Router uses BGP (Border Gateway Protocol) to dynamically exchange routes between your VPC and on-premises network via VPN or Interconnect. This eliminates manual route management when networks change, providing automatic failover and load distribution.",wrongExplanations:{1:"Static routes require manual updates whenever network topology changes. Cloud Router with BGP automates route exchange, providing dynamic adaptation to network changes.",2:"Manual route updates don't scale, are error-prone, and can't provide automatic failover. Cloud Router with BGP automates routing.",3:"This is false. Cloud Router with BGP specifically provides dynamic routing for VPN and Interconnect connections."}},{id:284,domain:"Configuring access and security",subdomain:"4.2 Network security - Hierarchical Firewall Policies",question:"You need to enforce organization-wide firewall rules across all VPCs and projects. What should you use?",options:["Hierarchical Firewall Policies at organization or folder level to enforce rules across multiple projects","Use organization policies","Cross-project firewall rules are not possible","Create identical VPC firewall rules in each project manually"],correct:0,explanation:"Hierarchical Firewall Policies apply at organization or folder level, enforcing rules across all VPCs in child projects. This enables centralized security policy management without duplicating rules in each project. Rules are inherited and can't be overridden by projects.",wrongExplanations:{1:"Creating identical rules in each project is error-prone, doesn't scale, and lacks centralized control. Hierarchical policies provide centralized enforcement.",2:"Organization policies control GCP resource configurations (e.g., allowed regions) but don't provide network firewall functionality. Hierarchical Firewall Policies handle network security.",3:"This is false. Hierarchical Firewall Policies specifically enable organization-wide network security policies across projects and VPCs."}},{id:285,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Internal Load Balancer",question:"You need to load balance traffic between VMs for a private internal application not exposed to the internet. What load balancer should you use?",options:["Internal load balancing is not available","Network Load Balancer (external)","Global HTTP(S) Load Balancer","Internal TCP/UDP Load Balancer for private load balancing within your VPC"],correct:3,explanation:"Internal TCP/UDP Load Balancer distributes traffic within your VPC using private IPs. It's not accessible from internet, making it ideal for internal services, microservices communication, and multi-tier applications. Supports regional backends with health checks.",wrongExplanations:{1:"Global HTTP(S) Load Balancer is for internet-facing applications with global backends. For private internal load balancing, Internal Load Balancer is the correct choice.",2:"Network Load Balancer (external) uses external IPs and is internet-facing. For internal applications, Internal Load Balancer with private IPs is appropriate.",3:"This is false. Internal TCP/UDP Load Balancer and Internal HTTP(S) Load Balancer specifically provide private load balancing within VPCs."}},{id:286,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Network metrics",question:"You need to monitor load balancer performance including latency, request count, and backend health. Where should you look?",options:["Load balancer logs only","Guess performance from application behavior","Load balancer metrics are not available","Cloud Monitoring metrics for load balancers showing request count, latency, error rates, and backend health"],correct:3,explanation:"Load balancers export comprehensive metrics to Cloud Monitoring including request/response counts, latency percentiles, error rates, bandwidth, and backend health status. These metrics enable performance monitoring, capacity planning, and alerting.",wrongExplanations:{1:"Logs provide request details but metrics provide aggregated performance data better suited for monitoring trends, alerting on thresholds, and dashboards.",2:"Guessing is ineffective. Cloud Monitoring provides detailed, real-time metrics for data-driven performance analysis.",3:"This is false. Load balancers export comprehensive metrics to Cloud Monitoring for performance monitoring and alerting."}},{id:287,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud Domains",question:"You need to register a new domain name and manage DNS within Google Cloud. What service should you use?",options:["Domain registration is not available in GCP","Cloud DNS can register domains","Cloud Domains for domain registration and integrated DNS management with Cloud DNS","Use external registrar only"],correct:2,explanation:"Cloud Domains provides domain registration with integrated Cloud DNS management. You can register new domains, transfer existing domains, and manage DNS zones in one place. It simplifies domain management within GCP with built-in DNSSEC support.",wrongExplanations:{1:"While external registrars work, Cloud Domains provides integrated experience with Cloud DNS, GCP billing, and IAM. It simplifies management for GCP-hosted applications.",2:"This is false. Cloud Domains specifically provides domain registration services integrated with GCP.",3:"Cloud DNS provides DNS hosting and management but not domain registration. Cloud Domains handles registration, while Cloud DNS manages zones."}},{id:288,domain:"Configuring access and security",subdomain:"4.2 Network security - Private Service Connect endpoints",question:"You need to access a partner's service privately without exposing traffic to the internet. The partner has published their service via Private Service Connect. What should you create?",options:["Set up VPN to partner's network","Private access to partner services is not possible","Create a Private Service Connect endpoint in your VPC to access the partner's service privately","Use public internet connection"],correct:2,explanation:"Private Service Connect endpoints allow consuming services (from Google, partners, or other projects) via private IP addresses in your VPC. Traffic stays on Google's network without internet exposure. Provides secure, private connectivity to published services.",wrongExplanations:{1:"Public internet exposes traffic, increases latency, and creates security risks. Private Service Connect provides private connectivity.",2:"VPN requires coordination with partner and setup on both sides. Private Service Connect provides one-way private access without bilateral VPN setup when the service is PSC-published.",3:"This is false. Private Service Connect specifically enables private consumption of services published by other organizations or projects."}},{id:289,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Network Address Usage",question:"You're planning VPC subnet IP address ranges for a growing application. What should you consider?",options:["Use smallest possible subnet ranges","Plan subnet ranges with enough capacity for growth, avoid overlapping with other networks, and remember you can expand subnets later","Subnet size cannot be changed","IP planning is not necessary"],correct:1,explanation:"VPC subnets should be sized for current needs plus growth. Avoid overlaps with on-premises or other VPCs for peering/interconnect. Good news: subnets can be expanded (not shrunk) later, so start reasonable and expand as needed. Consider secondary ranges for GKE pods/services.",wrongExplanations:{1:"Smallest ranges may require frequent expansion or cause fragmentation. While expansion is possible, planning for growth reduces operational overhead.",2:"This is false. VPC subnets can be expanded (increasing CIDR range) without recreating the subnet or VMs. They cannot be shrunk, only expanded.",3:"IP planning prevents overlaps with other networks, ensures sufficient capacity, and enables VPC peering and interconnect. Poor planning causes issues later."}},{id:290,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Firewall Insights",question:"You want to identify unused or overly permissive firewall rules in your VPC. What tool should you use?",options:["Rule analysis is not available","Firewall Insights (part of Network Intelligence Center) to analyze firewall rule usage and identify shadowed, unused, or overly permissive rules","Delete all rules and recreate","Manually review all rules"],correct:1,explanation:"Firewall Insights analyzes firewall rules and provides insights about shadowed rules (overridden by higher priority), unused rules (no matching traffic), overly permissive rules, and rules that allow access from all IPs. Helps optimize security and reduce complexity.",wrongExplanations:{1:"Manual review of hundreds of rules is time-consuming and error-prone. Firewall Insights automates analysis and identifies issues.",2:"Deleting all rules causes service disruption. Firewall Insights identifies which rules can be safely removed or tightened.",3:"This is false. Firewall Insights specifically provides automated analysis of firewall rules for security optimization."}},{id:291,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Network Service Tiers",question:"You need to optimize costs for outbound internet traffic and can accept standard internet routing. What network service tier should you use?",options:["Service tiers are not available","Premium Tier is always required","Standard Tier for lower-cost internet egress using standard routing","Network tiers don't affect cost"],correct:2,explanation:"Network Service Tiers: Premium (default) routes traffic on Google's network globally for best performance. Standard routes using public internet, reducing egress costs. For cost-sensitive, less latency-critical workloads, Standard Tier reduces costs by 30-50% for egress.",wrongExplanations:{1:"This is false. Premium Tier provides best performance but higher cost. Standard Tier is available for cost-optimized egress at the expense of routing on public internet.",2:"This is false. Network Service Tiers (Premium vs Standard) directly impact egress pricing. Standard Tier costs 30-50% less for outbound internet traffic.",3:"This is false. Google Cloud offers Premium and Standard Network Service Tiers with different performance and cost characteristics."}},{id:292,domain:"Configuring access and security",subdomain:"4.2 Network security - Cloud IDS",question:"You need intrusion detection capabilities to identify network threats in your VPC traffic. What should you deploy?",options:["Cloud Armor only","Cloud IDS (Intrusion Detection System) to detect malware, spyware, and command-and-control attacks","Firewall rules only","Intrusion detection is not available"],correct:1,explanation:"Cloud IDS provides managed intrusion detection powered by Palo Alto Networks technology. It inspects traffic using Packet Mirroring, detects threats (malware, spyware, command-and-control), and integrates with Chronicle Security or SIEM tools for threat analysis.",wrongExplanations:{1:"Firewall rules block/allow based on IP, port, protocol but don't perform deep packet inspection for malware or exploits. Cloud IDS provides threat detection beyond basic filtering.",2:"Cloud Armor protects HTTP(S) applications from DDoS and web attacks but doesn't provide general network intrusion detection for all protocols. Cloud IDS monitors all traffic types.",3:"This is false. Cloud IDS specifically provides managed intrusion detection for VPC traffic."}},{id:293,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Bring your own IP",question:"Your organization has existing public IP addresses and wants to use them in Google Cloud for brand recognition and IP reputation. Is this possible?",options:["BYOIP is only for private IPs","IP addresses cannot be transferred","No, you must use Google-provided IPs","Yes, use Bring Your Own IP (BYOIP) to bring your own public IP addresses to GCP"],correct:3,explanation:"BYOIP allows bringing your own public IP address ranges (IPv4 /24 or larger) to Google Cloud. Useful for maintaining IP reputation, avoiding IP changes during migration, or meeting compliance requirements. Requires proof of ownership and ROA configuration.",wrongExplanations:{1:"This is false. BYOIP specifically enables using your own public IP addresses in GCP after verification and configuration.",2:"BYOIP is for public IP addresses. Private IPs are defined in your VPC subnets and don't require BYOIP since they're not globally routed.",3:"IP addresses can be transferred to GCP through BYOIP after proving ownership and configuring route origin authorization (ROA) with your RIR."}},{id:294,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Load balancer troubleshooting",question:"Your HTTP(S) Load Balancer is returning 502 Bad Gateway errors. What are likely causes to investigate?",options:["Firewall rules are blocking frontend traffic","The load balancer is broken","SSL certificate is expired","Backend instances are unhealthy, not responding, or responding with errors; check backend health and application logs"],correct:3,explanation:"502 errors indicate the load balancer received invalid responses from backends. Common causes: backends failing health checks, application crashes, backend timeouts, or backends returning errors. Check backend health status, application logs, and backend capacity.",wrongExplanations:{1:"Load balancers are highly reliable managed services. 502 errors typically indicate backend issues, not load balancer failure. Investigate backend health first.",2:"If firewall blocked frontend traffic, clients couldn't reach the load balancer (connection timeout). 502 errors mean load balancer received the request but backends responded improperly.",3:"Expired SSL certificates cause SSL/TLS errors, not 502. 502 specifically indicates bad responses from backends after successful SSL termination at the load balancer."}},{id:295,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Traffic Director",question:"You need advanced traffic management for microservices (traffic splitting, A/B testing) across GKE and Compute Engine with service mesh capabilities. What should you use?",options:["Manual traffic distribution","Traffic Director for service mesh traffic management across hybrid environments","Advanced traffic management is not available","Basic load balancers only"],correct:1,explanation:"Traffic Director provides service mesh traffic management including advanced routing, traffic splitting, A/B testing, and failover. Works with Envoy proxies across GKE, Compute Engine, and on-premises. Enables advanced microservices patterns without code changes.",wrongExplanations:{1:"Basic load balancers provide simple round-robin or weighted distribution but lack advanced routing rules, traffic splitting percentages, and service mesh capabilities.",2:"Manual traffic distribution requires code changes or complex configurations. Traffic Director provides declarative traffic management through service mesh.",3:"This is false. Traffic Director specifically provides advanced service mesh traffic management for microservices architectures."}},{id:296,domain:"Configuring access and security",subdomain:"4.2 Network security - Firewall rule logging",question:"You need to audit which firewall rules are allowing or denying traffic for compliance. What should you enable?",options:["Enable firewall rule logging on specific rules to log allowed or denied connections","Firewall logs are automatically enabled","Use VPC Flow Logs only","Firewall logging is not available"],correct:0,explanation:"Firewall rule logging can be enabled per rule to log connections allowed or denied by that rule. Logs include source/destination IPs, ports, protocol, and rule applied. Useful for auditing, troubleshooting, and compliance. Logs go to Cloud Logging.",wrongExplanations:{1:"Firewall rule logging must be explicitly enabled per rule. It's not automatic to avoid generating excessive logs for rules that don't require auditing.",2:"VPC Flow Logs show traffic patterns but don't indicate which firewall rule applied. Firewall rule logging specifically identifies the rule for each connection.",3:"This is false. Firewall rules support logging to track which rules allow or deny traffic for auditing and troubleshooting."}},{id:297,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Network tags",question:"You need to apply firewall rules to a subset of instances without using IP addresses. What mechanism should you use?",options:["Firewall rules apply to all instances only","List individual instance IPs in rules","Use network tags to identify instances and apply firewall rules based on tags","Instance-specific rules are not possible"],correct:2,explanation:"Network tags are labels applied to instances that can be used in firewall rules as targets or source filters. This provides flexible, maintainable rules (e.g., 'allow SSH to instances tagged web-servers') without managing IP lists. Tags can be changed without modifying rules.",wrongExplanations:{1:"Listing individual IPs is brittle and doesn't scale. When instances change, IP-based rules must be updated. Tags provide dynamic, maintainable rule application.",2:"This is false. Firewall rules can target all instances, instances with specific tags, instances using specific service accounts, or instances in specific IP ranges.",3:"This is false. Network tags specifically enable applying firewall rules to specific subsets of instances without IP address management."}},{id:298,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Backend service health",question:"Your load balancer shows backends as unhealthy but the VMs are running. What should you investigate?",options:["Health check configuration cannot cause issues","Delete and recreate load balancer","Restart all backends","Check health check configuration (protocol, port, path), verify application responds on health check endpoint, and check firewall rules allow health checker IPs"],correct:3,explanation:"Common causes of unhealthy backends: health check configured for wrong port/path, application not responding on health endpoint, firewall blocking health checker IP ranges (35.191.0.0/16, 130.211.0.0/22), or application requiring too long to respond. Systematic troubleshooting identifies the issue.",wrongExplanations:{1:"Restarting backends doesn't fix configuration issues like wrong health check port or blocked health checker IPs. Investigate health check settings first.",2:"Deleting and recreating doesn't fix misconfigurations. The same issues would persist. Troubleshoot health check configuration systematically.",3:"This is false. Misconfigured health checks are a common cause of unhealthy backend status. Port, path, protocol, and firewall rules all affect health check success."}},{id:299,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - IPv6 support",question:"Your application needs to support IPv6 clients. Can you configure IPv6 in Google Cloud VPCs?",options:["No, only IPv4 is supported","Yes, VPCs support IPv6 with dual-stack subnets (both IPv4 and IPv6) and IPv6 load balancing","IPv6 requires separate VPCs","IPv6 is only for external IPs"],correct:1,explanation:"GCP supports IPv6 with dual-stack VPC subnets (both IPv4 and IPv6 addresses). Global HTTP(S) Load Balancer supports IPv6 clients. VMs can have both IPv4 and IPv6 addresses. This enables serving IPv6-only clients while maintaining IPv4 compatibility.",wrongExplanations:{1:"This is false. GCP supports IPv6 in VPCs with dual-stack configuration, allowing both IPv4 and IPv6 communication.",2:"IPv6 is supported for both external (internet-facing) and internal (VPC) addressing in dual-stack configuration.",3:"VPCs support dual-stack (IPv4 + IPv6) within the same VPC. Separate VPCs are not required for IPv6 support."}},{id:300,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"A junior developer is struggling to write a complex `gcloud` command to create a custom Compute Engine instance with specific network tags and a startup script. They need to do this quickly without searching through extensive documentation. Which Google Cloud tool should they use to get a context-aware, executable `gcloud` command by describing their goal in natural language?",options:["Gemini Cloud Assist","Google Cloud documentation search","Cloud Shell Editor","Stack Overflow"],correct:0,explanation:"Gemini Cloud Assist is an AI-powered assistant integrated into the Google Cloud console that can generate configurations, code, and commands based on natural language prompts. It is the most direct and efficient tool for this specific task.",wrongExplanations:{1:"Cloud Shell Editor is an IDE, but it doesn't generate commands from natural language. The user would still need to know the command syntax.",2:"Searching the documentation is a valid but slower approach compared to using the AI assistant designed for this purpose.",3:"While Stack Overflow is a useful resource, it's external to the Google Cloud environment and may not provide a command tailored to the user's specific project context as Gemini can."}},{id:301,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"Your company's security team wants to enforce a policy that prevents any new VM instances from being created with public IP addresses across the entire Google Cloud organization, except for a specific project used for public-facing web servers. How can you implement this requirement in the most efficient way?",options:["Write a script that runs hourly to check for and delete VMs with public IPs in non-approved projects.","Set an Organization Policy with the `compute.vmExternalIpAccess` constraint at the organization root, and disable it for the specific project.","Configure VPC firewall rules in each project to deny egress traffic, except for the specific project.","Create a custom IAM role that denies the `compute.instances.addAccessConfig` permission and apply it to all users except those in the specific project."],correct:1,explanation:"Organization Policies are the standard, hierarchical way to enforce constraints on cloud resources. Applying the `compute.vmExternalIpAccess` constraint at the organization level and then modifying it for the specific project (by overriding the policy) is the most scalable and correct approach.",wrongExplanations:{1:"Managing this through IAM is cumbersome, error-prone, and doesn't prevent a user with sufficient privileges from adding a public IP. Organization policies provide a direct guardrail.",2:"A reactive script is inefficient and creates a time window where non-compliant resources can exist. A preventative control like an Organization Policy is the best practice.",3:"Firewall rules control traffic, not the configuration of the VM itself. A VM could still be created with a public IP, even if traffic is blocked."}},{id:302,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE Autopilot",question:"A development team wants to deploy a new containerized microservice on Google Kubernetes Engine (GKE). They want to focus solely on their application code and not manage the underlying nodes, scaling, or resource allocation. Their highest priority is operational efficiency and reduced management overhead. Which GKE mode of operation should they choose?",options:["Regional Cluster","Standard","Private Cluster","Autopilot"],correct:3,explanation:"GKE Autopilot is a fully managed mode of operation where Google manages the cluster's control plane, nodes, and resources. You only define your workloads (pods, deployments) and pay for the resources they consume, aligning perfectly with the team's requirement to eliminate node management.",wrongExplanations:{1:"GKE Standard requires you to manage your own node pools, including selecting machine types, configuring scaling, and performing upgrades. This introduces significant management overhead.",2:"'Regional Cluster' describes a high-availability feature available in both Standard and Autopilot modes, but it is not a mode of operation itself.",3:"'Private Cluster' describes a security feature for network isolation available in both modes, not a mode of operation."}},{id:303,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud NGFW",question:"A network administrator needs to implement a granular firewall policy for a complex VPC. They want to apply rules based on application identity (e.g., 'frontend-web-servers') rather than static IP addresses, to simplify management as services scale and IPs change. Which feature of Cloud NGFW (Next-Generation Firewall) should they use to achieve this?",options:["VPC firewall rules with IP ranges","Secure tags and network firewall policies","Hierarchical firewall policies","Identity-Aware Proxy (IAP)"],correct:1,explanation:"Cloud NGFW introduces secure tags, which are distinct from network tags. Secure tags are IAM-controlled attributes that can be attached to resources like VMs. Network firewall policies can then use these tags to define rules, allowing for identity-based microsegmentation independent of IP addresses.",wrongExplanations:{1:"Hierarchical firewall policies apply rules at the Organization/Folder level but don't inherently provide application-identity based rules like secure tags do.",2:"Using traditional VPC firewall rules with IP ranges is precisely the static, difficult-to-manage approach the administrator wants to avoid.",3:"IAP is used to control access to applications based on user identity (e.g., a specific Google user), not for defining network-layer traffic rules between services within a VPC."}},{id:304,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"A database administrator notices that a critical application using Cloud SQL for PostgreSQL is experiencing performance degradation. They suspect inefficient queries are the cause but need to identify the exact queries and users responsible without installing third-party tools. Which tool should they use?",options:["Cloud SQL logs in Cloud Logging","Cloud Monitoring metrics for Cloud SQL","Cloud Profiler","Query Insights"],correct:3,explanation:"Query Insights is a built-in tool for Cloud SQL that provides visual, detailed diagnostics for query performance. It helps you detect, diagnose, and prevent query performance problems for Cloud SQL databases by showing query load, problematic queries, and where the query load is originating from (user, IP address).",wrongExplanations:{1:"While Cloud SQL logs contain information, they are not designed for easy performance analysis and lack the visual dashboards and aggregated metrics of Query Insights.",2:"Cloud Monitoring provides high-level metrics like CPU and memory usage, but it does not give insight into the performance of individual SQL queries.",3:"Cloud Profiler is used for analyzing application code performance (CPU/memory profiling of the application itself), not for diagnosing database query performance."}},{id:305,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You need to grant a new DevOps engineer the ability to view almost all resources in a project and manage Compute Engine instances, but you must prevent them from modifying any IAM policies. Which combination of predefined roles should you assign?",options:["Compute Admin and Network Viewer","Editor and Security Reviewer","Project Owner","Viewer and Compute Admin"],correct:3,explanation:"The Viewer (`roles/viewer`) role provides read-only access to most Google Cloud resources. The Compute Admin (`roles/compute.admin`) role provides full control over Compute Engine resources. This combination meets the requirements without granting broad modification permissions on other services or IAM policies.",wrongExplanations:{1:"The Project Owner role is too permissive; it grants full control over all resources, including the ability to modify IAM policies.",2:"The Editor role is too broad, allowing modification of most resources, not just Compute Engine. Security Reviewer is a read-only role for security policies, but the combination with Editor is overly permissive.",3:"This combination is too restrictive. It only provides access to Compute Engine and networking resources, failing to meet the requirement of viewing almost all resources in the project."}},{id:306,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"A fleet manager is responsible for over 50 Cloud SQL, Bigtable, and Spanner instances across multiple projects. They need a centralized dashboard to get a high-level overview of the health, performance, and security posture of their entire database fleet without navigating to each individual instance. Which Google Cloud feature provides this capability?",options:["Security Command Center","Cloud Monitoring Dashboards","The Cloud SQL instances page","Database Center"],correct:3,explanation:"Database Center is a centralized dashboard designed to provide a comprehensive overview of your entire database fleet. It consolidates performance metrics, security findings, and operational health information for various database types into a single pane of glass.",wrongExplanations:{1:"While you could build a custom Cloud Monitoring Dashboard, it would require significant effort to replicate the specialized, out-of-the-box fleet-level insights provided by Database Center.",2:"The Cloud SQL instances page only shows Cloud SQL instances and lacks the fleet-wide, multi-service overview that includes Bigtable and Spanner.",3:"Security Command Center focuses on security vulnerabilities and threats, not on operational health and performance metrics for databases, which is a key feature of Database Center."}},{id:307,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"Your company is migrating a large, mission-critical PostgreSQL database to Google Cloud. They require full PostgreSQL compatibility, high availability with a 99.99% SLA, and performance that is significantly better than standard open-source PostgreSQL. Which database service is the best fit?",options:["PostgreSQL on a Compute Engine VM","Cloud Spanner with PostgreSQL interface","AlloyDB for PostgreSQL","Cloud SQL for PostgreSQL"],correct:2,explanation:"AlloyDB for PostgreSQL is a fully managed, PostgreSQL-compatible database service designed for high-performance, high-availability workloads. It offers superior performance compared to standard PostgreSQL and a 99.99% availability SLA, making it the ideal choice for this demanding scenario.",wrongExplanations:{1:"Cloud SQL for PostgreSQL is a great choice for general-purpose PostgreSQL workloads but may not meet the 'significantly better' performance requirement. Its SLA is typically 99.95% for regional instances.",2:"Cloud Spanner is a globally distributed database. While it has a PostgreSQL interface, it's not fully wire-compatible and is designed for different use cases (global scale, horizontal scalability) than a traditional lift-and-shift of a PostgreSQL application.",3:"Running PostgreSQL on a VM provides full control but shifts all management responsibility (HA, backups, patching) to the user and does not inherently provide the required performance or SLA without complex custom architecture."}},{id:308,domain:"Section 4: Configuring access and security",subdomain:"Cloud NGFW policies",question:"A security admin is using Cloud NGFW with secure tags to control traffic. They have a VM tagged with `role=frontend` and a database tagged with `role=backend`. They need to create a rule that allows TCP port 5432 traffic from any frontend server to any backend server within the VPC. What should be the source and destination in the network firewall policy rule?",options:["Source: Service Account of frontend VMs, Destination: Service Account of backend VMs","Source: Network tag `frontend`, Destination: Network tag `backend`","Source: Tag `role=frontend`, Destination: Tag `role=backend`","Source: IP range of frontend VMs, Destination: IP range of backend VMs"],correct:2,explanation:"Network firewall policies in Cloud NGFW are designed to use secure tags for defining source and destination. This allows for an identity-based security posture where rules are not tied to ephemeral IP addresses, which is the core benefit of this feature.",wrongExplanations:{1:"Network tags are a legacy feature and are not used in the more advanced network firewall policies. The question specifically mentions secure tags, which are different.",2:"Using IP ranges is the static method that secure tags are designed to replace. It's less flexible and harder to manage.",3:"While firewall rules can use service accounts, secure tags are the more modern and flexible Cloud NGFW feature for creating granular, resource-identity-based policies."}},{id:309,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You are setting up an alerting policy in Cloud Monitoring for a critical Compute Engine instance group. You want to be notified if the average CPU utilization across the group exceeds 80% for a continuous period of 10 minutes. How should you configure the alerting policy condition?",options:["Metric: CPU load (1m), Aggregation: mean, Condition: is above 0.80, Duration: 10 minutes","Metric: CPU utilization, Aggregation: mean, Condition: is above 80%, Duration: 10 minutes","Create a Log-based metric for CPU utilization and alert when its count exceeds a threshold.","Metric: CPU utilization, Aggregation: max, Condition: is above 80%, Duration: 10 minutes"],correct:1,explanation:"To monitor the average CPU utilization for a group, you should select the 'CPU utilization' metric, aggregate it across the group using 'mean', set the condition to trigger 'is above' a threshold of 80%, and set the duration for the condition to be met to '10 minutes'. This configuration precisely matches the requirement.",wrongExplanations:{1:"Using 'max' for aggregation would trigger an alert if any single instance spikes above 80%, not when the average of the group does.",2:"CPU load is a different metric than CPU utilization. While related, utilization is the direct measure of the percentage of CPU being used.",3:"Log-based metrics are for analyzing log entries. CPU utilization is a standard system metric available directly in Cloud Monitoring and does not require creating metrics from logs."}},{id:310,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"Your finance team needs to receive an automated notification when a specific development project's spending is projected to exceed its monthly budget of $500. You must implement this with minimal effort. What should you do?",options:["Write a Cloud Function that is triggered daily to analyze billing data from BigQuery and send an email.","Manually export the billing report each week and check the project's spend.","Set up a Cloud Monitoring alert on the 'Total Cost' metric for the project.","Create a budget in Cloud Billing for the project with a 100% projected spend alert."],correct:3,explanation:"Cloud Billing budgets are the native, simplest way to monitor and control costs. You can set a budget amount and configure alert thresholds based on actual or projected spend, which directly addresses the requirement.",wrongExplanations:{1:"This is overly complex. While possible, it requires setting up a billing export to BigQuery and writing custom code, which is not minimal effort.",2:"Cloud Monitoring is for performance metrics, not for sophisticated budget alerting based on projections. Billing budgets are the specific tool for this purpose.",3:"Manual checking is inefficient, error-prone, and not automated, failing to meet a key requirement."}},{id:311,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You are deploying a batch processing workload that is highly fault-tolerant and can be interrupted. The job runs for several hours. To minimize costs, which type of Compute Engine VM should you use?",options:["Standard VM","Spot VM","Preemptible VM","E2-series VM"],correct:1,explanation:"Spot VMs are the latest generation of interruptible instances, offering the largest discounts (60-91% off standard prices). They are the ideal choice for fault-tolerant, cost-sensitive workloads that can handle interruptions, as they provide the best cost savings.",wrongExplanations:{1:"Preemptible VMs are the older generation of interruptible instances. While they also offer discounts, Spot VMs are the newer, recommended option with generally better pricing and features.",2:"Standard VMs offer no discounts and are not the most cost-effective choice for an interruptible workload.",3:"E2-series VMs are a cost-effective machine series, but they are not specifically designed for interruptible workloads and do not offer the steep discounts of Spot VMs."}},{id:312,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"Your company has a strict data residency policy that requires all audit logs generated in a specific European project to be stored in a Cloud Storage bucket located in the EU for long-term retention. How can you automate this process?",options:["Set up a Pub/Sub topic to receive logs and a Dataflow job to write them to the bucket.","Create a log sink at the project level with a Cloud Storage bucket in the EU as the destination.","Configure the default log bucket's location to be in the EU.","Write a script to run `gcloud logging read` and pipe the output to a file in an EU bucket."],correct:1,explanation:"Log sinks are the standard mechanism in Cloud Logging for routing log entries to supported destinations. Creating a sink at the project level allows you to filter for specific logs (like audit logs) and send them to a Cloud Storage bucket, Pub/Sub topic, or BigQuery dataset, respecting the destination's location.",wrongExplanations:{1:"A manual script is not a reliable or scalable automation solution. Log sinks are the native, managed way to handle this.",2:"You cannot change the location of the default `_Default` and `_Required` log buckets. You must create a sink to route logs elsewhere.",3:"Using Pub/Sub and Dataflow is a valid but overly complex and expensive solution for a simple log routing and storage requirement. A direct sink is much more efficient."}},{id:313,domain:"Section 4: Configuring access and security",subdomain:"Service Accounts",question:"An application running on a Compute Engine VM needs to read files from a Cloud Storage bucket. Following the principle of least privilege, what is the most secure way to grant this access?",options:["Assign the 'Editor' role to the VM's service account.","Generate a service account key, store it on the VM, and use it to authenticate.","Grant the 'Storage Object Viewer' role to the 'allUsers' special identifier on the bucket.","Attach a service account with the 'Storage Object Viewer' role to the VM instance."],correct:3,explanation:"The best practice is to attach a service account with the minimal necessary permissions (in this case, `roles/storage.objectViewer`) directly to the Compute Engine instance. This allows the application to use the built-in metadata server to get credentials automatically, avoiding the security risk of managing and storing service account keys.",wrongExplanations:{1:"Storing service account keys on a VM is a security risk. If the VM is compromised, the key can be stolen and used from anywhere. This practice should be avoided whenever possible.",2:"Granting access to 'allUsers' would make the bucket public, which is a major security violation.",3:"The 'Editor' role is overly permissive. It grants read and write access to most project resources, which violates the principle of least privilege."}},{id:314,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"You are designing the network for a new application. You have two main services, a web frontend and a database backend, that will run in different subnets. You need to ensure the web frontend can communicate with the database on port 3306, but no other traffic is allowed to the database from the web subnet. What should you do?",options:["Create a Cloud NAT gateway to allow traffic between the subnets.","Create a VPC firewall rule with a network tag for the database, allowing ingress on TCP:3306 from the web frontend's network tag.","Configure Private Google Access for the database subnet.","Establish a VPC Peering connection between the two subnets."],correct:1,explanation:"VPC firewall rules are the standard way to control traffic between subnets and instances within a VPC. Using network tags allows you to apply the rule specifically to the database VMs (target tag) and allow traffic only from the web VMs (source tag) on the specified port, enforcing the principle of least privilege.",wrongExplanations:{1:"Cloud NAT is for allowing instances without external IPs to access the internet; it does not control internal traffic between subnets.",2:"Private Google Access allows instances without external IPs to reach Google APIs; it does not control internal traffic between subnets.",3:"VPC Peering is for connecting two separate VPCs. Since the subnets are in the same VPC, peering is not necessary."}},{id:315,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"A web application running on a managed instance group (MIG) is experiencing sudden traffic spikes. You need to ensure the group scales up quickly to handle the load and scales down to save costs when the traffic subsides. The scaling decisions should be based on the number of active user connections. Which autoscaling policy should you configure?",options:["A queue-based scaling policy using a Cloud Monitoring metric from Pub/Sub.","A schedule-based autoscaling policy to increase capacity during peak hours.","CPU utilization, with a target utilization of 60%.","Cloud Load Balancing utilization, with a capacity setting based on requests per second."],correct:3,explanation:"For a web application behind a load balancer, the most effective scaling signal is the load balancer's own utilization metric, specifically requests per second (RPS). This directly measures the application's load and allows the MIG to scale based on actual user traffic, which is more accurate than indirect metrics like CPU.",wrongExplanations:{1:"CPU utilization can be a good signal, but it's indirect. A high number of connections might not always correlate with high CPU, especially if requests are I/O-bound. Scaling on RPS is more direct.",2:"Schedule-based scaling is only appropriate for predictable traffic patterns, not for handling sudden, unpredictable spikes.",3:"Queue-based scaling is designed for backend processing workloads that pull tasks from a queue like Pub/Sub, not for a user-facing web application."}},{id:316,domain:"Section 4: Configuring access and security",subdomain:"Encryption",question:"Your company has a strict compliance requirement that all data at rest in a Cloud Storage bucket must be encrypted with a key that your company manages directly. Google should not have access to the unencrypted keys. Which encryption option should you use for the bucket?",options:["Customer-Managed Encryption Keys (CMEK) with Cloud KMS","Customer-Supplied Encryption Keys (CSEK)","Google-managed encryption","Application-layer encryption"],correct:1,explanation:"CSEK requires you to provide your own encryption key with every request to Cloud Storage. Google only holds the key in memory temporarily to perform the operation and does not store it. This gives you full control and ensures Google cannot access the data without the key you supply, meeting the strict requirement.",wrongExplanations:{1:"With CMEK, you manage the key in Cloud KMS, but Google's services still interact with KMS on your behalf to encrypt/decrypt data. While you control the key's lifecycle, Google's systems are involved in the process, which doesn't meet the 'Google should not have access' criteria as strictly as CSEK.",2:"Google-managed encryption is the default, where Google fully manages the keys. This does not meet the customer-managed requirement.",3:"Application-layer encryption is a valid strategy but refers to encrypting data within your application before sending it to Google. The question asks about Cloud Storage encryption options, and CSEK is the direct answer within that context."}},{id:317,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"A large enterprise is structuring their Google Cloud environment. They have multiple departments (e.g., Finance, Engineering, Marketing), each with its own set of applications and projects. They need to apply distinct IAM policies and network configurations at the department level. What is the recommended way to structure this using the resource hierarchy?",options:["Create a single Project for the entire company and use VPCs to isolate department resources.","Create a Folder for each department under the Organization node, and place each department's Projects inside their respective Folder.","Use labels on Projects to identify which department they belong to and apply policies based on labels.","Create a separate Organization node for each department."],correct:1,explanation:"Folders are the intended mechanism in the resource hierarchy for grouping projects that share common requirements, such as those belonging to a single department. You can apply IAM policies and Organization Policies to a Folder, and they will be inherited by all Projects within it, making it the ideal tool for departmental isolation and governance.",wrongExplanations:{1:"An Organization node is typically tied to a single company domain (e.g., mycompany.com). Creating multiple organizations is not the standard or recommended practice for departmental separation.",2:"Labels are for metadata, cost tracking, and filtering, but they are not a primary mechanism for enforcing hierarchical policy inheritance. Folders are designed for this purpose.",3:"A single project for a large enterprise is not scalable and does not provide sufficient isolation for billing, quotas, or security boundaries. This would be an administrative nightmare."}},{id:318,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"BigQuery",question:"You are designing a large BigQuery table that will store daily sales transaction data and will be queried frequently by region and transaction date. To optimize query performance and reduce costs, how should you configure the table?",options:["Create a separate table for each day's transactions.","Cluster the table by both region and transaction date.","Use a wildcard query to select from multiple daily tables.","Partition the table by transaction date and cluster it by region."],correct:3,explanation:"Partitioning by transaction date will create daily partitions. When queries filter on this date (a common pattern), BigQuery only scans the relevant partitions, drastically reducing the amount of data processed and thus lowering costs and improving speed. Clustering by region will then sort the data within each partition by region, further improving performance for queries that filter on region.",wrongExplanations:{1:"Creating thousands of individual tables is difficult to manage and query. Partitioning is the BigQuery-native solution to this problem.",2:"You can only cluster on up to four columns, but partitioning is a separate and more powerful mechanism for date-based pruning. The combination of partitioning and clustering is optimal here.",3:"Wildcard queries are a way to deal with the anti-pattern of having many small tables. Using a partitioned table is the correct and more performant design."}},{id:319,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Trace",question:"Your microservices-based application is experiencing high latency on certain user requests, but you cannot pinpoint which service is the bottleneck. The request flows through multiple services (e.g., Frontend -> API Gateway -> Order Service -> Database). Which tool would be most effective for visualizing the entire request flow and identifying the latency contribution of each service?",options:["Cloud Trace","Cloud Logging","Cloud Monitoring","Cloud Profiler"],correct:0,explanation:"Cloud Trace is specifically designed for distributed tracing. It collects latency data from your applications, tracks how requests propagate through different services, and displays it in a near real-time waterfall graph. This allows you to immediately see which service call is taking the most time and causing the bottleneck.",wrongExplanations:{1:"Cloud Logging can show you logs from each service, but it's very difficult to manually correlate timestamps across multiple services to diagnose a latency issue. Trace automates this.",2:"Cloud Profiler is used to analyze the code-level performance (CPU, memory) *within a single service*. It can't show you the end-to-end latency across multiple services.",3:"Cloud Monitoring can show you high-level latency metrics for each service, but it doesn't provide the detailed, request-level trace needed to understand the entire flow and pinpoint the exact source of delay."}},{id:320,domain:"Section 4: Configuring access and security",subdomain:"VPC Security",question:"You have a set of Compute Engine instances in a private subnet that need to download software updates from the internet. However, company policy forbids these instances from having public IP addresses, so they cannot be directly reached from the internet. How can you enable this outbound internet access securely?",options:["Assign a public IP address to each instance temporarily.","Set up Identity-Aware Proxy (IAP) for the instances.","Configure a Cloud NAT gateway for the private subnet.","Create a firewall rule to allow all egress traffic from the instances."],correct:2,explanation:"Cloud NAT is a managed service that allows instances in a private subnet (without external IPs) to initiate outbound connections to the internet, but prevents unsolicited inbound connections. This is the standard and most secure solution for providing internet access for patching, updates, or API calls from private resources.",wrongExplanations:{1:"IAP is for controlling *inbound* SSH/RDP or TCP traffic to your instances based on user identity. It does not provide general-purpose outbound internet access.",2:"Assigning public IPs violates the core security requirement that the instances should not be directly reachable from the internet.",3:"An egress firewall rule only permits traffic; it does not solve the networking problem of how a private IP can be routed to the public internet. You still need a NAT or a public IP for that."}},{id:321,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You need to store monthly backups of a database in Cloud Storage. The data is critical and must be retained for seven years for compliance. It will be accessed very rarely, likely only in a disaster recovery scenario. Which Cloud Storage class offers the lowest storage cost for this use case?",options:["Archive","Coldline","Nearline","Standard"],correct:0,explanation:"Archive Storage is designed for long-term data archiving, backup, and disaster recovery. It offers the absolute lowest at-rest storage cost but has higher retrieval costs and longer retrieval times compared to other classes. This aligns perfectly with the requirement for long-term, rarely accessed data.",wrongExplanations:{1:"Coldline Storage is for data accessed at most once a year. While it's a good option for backups, Archive is even cheaper for data that will be stored for many years with minimal expected access.",2:"Nearline Storage is for data accessed at most once a month. This is too frequent and more expensive than Coldline or Archive for this long-term backup scenario.",3:"Standard Storage is for frequently accessed ('hot') data and has the highest storage cost, making it completely unsuitable for this use case."}},{id:322,domain:"Section 1: Setting up a cloud solution environment",subdomain:"API enablement",question:"A developer in your team tries to run a `gcloud compute instances create` command in a newly created project but receives a permission denied error, indicating that the 'Compute Engine API' has not been enabled. What is the most direct way to resolve this issue?",options:["Grant the developer the 'Project Owner' IAM role.","Enable the Compute Engine API in the Google Cloud Console under 'APIs & Services'.","Run `gcloud services enable compute.googleapis.com`.","Wait for 60 minutes for the API to be enabled automatically."],correct:2,explanation:"The `gcloud services enable` command is the command-line equivalent of enabling an API in the console. It directly targets and enables the specified API (`compute.googleapis.com`) for the project, which is the root cause of the error. Both this and enabling it via the Console are correct, but this is a common exam-style question testing gcloud knowledge.",wrongExplanations:{0:"This is also a correct way to solve the problem, but exam questions often favor the `gcloud` command-line solution when available.",1:"Changing IAM roles will not fix the issue. Even a Project Owner cannot use a service if its API is not enabled for the project.",3:"APIs are not enabled automatically. They must be explicitly enabled before their resources can be provisioned or managed."}},{id:323,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Error Reporting",question:"An application deployed on Cloud Run is experiencing intermittent crashes. You want to automatically capture, group, and get notified about these application exceptions without instrumenting your code with a new library. Which Google Cloud's operations suite tool should you use?",options:["Cloud Monitoring dashboards","Cloud Logging with a log-based metric","Cloud Debugger","Cloud Error Reporting"],correct:3,explanation:"Cloud Error Reporting is designed specifically for this purpose. It automatically collects and aggregates crash and exception data from services like Cloud Run, App Engine, and Cloud Functions. It intelligently groups similar errors and can send notifications, allowing you to quickly identify and address new issues.",wrongExplanations:{1:"You could try to parse stack traces in Cloud Logging and create alerts, but it's a complex, manual process. Error Reporting does this automatically and more effectively.",2:"Monitoring dashboards show metrics like request count or latency, but they don't provide details about application-level exceptions or stack traces.",3:"Cloud Debugger is for interactively inspecting the state of a running application at a specific line of code. It's for active debugging, not for automated crash reporting and aggregation."}},{id:324,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Load balancing",question:"You are deploying a global web application with users in North America, Europe, and Asia. You need to provide a single, global IP address for all users and route them to the closest healthy backend instance group to minimize latency. Which Google Cloud Load Balancer should you choose?",options:["Internal TCP/UDP Load Balancer","Regional external Application Load Balancer","Global external proxy Network Load Balancer","Global external Application Load Balancer"],correct:3,explanation:"The Global external Application Load Balancer (formerly known as HTTP(S) Load Balancer) is designed for this exact use case. It provides a single global Anycast IP address and uses Google's global network to route user traffic to the nearest backend with available capacity, ensuring the lowest possible latency and high availability.",wrongExplanations:{1:"A Regional load balancer only distributes traffic to backends within a single Google Cloud region. It cannot route traffic globally.",2:"A proxy Network Load Balancer is for non-HTTP(S) traffic like TCP/SSL. For a web application (HTTP/S), the Application Load Balancer is the appropriate choice as it operates at Layer 7.",3:"An Internal load balancer is for traffic *within* your VPC and cannot be used for public, internet-facing applications."}},{id:325,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"A third-party auditing firm needs temporary, read-only access to review the configurations of all resources in one of your projects for a compliance check. The access should automatically expire after two weeks. How should you grant this access according to Google Cloud best practices?",options:["Export all resource configurations to a Cloud Storage bucket and grant the auditor access to the bucket.","Create a new user account for the auditor, give it the Viewer role, and manually delete it after two weeks.","Give the auditor the Project Owner role so they don't encounter any permissions issues.","Grant the auditor's user account the Viewer role with an IAM Condition that expires after 14 days."],correct:3,explanation:"IAM Conditions allow you to grant temporary and conditional access to resources. You can add a time-based condition to a role binding that specifies a start and end date. This is the most secure and automated way to provide temporary access that is automatically revoked, following the principle of least privilege.",wrongExplanations:{1:"Manual deletion is error-prone. An administrator might forget to delete the account, leaving a security vulnerability. IAM Conditions automate the revocation.",2:"The Project Owner role is extremely permissive and violates the principle of least privilege. An auditor only needs read-only (Viewer) access.",3:"Exporting data creates a static, point-in-time copy. The auditor needs to review the live configurations. This approach is also cumbersome and less secure."}},{id:326,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Run and Cloud Functions",question:"You need to build a lightweight service that responds to file uploads in a Cloud Storage bucket. Whenever a new image is uploaded, the service must resize it and save a thumbnail to another bucket. The processing for each image is quick and independent. Which compute service is the most cost-effective and operationally efficient for this task?",options:["Cloud Functions","A GKE Autopilot cluster","Cloud Run","A Compute Engine VM"],correct:0,explanation:"Cloud Functions is an event-driven, serverless compute platform. It is designed to execute code in response to events, such as a file upload to Cloud Storage. Since the task is small, quick, and triggered by an event, Cloud Functions is the most efficient and cost-effective choice, as you only pay for the execution time and don't manage any infrastructure.",wrongExplanations:{1:"Cloud Run is also serverless, but it is designed for running containerized applications that typically serve HTTP requests. While it *can* be configured to respond to events, Cloud Functions is a more direct and simpler solution for this specific event-driven use case.",2:"Using GKE, even Autopilot, is overkill for this simple task. It would be more expensive and complex than necessary.",3:"A Compute Engine VM requires you to manage the operating system, patching, and scaling. It is the least operationally efficient and cost-effective choice for a simple, event-driven task."}},{id:327,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Backup strategies",question:"You are managing a critical Cloud SQL for MySQL database. You need to be able to restore the database to its state at any specific minute within the last 7 days. What features must you enable on the instance?",options:["Manual backups and binary logging","Read replicas in another zone","Automated backups and point-in-time recovery (binary logging)","Only automated backups"],correct:2,explanation:"Point-in-time recovery (PITR) is the feature that allows you to restore a database to a specific moment. To enable PITR in Cloud SQL, you must first have automated backups enabled, and you must also enable binary logging. The combination of the last full backup and the subsequent binary logs allows Cloud SQL to reconstruct the database to any point in time.",wrongExplanations:{1:"Automated backups alone only allow you to restore to the specific time the backup was taken (e.g., 4:00 AM), not to any minute in between.",2:"Manual backups are point-in-time snapshots, but they do not support the continuous logging needed for PITR. PITR must be used with automated backups.",3:"Read replicas are for high availability and read-scaling, not for backup and restore. They replicate the current state of the primary, not historical states."}},{id:328,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You frequently work with two different Google Cloud projects: `project-dev` and `project-prod`. You want to be able to switch the default project for your `gcloud` commands easily without typing the full `--project` flag every time. Which `gcloud` command should you use?",options:["gcloud projects set-default","gcloud auth login","gcloud init","gcloud config configurations create/activate"],correct:3,explanation:"`gcloud config configurations` allows you to create named configuration sets. You can create one for `dev` (with `project-dev` as the default project) and one for `prod`. Then, you can easily switch between them using `gcloud config configurations activate [NAME]`. This is the most robust way to manage settings for multiple distinct projects or accounts.",wrongExplanations:{1:"This command does not exist. The correct command is `gcloud config set project [PROJECT_ID]`.",2:"`gcloud init` is used for initializing or re-initializing gcloud settings, which includes setting up a configuration. However, `gcloud config configurations` is the specific command set for managing and switching between multiple existing configurations.",3:"`gcloud auth login` is for authenticating your user account with Google Cloud. It does not manage project settings."}},{id:329,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Hybrid connectivity",question:"Your company needs to establish a highly reliable, low-latency, high-bandwidth connection between your on-premises data center and your Google Cloud VPC. The connection will carry business-critical traffic and requires a 99.99% availability SLA. Which connectivity option should you choose?",options:["Partner Interconnect","Cloud VPN (HA)","Direct Peering","Dedicated Interconnect"],correct:3,explanation:"Dedicated Interconnect provides a direct, private physical connection between your on-premises network and Google's network. When configured for high availability (two connections in different locations), it can provide a 99.99% SLA. It offers the highest bandwidth and lowest latency, making it suitable for business-critical traffic.",wrongExplanations:{1:"Partner Interconnect also provides a private connection but through a service provider. While it can also offer a high SLA (up to 99.99%), Dedicated Interconnect is the option that provides a direct physical link solely for your company.",2:"Cloud VPN (HA) provides a secure connection over the public internet. It is a great option for many use cases, but it cannot match the low latency and high, consistent bandwidth of a dedicated physical connection. Its SLA is typically 99.9%.",3:"Direct Peering is for exchanging traffic between your network and Google's public-facing properties (like YouTube, Search), not for connecting to your private VPC resources."}},{id:330,domain:"Section 4: Configuring access and security",subdomain:"Organization policies",question:"To prevent accidental data exfiltration, your security team wants to ensure that all new Cloud Storage buckets are created with Uniform bucket-level access enabled and do not use legacy Access Control Lists (ACLs). How can you enforce this across the entire organization?",options:["Run a daily script to find and convert buckets that are not using uniform bucket-level access.","Set an Organization Policy with the `storage.uniformBucketLevelAccess` constraint enforced.","Educate all developers on the new policy and trust them to comply.","Create a custom IAM role that denies the `storage.buckets.setIamPolicy` permission for buckets without uniform access."],correct:1,explanation:"Organization Policies provide centralized, programmatic control over your organization's cloud resources. The `storage.uniformBucketLevelAccess` constraint is specifically designed to enforce the use of uniform bucket-level access, which simplifies permissions and disables ACLs. Applying this at the organization root is the most effective way to enforce the policy.",wrongExplanations:{1:"Managing this through IAM is complex and indirect. An Organization Policy is a direct, preventative control.",2:"A reactive script is not a preventative measure. It only fixes non-compliant resources after they have been created.",3:"Relying on education alone is not a sufficient security control for a critical policy. An automated enforcement mechanism is required."}},{id:331,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You are managing a GKE Standard cluster and need to upgrade the Kubernetes version on your node pools with minimal disruption to running applications. You want to upgrade nodes one by one, moving workloads to a new node before draining the old one. Which upgrade strategy should you use?",options:["Blue-green upgrades","In-place upgrades","Surge upgrades","Canary upgrades"],correct:2,explanation:"Surge upgrades are the default and recommended strategy for GKE node pools. This strategy creates a new node with the updated version (the 'surge' node) before draining an old node. Once the old node is drained and its workloads are rescheduled, it is deleted. This rolling process minimizes disruption and ensures capacity is maintained during the upgrade.",wrongExplanations:{1:"Blue-green upgrades involve creating an entire parallel node pool with the new version, shifting traffic over, and then deleting the old pool. While a valid strategy, surge upgrades are the built-in GKE mechanism for rolling node pool updates.",2:"Canary upgrades are a deployment strategy for applications, not for underlying node infrastructure.",3:"In-place upgrades are not a standard GKE feature. The standard process involves replacing old nodes with new ones."}},{id:332,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE configurations",question:"You are deploying a critical application in GKE that must remain available even if an entire Google Cloud zone fails. Which type of GKE cluster should you create?",options:["An Autopilot cluster","A regional cluster","A zonal cluster","A private cluster"],correct:1,explanation:"A regional cluster distributes its control plane and nodes across multiple zones within a single region. If one zone fails, the control plane remains available, and workloads can continue running in the other zones, providing high availability against zonal failures.",wrongExplanations:{1:"A zonal cluster has its control plane and all its nodes located within a single zone. If that zone fails, the entire cluster becomes unavailable.",2:"A private cluster is a security feature that gives nodes private IPs; it does not determine the cluster's availability across zones.",3:"An Autopilot cluster can be either zonal or regional. The choice of regional is what provides the high availability, not the Autopilot mode itself."}},{id:333,domain:"Section 4: Configuring access and security",subdomain:"Cloud Audit Logs",question:"A security auditor needs to see a log of all administrative changes made to your Google Cloud project, such as creating a VM or modifying an IAM policy. They also need to know which user performed each action. Which log type in Cloud Audit Logs should you provide?",options:["Admin Activity audit logs","Policy Denied audit logs","System Event audit logs","Data Access audit logs"],correct:0,explanation:"Admin Activity audit logs record all API calls and other actions that modify the configuration or metadata of resources. They are enabled by default and cannot be disabled. They include the 'who, what, when, and where' for all administrative actions, which is exactly what the auditor requires.",wrongExplanations:{1:"Data Access audit logs record who accessed data (e.g., reading a file from a bucket). They are high-volume and disabled by default. The auditor is asking for administrative changes, not data access.",2:"System Event audit logs record actions taken by Google Cloud systems, not by user accounts.",3:"Policy Denied audit logs record when a user or service was denied access because of a security policy. While useful, they don't show the successful administrative changes."}},{id:334,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Quotas",question:"You are about to launch a new application that will require 200 vCPUs in a specific region. When you try to provision the VMs, you receive an error that you have exceeded your quota. The project is new, and this is a legitimate business need. What should you do?",options:["Create a new project, as it will have a fresh set of default quotas.","Use smaller machine types to reduce the total vCPU count.","Request a quota increase for the vCPU quota in that region from the IAM & Admin Quotas page.","Spread the VMs across multiple regions to stay within the default quota for each."],correct:2,explanation:"Quotas are limits on the amount of a particular resource you can use. They are a safety mechanism. When you have a valid need to exceed a default quota, the standard procedure is to navigate to the Quotas page in the console and submit a request to increase the specific quota for your project.",wrongExplanations:{1:"Creating a new project will not solve the problem, as it will have the same low default quota. The limit is per project.",2:"Spreading VMs across regions might be a temporary workaround, but it adds network latency and complexity. The correct solution is to get the necessary quota in the region where you need the resources.",3:"Using smaller machine types might compromise the performance of your application. If you need 200 vCPUs, you should request the appropriate quota rather than downgrading your architecture."}},{id:335,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Pub/Sub",question:"You are designing an application where a frontend service needs to send tasks to multiple, independent backend services for processing. Each task must be processed by exactly one of the backend services. The system needs to be highly scalable and decoupled. Which service should you use to facilitate this communication?",options:["Cloud Pub/Sub","Cloud Memorystore","Cloud Load Balancing","Cloud Tasks"],correct:0,explanation:"Cloud Pub/Sub is a fully-managed, real-time messaging service that allows you to send and receive messages between independent applications. By having the frontend publish a message to a topic and having the backend services subscribe to that topic, you create a decoupled, many-to-many communication system that is highly scalable.",wrongExplanations:{1:"Cloud Tasks is designed for asynchronous task execution, but it's more focused on guaranteed delivery and rate controls for specific webhook targets. Pub/Sub is better for fanning out messages to multiple, independent subscriber groups.",2:"Cloud Load Balancing is for distributing user-facing traffic to a set of identical backends; it is not a messaging service for backend decoupling.",3:"Cloud Memorystore is an in-memory database service (Redis/Memcached); it is not a messaging service."}},{id:336,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Serial console access",question:"A Linux Compute Engine instance has failed to boot correctly, and you cannot connect to it using SSH because the networking service has not started. You need to interact with the instance to diagnose the boot issue. Which method should you use to gain access?",options:["Check the instance's logs in Cloud Logging.","Mount the instance's boot disk to another VM.","View the instance's screenshot.","Connect to the serial console of the instance."],correct:3,explanation:"The serial console provides direct, low-level access to a VM instance, independent of its network state. It allows you to interact with the bootloader and operating system as if you had a physical monitor and keyboard connected, which is essential for diagnosing boot-time failures before SSH is available.",wrongExplanations:{1:"Viewing the screenshot can show you where the boot process is stuck, but it does not provide an interactive console to run diagnostic commands.",2:"Mounting the disk to another VM is a valid but more complex and disruptive troubleshooting step. Accessing the serial console is the first and most direct method.",3:"While logs might contain useful information, they may not capture the specific bootloader errors, and they do not provide an interactive way to fix the problem."}},{id:337,domain:"Section 4: Configuring access and security",subdomain:"Identity-Aware Proxy (IAP)",question:"You have an internal web application running on a Compute Engine instance. You want to allow employees to access it from the internet without using a VPN, but you must ensure that only authenticated users from your company's domain can reach it. What is the most secure and managed way to achieve this?",options:["Use Identity-Aware Proxy (IAP) with the external Application Load Balancer.","Restrict access to the application using a firewall rule that allows traffic only from your corporate IP range.","Assign a public IP to the instance and configure application-level authentication.","Install an SSH bastion host and have users tunnel to the application."],correct:0,explanation:"IAP is a service that uses user identity and context to guard access to applications. When placed in front of an application via a load balancer, it intercepts all requests and requires users to authenticate with their Google identity (e.g., their corporate Google Workspace account). This allows you to enforce fine-grained access policies without a VPN, following a zero-trust security model.",wrongExplanations:{1:"Restricting by IP is not a complete solution. It doesn't work for remote employees not on the corporate network, and it doesn't verify individual user identity.",2:"An SSH bastion host is a common pattern but is less user-friendly and harder to manage for web applications than the seamless experience provided by IAP.",3:"Exposing the instance directly with a public IP and relying solely on application-level authentication is less secure. IAP provides a robust, managed authentication and authorization layer before traffic even reaches your application."}},{id:338,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Dataflow",question:"Your company collects a continuous stream of sensor data into a Pub/Sub topic. You need to process this data in real-time: filter out invalid readings, enrich the data by calling an external API, and write the results to a BigQuery table for analysis. The solution must scale automatically based on the volume of incoming data. Which service is best suited for this task?",options:["Cloud Functions","Cloud Dataflow","A script running on a Compute Engine VM","BigQuery scheduled queries"],correct:1,explanation:"Cloud Dataflow is a fully managed service for stream and batch data processing. It is built on Apache Beam and is designed for complex data processing pipelines (ETL) like this one. It can read from Pub/Sub, perform transformations like filtering and enrichment, and write to BigQuery, all while scaling its worker resources automatically.",wrongExplanations:{1:"A script on a VM is not a managed or auto-scaling solution. You would be responsible for all reliability and scalability aspects.",2:"Cloud Functions could be used for simple transformations, but for a multi-stage pipeline with external API calls and potential for large volume, Dataflow provides a more robust and scalable framework.",3:"BigQuery scheduled queries are for running SQL against data already in BigQuery on a schedule (batch processing), not for real-time stream processing from Pub/Sub."}},{id:339,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Profiler",question:"An application running on GKE is consuming an unexpectedly high amount of CPU, and you need to determine which specific functions within the code are responsible for the high usage. The application is written in Go. Which tool should you use to analyze the code's performance in production with low overhead?",options:["Cloud Trace","Cloud Debugger","Cloud Profiler","Cloud Monitoring"],correct:2,explanation:"Cloud Profiler is a low-overhead production profiler that continuously analyzes CPU usage and memory allocation of your application's code. It presents the data in a flame graph, allowing you to quickly identify the most resource-intensive functions and code paths, which is exactly what is needed to diagnose this issue.",wrongExplanations:{1:"Cloud Trace is for analyzing request latency across distributed systems, not for analyzing CPU usage within a single application's code.",2:"Cloud Monitoring tracks high-level metrics like overall CPU usage for a container or node, but it cannot tell you which specific function inside your code is responsible.",3:"Cloud Debugger allows you to inspect application state without stopping it, but it is not a performance analysis tool for identifying CPU-intensive functions."}},{id:340,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Config Connector",question:"A platform engineering team manages all their application deployments on GKE using a GitOps workflow with declarative YAML files. They want to extend this workflow to manage Google Cloud infrastructure, such as Cloud SQL instances and IAM policy bindings, using the same tools (`kubectl`, Git). Which Google Cloud component should they install in their GKE cluster to enable this?",options:["Terraform","Cloud Deployment Manager","Config Connector","Cloud Build"],correct:2,explanation:"Config Connector is a Kubernetes addon that allows you to manage Google Cloud resources through Kubernetes-style resource definitions (CRDs). Once installed, you can create, update, and delete GCP resources like `SQLInstance` or `IAMPolicyMember` by applying YAML manifests to your cluster, integrating seamlessly with existing GitOps and Kubernetes tooling.",wrongExplanations:{1:"Terraform is an excellent infrastructure-as-code tool, but it is a separate binary and workflow from `kubectl`. Config Connector is the solution that integrates directly into the Kubernetes control plane.",2:"Cloud Deployment Manager is Google's native infrastructure deployment service, but it uses its own template format and is not integrated with the Kubernetes resource model.",3:"Cloud Build is a CI/CD service used for building and testing code, not for declaratively managing infrastructure from within a Kubernetes cluster."}},{id:341,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"The Database Center dashboard shows a high 'CPU utilization' warning for a specific Cloud SQL instance. What is the most direct next step you can take from within the Google Cloud Console to diagnose which specific queries are causing the high CPU load?",options:["SSH into a VM and connect to the database to run `SHOW PROCESSLIST`.","Navigate to Cloud Monitoring and build a custom dashboard for the instance.","Click on the instance in Database Center to navigate directly to Query Insights.","Check the Admin Activity logs for the instance."],correct:2,explanation:"Database Center is designed as a centralized launchpad for database observability. It integrates tightly with other tools, and a primary workflow is to identify a problem (like high CPU) in the fleet-level view and then drill down into the specific instance's Query Insights page for detailed, query-level performance analysis.",wrongExplanations:{1:"While Cloud Monitoring has CPU metrics, it won't show you the individual queries. Query Insights is the specific tool for that level of detail.",2:"Manually connecting and running commands is an older, less efficient method of diagnosis. Query Insights provides historical data and a much richer user interface for identifying problematic queries.",3:"Admin Activity logs track administrative changes (e.g., changing instance settings), not query performance or CPU load."}},{id:342,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"You are new to Google Cloud and need to set up a two-tier web application with a load-balanced managed instance group and a Cloud SQL database. You understand the high-level architecture but are unsure of the specific steps and `gcloud` commands required. How can you use Gemini Cloud Assist to help you?",options:["Open Cloud Shell and wait for Gemini to suggest commands automatically.","Use Gemini to write a Terraform script for the entire architecture.","Ask Gemini in the console sidebar: 'Show me the steps to create a load balancer with a MIG and a Cloud SQL backend'.","Ask Gemini to troubleshoot why your application code is not working."],correct:2,explanation:"Gemini Cloud Assist excels at providing architectural guidance and generating the necessary commands. By describing your goal in natural language, Gemini can provide a step-by-step plan and the corresponding `gcloud` commands or code snippets to implement the desired architecture, significantly accelerating the learning and implementation process.",wrongExplanations:{1:"While Gemini can help write Terraform code, its primary integration in the console is for generating explanations and CLI commands, which is more direct for a beginner.",2:"Gemini does not automatically suggest commands in Cloud Shell without being prompted. You must actively ask it for help.",3:"Gemini assists with cloud infrastructure and operations. While it might offer general coding advice, it is not designed for debugging application-specific logic."}},{id:343,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud NGFW",question:"Your organization is migrating from traditional VPC firewall rules to network firewall policies using Cloud NGFW. You need to create a rule that blocks all outbound traffic to the internet (0.0.0.0/0) on TCP port 25 (SMTP) for all VMs in a project to prevent spam, but still allow all other internal VPC traffic. Where should this rule be configured?",options:["In a hierarchical firewall policy at the folder level.","As a VPC firewall rule with a priority of 65535.","In Cloud Armor as a security policy.","In a network firewall policy with a low priority (higher number) that applies to the project's VPC."],correct:3,explanation:"Network firewall policies are the next generation of VPC firewalls and are the correct place for this type of rule. A deny rule for a specific port to an external IP range should be given a lower priority (a higher number, e.g., 1000 or more) so that it doesn't override more specific allow rules for internal traffic, which would have a higher priority (lower number).",wrongExplanations:{1:"While a hierarchical policy could enforce this, network firewall policies are designed for VPC-specific rules. Hierarchical policies are better for broader, organization-wide guardrails.",2:"Legacy VPC firewall rules are being superseded by network firewall policies. While you could create this rule, the question implies a migration to the newer Cloud NGFW feature set.",3:"Cloud Armor is a Web Application Firewall (WAF) that protects against application-layer attacks (like SQL injection) on services behind an external load balancer. It does not control general egress traffic from VMs."}},{id:344,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"Query Insights shows that a specific query is consuming a large amount of CPU time and has a high 'I/O Wait' time. The query plan indicates a full table scan is being performed on a very large table. What is the most likely solution to improve this query's performance?",options:["Migrate the database to BigQuery.","Increase the vCPU count of the Cloud SQL instance.","Rewrite the query to use a different `JOIN` type.","Add an appropriate database index on the columns used in the `WHERE` clause of the query."],correct:3,explanation:"A full table scan combined with high I/O wait is a classic indicator of a missing index. Adding an index allows the database to seek directly to the relevant rows instead of reading the entire table from disk, which drastically reduces I/O and CPU consumption for the query.",wrongExplanations:{1:"Increasing the vCPU count (vertical scaling) might provide temporary relief but it does not fix the root cause of the inefficient query. The query will still perform a full table scan, just faster, and it's a very expensive solution.",2:"Migrating to BigQuery is a massive architectural change. BigQuery is an analytical data warehouse, not a transactional database, and this would not be an appropriate or simple solution.",3:"Changing the `JOIN` type would only be relevant if the query involved multiple tables. The core problem identified is a full table scan, which is addressed by indexing."}},{id:345,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"Your company stores highly sensitive data in a BigQuery dataset. To prevent data exfiltration, you must ensure that this data can only be accessed by specific authorized VMs running within your VPC, and that the BigQuery API cannot be called from the public internet. Which security control should you implement?",options:["IAM Conditions based on IP address","A VPC firewall rule to deny all egress traffic","An organization policy to restrict public IPs","VPC Service Controls"],correct:3,explanation:"VPC Service Controls create a service perimeter around Google-managed services like BigQuery and Cloud Storage. This perimeter blocks access from outside the perimeter (e.g., the public internet) and prevents data from being exfiltrated to a location outside the perimeter, even by an identity with valid IAM permissions. It is the definitive tool for this use case.",wrongExplanations:{1:"Restricting public IPs on VMs prevents them from being reached from the internet, but it does not prevent a malicious actor on a VM from exfiltrating data by calling the public BigQuery API endpoint.",2:"IAM Conditions can restrict access by IP, but this is less secure and harder to manage than a service perimeter, as it doesn't control where the data can be sent.",3:"Denying all egress traffic from your VPC would break many legitimate functionalities, including the ability for VMs to call necessary Google APIs. VPC Service Controls provide a much more granular and effective boundary."}},{id:346,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Identity",question:"Your company uses Google Workspace for email and collaboration. You want to use the same user identities and groups to manage IAM permissions in Google Cloud. What is the most straightforward way to achieve this?",options:["Export all users from Google Workspace and import them into a new Cloud Identity domain.","Set up federation with your on-premises Active Directory to sync users to Google Cloud.","Create a new service account for each user in Google Workspace.","Use your existing Google Workspace account as your Cloud Identity source. The users and groups will be available automatically."],correct:3,explanation:"When you have a Google Workspace account, it can act as your Cloud Identity provider by default. This means all your existing users and groups are automatically available in Google Cloud for you to assign IAM roles to, providing a seamless identity management experience.",wrongExplanations:{1:"Exporting and importing users is an unnecessary and error-prone manual step. The integration is designed to be automatic.",2:"Federating with Active Directory would be the solution if your primary identity source was on-premises AD, not Google Workspace.",3:"Service accounts are for applications, not for human users. This approach is incorrect and unmanageable."}},{id:347,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You are hosting a static website on Cloud Storage. You have enabled versioning on the bucket to protect against accidental deletions. A user accidentally deletes the `index.html` file. How can you restore the most recent version of the file?",options:["Enable object holds on the bucket and then try to undelete the file.","Restore the entire bucket from the previous day's backup.","The file is permanently gone because it was deleted.","Show noncurrent versions in the Cloud Console, find the previous version of `index.html`, and restore it."],correct:3,explanation:"When object versioning is enabled, deleting an object simply creates a special 'delete marker' and makes the live version noncurrent. It is not actually deleted. You can view these noncurrent versions and choose to restore any of them, which makes it the new live version.",wrongExplanations:{1:"This is incorrect. The purpose of versioning is to prevent permanent data loss from deletions or overwrites.",2:"Cloud Storage does not have automatic daily backups. You would need to configure this yourself. Versioning is the direct feature for this use case.",3:"Object holds are a compliance feature to prevent deletion. They must be set *before* the deletion attempt and would not help recover an already deleted object."}},{id:348,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Storage",question:"You have a Cloud Storage bucket with a lifecycle policy that moves objects to Nearline storage after 30 days and deletes them after 365 days. You have a specific object, `legal-document.pdf`, that must be preserved indefinitely for legal reasons. How can you prevent this one object from being deleted by the lifecycle policy?",options:["Move the object to Archive storage class.","Create a new lifecycle rule that excludes `legal-document.pdf`.","Place an object hold on `legal-document.pdf`.","Copy the object to a different bucket without a lifecycle policy."],correct:2,explanation:"An object hold is a feature that prevents an object version from being deleted or overwritten, regardless of any bucket lifecycle policies. This is the intended mechanism for protecting individual objects for legal or compliance reasons.",wrongExplanations:{1:"Lifecycle rules can be based on prefixes, age, or storage class, but not on individual object names. It's not feasible to exclude a single file this way.",2:"Moving the object to Archive would still subject it to the 365-day deletion rule. The lifecycle policy applies to all objects in the bucket regardless of class.",3:"While copying the object would work, it's a manual workaround. The object hold is the built-in, correct feature to use within the same bucket."}},{id:349,domain:"Section 4: Configuring access and security",subdomain:"Security best practices",question:"You are reviewing your company's Google Cloud project and discover several developers have created and downloaded service account keys to their local machines. According to Google Cloud security best practices, what should you advise them to do instead?",options:["Advise them to rotate their service account keys every 90 days.","Advise them to use short-lived credentials by impersonating a service account with their user credentials.","Advise them to store the keys in a private Git repository.","Advise them to encrypt the keys on their local disk."],correct:1,explanation:"Long-lived static service account keys are a significant security risk. The best practice is to avoid them entirely. Service account impersonation allows a user with sufficient permissions to generate short-lived credentials for a service account, which automatically expire. This eliminates the risk of a leaked key.",wrongExplanations:{1:"Storing any credentials, even encrypted, in a source code repository is a major security anti-pattern.",2:"While key rotation is better than not rotating, the best practice is to eliminate the long-lived keys altogether.",3:"Encrypting the keys is a good practice, but it doesn't mitigate the risk if the decryption key is also compromised or if the key is accidentally exposed while decrypted in memory."}},{id:350,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"Your company wants to ensure that all new projects created within the organization automatically have a default set of essential APIs enabled, such as Compute Engine, Cloud Storage, and BigQuery. How can you automate this?",options:["Create a project template in Deployment Manager.","Configure an Organization Policy to enforce API enablement.","There is no direct way to automate this; it must be done manually or with a custom script after project creation.","Use the `gcloud projects create` command with a flag to enable APIs."],correct:2,explanation:"Currently, Google Cloud's Organization Policies and resource hierarchy do not have a built-in feature to automatically enable a specific set of APIs upon project creation. This process must be orchestrated using other tools, typically a script that runs after a project is created, often triggered by a log entry about the project creation event.",wrongExplanations:{1:"The `gcloud projects create` command does not have a flag to enable multiple APIs during the creation process.",2:"Organization Policies can be used to *restrict* API usage (e.g., deny access to a certain service), but not to *enable* them by default.",3:"Deployment Manager is for deploying resources *within* a project, not for configuring the project itself during creation."}},{id:351,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to run a legacy monolithic application on a Compute Engine VM. The application is single-threaded and performs best with the highest possible CPU clock speed. Which machine series should you choose to get the best performance?",options:["M2 (Memory-optimized)","E2 (Cost-optimized)","N2 (General-purpose)","C3 or C3D (Performance-optimized)"],correct:3,explanation:"The C-series (Compute-optimized) instances are designed for compute-intensive workloads and typically offer the highest per-core performance and clock speeds. For a single-threaded application that is CPU-bound, a compute-optimized instance will provide the best results.",wrongExplanations:{1:"E2 instances are cost-optimized and do not offer the highest performance. They are suitable for general-purpose workloads where cost is the primary concern.",2:"N2 instances are general-purpose and offer a good balance of price and performance, but the C-series is specifically optimized for maximum CPU performance.",3:"M2 instances are memory-optimized and provide a very high ratio of memory to vCPU. This would be wasteful and not cost-effective for a CPU-bound application."}},{id:352,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You want to create a single chart in a Cloud Monitoring dashboard that shows the average CPU utilization of all VMs in your project that have a specific label, for example, `app: my-critical-app`. How can you achieve this?",options:["Create a chart, select the CPU utilization metric, and then filter by the label `app: my-critical-app`.","This is not possible; you can only filter by instance name or group.","Write a custom script using the Monitoring API to generate the chart data.","Create a separate chart for each VM and arrange them on the dashboard."],correct:0,explanation:"Cloud Monitoring fully supports filtering and grouping metrics by resource labels. This is a core feature that allows you to create aggregated views of your resources. You can easily add a widget, select the metric, and apply a filter based on any label you have assigned to your VMs.",wrongExplanations:{1:"This is inefficient and doesn't provide the single aggregated view that was requested.",2:"This is overly complex. The functionality is built directly into the Monitoring UI.",3:"This is incorrect. Filtering by labels is a standard and powerful feature of Cloud Monitoring."}},{id:353,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You have a group of junior developers who need to deploy applications to a GKE cluster. They should be able to manage Deployments, Pods, and Services, but should not be able to view or modify the cluster's configuration itself (e.g., change the node pool size). Which role should you grant them?",options:["Kubernetes Engine Developer (`roles/container.developer`)","Compute Viewer (`roles/compute.viewer`)","Kubernetes Engine Admin (`roles/container.admin`)","Project Editor (`roles/editor`)"],correct:0,explanation:"The Kubernetes Engine Developer role provides the necessary permissions to interact with Kubernetes objects within a cluster (like Pods and Deployments) via `kubectl`, but it does not grant permissions to modify the underlying GKE cluster infrastructure. This adheres to the principle of least privilege.",wrongExplanations:{1:"The Kubernetes Engine Admin role is too permissive; it grants full control over the GKE cluster, including the ability to modify its configuration and delete it.",2:"The Project Editor role is far too broad. It grants permissions to modify almost all resources in the project, not just the Kubernetes objects.",3:"The Compute Viewer role is too restrictive. It only allows viewing of Compute Engine resources and does not grant any permissions to interact with GKE."}},{id:354,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud DNS",question:"You have a domain `example.com` registered with a third-party registrar. You want to manage its DNS records using Cloud DNS. What are the high-level steps you need to take?",options:["Create a managed public zone in Cloud DNS for `example.com`, then update the name server (NS) records at your registrar to point to the Cloud DNS name servers.","Create a managed private zone in Cloud DNS for `example.com`.","Transfer the domain registration from your current registrar to Google Domains.","Point your domain's A record at your registrar to a Google Cloud IP address."],correct:0,explanation:"To delegate DNS management to Cloud DNS, you first create a zone for your domain in the Cloud DNS service. Cloud DNS will then assign a set of authoritative name servers for that zone. The final step is to go to your domain registrar and replace the existing name server records with the ones provided by Cloud DNS. This tells the internet to query Cloud DNS for records related to `example.com`.",wrongExplanations:{1:"While you can transfer the domain to Google Domains, it is not a requirement for using Cloud DNS. You can use any registrar.",2:"A private zone is for resolving names *within* your VPCs and is not visible to the public internet. For a public website, you need a public zone.",3:"Simply pointing an A record does not delegate the entire management of the domain's DNS records to Cloud DNS."}},{id:355,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"A startup script on a Linux VM is failing intermittently. You need to view the full output of the startup script to diagnose the problem. Where can you find this output?",options:["In the VM's metadata server.","In the Admin Activity audit log.","In the instance's serial port 1 output.","In the `/var/log/syslog` file on the instance."],correct:2,explanation:"By default, the output of startup scripts (both stdout and stderr) is written to the instance's serial port console (specifically port 1). You can view this output in the Google Cloud Console or by using the `gcloud compute get-serial-port-output` command. This is the most reliable place to look for startup script logs.",wrongExplanations:{1:"Audit logs track API calls and administrative actions, not the output of scripts running inside a VM.",2:"The metadata server provides information *to* the VM; it does not store output *from* the VM.",3:"While the script's output *might* also be directed to syslog depending on the OS configuration, the serial port is the standard, guaranteed location provided by the Compute Engine platform."}},{id:356,domain:"Section 4: Configuring access and security",subdomain:"Cloud Armor",question:"Your e-commerce website, which is behind a global external Application Load Balancer, is experiencing a DDoS attack in the form of a large volume of HTTP requests from a specific country. How can you quickly mitigate this attack?",options:["Create a Cloud Armor security policy with a rule to deny traffic from that specific country and attach it to the load balancer's backend service.","Scale up your managed instance group to handle the extra traffic.","Use Identity-Aware Proxy to block the country.","Add a VPC firewall rule to deny ingress traffic from the country's IP ranges."],correct:0,explanation:"Cloud Armor is Google's DDoS protection and Web Application Firewall (WAF) service. It integrates with the global external Application Load Balancer and allows you to create rules to allow or deny traffic based on various attributes, including IP addresses, regions, and request headers. A geo-based block rule is a standard and effective way to mitigate this type of attack.",wrongExplanations:{1:"VPC firewall rules operate at the VM instance level. They cannot be attached to a global load balancer and would not be an effective way to stop a large-scale DDoS attack at the edge of Google's network.",2:"IAP is for user identity-based authentication, not for blocking large-scale, anonymous DDoS traffic.",3:"Scaling up might keep your site online temporarily, but it doesn't stop the attack and will result in extremely high costs. The best practice is to block the malicious traffic at the edge."}},{id:357,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"You are building a mobile gaming application with a global user base. You need a database that can handle high volumes of reads and writes, provide strong consistency, and offer low-latency access for users in multiple continents. The database must scale horizontally. Which Google Cloud database is the best choice?",options:["Firestore","Cloud SQL","Cloud Spanner","BigQuery"],correct:2,explanation:"Cloud Spanner is a globally distributed, strongly consistent, and horizontally scalable relational database. It is uniquely designed for use cases that require both global scale and strong transactional consistency, making it the perfect fit for a high-traffic global application like a mobile game.",wrongExplanations:{1:"Cloud SQL is a regional database. While you can use read replicas in other regions, it does not provide the same low-latency writes and horizontal scalability on a global scale as Spanner.",2:"Firestore is a NoSQL document database. While it offers multi-region support, Spanner is the better choice when strong, relational consistency is required.",3:"BigQuery is an analytical data warehouse, not a transactional database suitable for powering a live application backend."}},{id:358,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You need to get a detailed description of a specific Compute Engine instance named `web-server-1` in the zone `us-central1-a`, including its network interfaces and attached disks. You want the output to be in YAML format for easy parsing by a script. Which command should you use?",options:["gcloud compute instances export web-server-1 --destination=config.yaml","gcloud compute instances describe web-server-1 --zone us-central1-a --format=yaml",'gcloud compute instances list --filter="name=web-server-1" --format=yaml',"gcloud compute instances get-details web-server-1 --zone us-central1-a"],correct:1,explanation:"The `gcloud compute instances describe` command is used to get detailed information about a single resource. The `--format` flag is a global `gcloud` flag that allows you to specify the output format, with `yaml`, `json`, and `text` being common options. This combination directly fulfills the request.",wrongExplanations:{1:"The `list` command is for listing multiple resources. While you could filter it down to one, `describe` is the more direct command for a single resource. Also, the output of `list` is a list containing one item, whereas `describe` returns just the item itself.",2:"`get-details` is not a valid `gcloud compute instances` subcommand.",3:"`export` is not a valid `gcloud compute instances` subcommand for this purpose."}},{id:359,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You need to find all log entries with a `severity` of `ERROR` that occurred in your production project within the last 24 hours. How would you write this query in the Logs Explorer?",options:["severity=ERROR","filter by severity ERROR","level:error","SELECT * WHERE severity = 'ERROR'"],correct:0,explanation:"The Cloud Logging query language uses a simple `[FIELD_NAME]=[VALUE]` syntax for basic equality filters. To filter for error-level severity, you would simply type `severity=ERROR` into the query bar. The time range can be selected using the time range picker in the UI.",wrongExplanations:{1:"This syntax is not correct. The field name is `severity`.",2:"While the query language is powerful, it does not use SQL syntax like `SELECT *`.",3:"This is a natural language description, not a valid query for the query language."}},{id:360,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Config Connector",question:"After installing Config Connector in your GKE cluster, you create a YAML manifest for a `PubSubTopic` resource. What is the next step to actually create the Pub/Sub topic in your Google Cloud project?",options:["Run `gcloud pubsub topics create` with the YAML file as an argument.","Check the manifest into a Git repository connected to Cloud Build.","Apply the YAML manifest to your GKE cluster using `kubectl apply -f [filename]`. ","Submit the manifest to Cloud Deployment Manager."],correct:2,explanation:"Config Connector works by extending the Kubernetes API. You manage Google Cloud resources just like you manage native Kubernetes objects like Pods or Deployments. The standard workflow is to define the resource in a YAML file and then use `kubectl apply` to submit it to the Kubernetes API server. Config Connector's controllers will then see this new resource and provision the actual Pub/Sub topic in GCP.",wrongExplanations:{1:"`gcloud` commands are separate from the Config Connector workflow, which is based entirely on the Kubernetes API (`kubectl`).",2:"Cloud Deployment Manager is a different Infrastructure-as-Code tool and does not understand Kubernetes-style manifests.",3:"While checking the manifest into Git is part of a GitOps workflow, the action that directly creates the resource is the `kubectl apply`."}},{id:361,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"You are viewing the logs for a crashing Cloud Run service in the Logs Explorer and see a complex Java stack trace. You are not a Java expert. How can Gemini Cloud Assist help you understand the problem?",options:["Ask Gemini to rewrite the Cloud Run service in a different language.","Gemini cannot help with application-level errors.","Ask Gemini to decompile the Java bytecode.","Click the Gemini icon next to the log entry to get a natural language explanation of the error."],correct:3,explanation:"A key feature of Gemini Cloud Assist is its integration into services like Cloud Logging. It can analyze complex log entries, such as stack traces, and provide a concise, natural language summary of what the error means, what might have caused it, and suggest potential fixes. This is extremely valuable for troubleshooting errors in unfamiliar code or platforms.",wrongExplanations:{1:"Rewriting the service is a massive step and not a troubleshooting action. Gemini's purpose here is to explain the current error.",2:"Decompiling bytecode is not a relevant or helpful action for understanding a stack trace.",3:"This is incorrect. Explaining application errors found in logs is a core use case for Gemini Cloud Assist."}},{id:362,domain:"Section 4: Configuring access and security",subdomain:"Cloud NGFW policies",question:"What is a key advantage of using secure tags with network firewall policies over traditional network tags with VPC firewall rules?",options:["Secure tags are managed with IAM permissions, allowing for delegated security administration.","Secure tags are automatically applied to all VMs in a project.","Secure tags can be applied to any resource, including Cloud Storage buckets.","Secure tags work across different VPCs without needing VPC Peering."],correct:0,explanation:"A major difference is the governance model. Traditional network tags can be added to a VM by anyone with the `compute.instances.setTags` permission (part of Compute Instance Admin). Secure tags, however, are separate resources, and the ability to attach a specific tag to a resource is controlled by a distinct IAM permission (`tags.tagBindings.create`). This allows a central security team to control who can apply which tags, enabling a more secure, delegated administration model.",wrongExplanations:{1:"Secure tags can only be applied to Compute Engine VMs at this time.",2:"Network firewall policies, like VPC firewall rules, apply only within a single VPC network.",3:"Secure tags must be manually or programmatically applied to resources; they are not automatic."}},{id:363,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"From the main Database Center dashboard, which of the following insights can you see at a glance for your entire database fleet?",options:["The total number of databases with high CPU utilization, and a list of databases with available security recommendations.","The exact cost incurred by each database instance in the last hour.","A button to failover all high-availability database instances simultaneously.","A real-time stream of all queries being executed across all databases."],correct:0,explanation:"Database Center is designed to provide a high-level, fleet-wide overview. It aggregates key performance metrics (like identifying which instances have high CPU) and security posture information (like highlighting instances that have security recommendations from services like Security Command Center) into a single dashboard.",wrongExplanations:{1:"Database Center provides aggregated metrics, not a real-time firehose of every query. You would use Query Insights for detailed query analysis on a per-instance basis.",2:"Cost information is found in Cloud Billing, not directly in the Database Center dashboard.",3:"Database Center is an observability tool, not a control plane for performing operational actions like failovers across the entire fleet."}},{id:364,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE Autopilot",question:"A team is deploying a standard web application on a GKE Autopilot cluster. They have provided a Deployment YAML file with resource requests (CPU and memory) for their pods. What is GKE Autopilot responsible for?",options:["Provisioning, managing, and scaling the underlying nodes to meet the pod's resource requests.","Creating the DNS records for the application's service.","Automatically determining the correct CPU and memory requests for the pods.","Ensuring the application code inside the container has no bugs."],correct:0,explanation:"The core value proposition of GKE Autopilot is that it abstracts away node management. You declare the resources your application pods need, and Autopilot handles the rest of the infrastructure lifecycle: creating appropriately sized nodes, adding new nodes as you scale up, removing nodes as you scale down, and performing node upgrades.",wrongExplanations:{1:"You are still responsible for specifying resource requests for your pods. Autopilot uses this information to provision the correct infrastructure. It does not determine the requests for you.",2:"GKE is a container orchestrator; it is not responsible for the quality of the application code itself.",3:"While GKE can create internal DNS records for service discovery, you are responsible for creating public-facing DNS records."}},{id:365,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"You have configured a billing export to a BigQuery dataset. What can you do with this data?",options:["Write detailed SQL queries to analyze costs by label, SKU, and project.","Receive real-time alerts within milliseconds of a cost being incurred.","Use it to provision new resources directly from BigQuery.","Modify past billing records to correct for accidental spending."],correct:0,explanation:"Exporting detailed billing data to BigQuery unlocks powerful, granular cost analysis capabilities. You can use standard SQL to query the data, allowing you to slice and dice your spending by any dimension present in the data, such as project, service, SKU, and any labels you've applied to your resources. This is essential for deep cost optimization.",wrongExplanations:{1:"The billing export is a read-only record of charges. You cannot modify it.",2:"BigQuery is an analytical data warehouse. You cannot use it to directly provision cloud resources.",3:"The billing export is updated periodically (a few times a day), not in real-time. For real-time alerting, you would use Billing budgets."}},{id:366,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"You are using Query Insights for a Cloud SQL for MySQL instance. What does the 'Query Load' chart primarily show you?",options:["The amount of data returned by each query.","The number of queries executed per second.","The financial cost of each query.","The total active time of all queries, broken down by CPU, I/O, and Lock Wait."],correct:3,explanation:"The primary chart in Query Insights shows the total query load, measured in 'CPU seconds'. It further breaks down this load into the main database wait states: CPU (active processing), I/O Wait (waiting for disk), and Lock Wait (waiting for a database lock). This allows you to see at a glance not just that the database is busy, but *why* it's busy.",wrongExplanations:{1:"While related, the query load is a more comprehensive metric than just queries per second. A single, slow query can generate more load than thousands of fast ones.",2:"Query Insights is a performance tool, not a cost analysis tool. It does not show the financial cost of queries.",3:"The amount of data returned is not the primary metric for load. A query can process terabytes of data but return only a single number, generating very high load."}},{id:367,domain:"Section 4: Configuring access and security",subdomain:"Service Accounts",question:"A Cloud Function needs to write an object to a Cloud Storage bucket. What is the recommended way to grant it the necessary permissions?",options:["Use the default App Engine service account, which has the Editor role on the project.","Assign a service account with the `Storage Object Creator` role to the Cloud Function during deployment.","Hardcode a user's credentials within the function's code.","Make the Cloud Storage bucket public."],correct:1,explanation:"Cloud Functions execute with the identity of a service account. The best practice is to create a dedicated service account for the function with only the minimal permissions it needs (in this case, `roles/storage.objectCreator`) and assign this service account to the function. This follows the principle of least privilege.",wrongExplanations:{1:"Using the default service account with the broad Editor role is a security risk. If the function were compromised, the attacker would have extensive permissions in the project.",2:"Hardcoding credentials is a major security anti-pattern. Credentials should never be stored in code.",3:"Making the bucket public is a massive security vulnerability and is not the correct way to grant write access to a specific service."}},{id:368,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud CDN",question:"You have a website with static assets (images, CSS, JS) stored in a Cloud Storage bucket. The website is served by a global external Application Load Balancer. Users are reporting slow load times in geographic locations far from the bucket's region. How can you improve performance for these users?",options:["Increase the instance size of your backend VMs.","Create replicas of the bucket in every region where you have users.","Choose a faster storage class, like Standard, for the bucket.","Enable Cloud CDN on the backend bucket used by the load balancer."],correct:3,explanation:"Cloud CDN (Content Delivery Network) is designed for this exact purpose. When you enable it on the backend of your load balancer, it caches your content at Google's globally distributed edge locations. When a user requests an asset, it is served from the edge cache closest to them, dramatically reducing latency and improving load times.",wrongExplanations:{1:"Manually replicating a bucket is complex and doesn't provide the same granular, global edge caching as a CDN.",2:"The assets are static and stored in Cloud Storage, not on backend VMs, so changing VM sizes would have no effect.",3:"The storage class affects retrieval time from the bucket itself and storage cost, but it does not solve the network latency problem for globally distributed users. A CDN is the correct solution for network latency."}},{id:369,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You have a critical web service and you need to be notified immediately if it stops responding to HTTP requests. You have configured an uptime check for the service's URL. What is the next step to ensure you get notified?",options:["Point the uptime check to Cloud Logging.","Create a custom dashboard to view the uptime check status.","Configure the uptime check to send an email directly.","Create an alerting policy that uses the uptime check as its condition."],correct:3,explanation:"An uptime check simply probes your endpoint and records the result (success or failure) as a metric. To actually get notified of a failure, you must create a separate alerting policy. In this policy, you select the uptime check metric as the signal and configure it to trigger an alert and send a notification (e.g., via email, SMS, PagerDuty) when the check fails for a specified duration.",wrongExplanations:{1:"Uptime checks generate metrics, not logs.",2:"Uptime checks themselves do not send notifications. They are the signal source for alerting policies, which handle the notifications.",3:"A dashboard is for visualization, not for generating alerts and notifications."}},{id:370,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"Your company has acquired another company that already has its own Google Cloud Organization. You need to manage both entities under a single billing account and apply some common security policies. What is the recommended approach?",options:["Keep both Organizations separate, but link them both to the same billing account. Use hierarchical firewall policies for common security rules.","Set up VPC Peering between all VPCs in both Organizations.","Create a new, parent Organization and nest both existing Organizations underneath it.","Migrate all projects from the acquired company's Organization into your primary Organization."],correct:3,explanation:"Migrating projects from one organization to another is the standard procedure for consolidating Google Cloud resources after an acquisition. This allows you to manage all projects under a single organizational structure, apply a unified set of Organization Policies, and simplify IAM and billing management.",wrongExplanations:{0:"While linking to a single billing account is possible, managing policies across two separate organizations is complex. Hierarchical firewalls can help, but a single org structure is cleaner. For full consolidation, migration is the goal.",2:"The Google Cloud resource hierarchy does not support nesting one Organization under another. The Organization is the root node.",3:"VPC Peering connects networks, but it doesn't address the core requirement of unified management, policy, and billing at the organizational level."}},{id:371,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"BigQuery",question:"A data analyst in your company frequently runs the same complex, resource-intensive query against a very large BigQuery table. To improve performance and control costs for this specific query, what BigQuery feature should you use?",options:["Increase the analyst's user quota for BigQuery.","Create a materialized view that pre-computes the results of the query.","Cache the query results.","Export the table to Cloud Storage and query it with Dataflow."],correct:1,explanation:"Materialized views are pre-computed views that periodically cache the results of a query. When a user's query is compatible with the materialized view, BigQuery can read the pre-computed results instead of executing the full, expensive query against the base table. This is ideal for common and predictable query patterns on large datasets.",wrongExplanations:{1:"BigQuery automatically caches query results for 24 hours, but this only helps if the *exact same query* is run again by any user and the underlying data hasn't changed. A materialized view is more robust and can accelerate even slightly different queries.",2:"User quotas do not affect the performance of a single query.",3:"This is a highly inefficient and complex workflow. BigQuery is the correct tool for querying data within BigQuery."}},{id:372,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"You have created a VPC Service Controls perimeter that includes your project and the Cloud Storage service. A developer with valid IAM permissions is trying to copy a file from a bucket inside the perimeter to a public bucket outside the perimeter using their local machine. What will happen?",options:["The operation will be denied by VPC Service Controls.","The developer will be prompted to provide a justification before the copy is allowed.","The operation will succeed, but it will be logged in the Data Access audit logs.","The operation will succeed because the developer has the correct IAM permissions."],correct:0,explanation:"The core purpose of VPC Service Controls is to prevent data exfiltration. The service perimeter creates a virtual boundary. Even if a user has IAM permissions to read from the source bucket and write to the destination bucket, the VPC Service Controls policy will block the operation because it crosses the perimeter boundary (from a protected project to a public one).",wrongExplanations:{1:"This is incorrect. VPC Service Controls act as an additional layer of security on top of IAM.",2:"The operation will be denied, so no data will be copied to be logged.",3:"Access transparency and justifications are different features. VPC Service Controls will simply block the request."}},{id:373,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Trace",question:"You are using Cloud Trace to analyze the performance of your application. You want to see an aggregated view of latency for a specific transaction, for example, '/checkout', over the last week. Which feature of the Cloud Trace UI should you use?",options:["Cloud Monitoring dashboards","The Trace analysis report","Cloud Logging","The Trace list view"],correct:1,explanation:"The Trace analysis report provides an aggregated view of your trace data. You can generate reports to see trends in latency over time, find the root causes of performance changes by comparing distributions across different time periods, and see a percentile breakdown of latency for specific requests. This is the correct tool for historical, aggregated analysis.",wrongExplanations:{1:"The Trace list view shows a list of individual, recent traces. It's useful for inspecting a single request but not for seeing aggregated trends over a week.",2:"Cloud Logging stores logs, not aggregated trace data.",3:"While you can create charts in Monitoring for overall latency, the Trace analysis report provides a much more detailed and specialized view for diagnosing latency regressions."}},{id:374,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Load balancing",question:"You have a set of legacy TCP-based services running on VMs in a single region. You need to provide a single, private IP address within your VPC that internal clients can use to connect to these services. The load balancer should distribute traffic based on a 5-tuple hash (source/destination IP, source/destination port, protocol). Which load balancer should you use?",options:["Internal TCP/UDP Load Balancer","Internal Application Load Balancer","Global external Application Load Balancer","External TCP/UDP Network Load Balancer"],correct:0,explanation:"The Internal TCP/UDP Load Balancer is designed specifically for this use case. It provides a regional, non-proxied load balancing service for TCP and UDP traffic. It uses a private IP address from your VPC and distributes connections directly to your backend VMs, preserving the client source IP.",wrongExplanations:{1:"The Internal Application Load Balancer is a proxy-based Layer 7 load balancer for HTTP and HTTPS traffic only. It cannot be used for generic TCP services.",2:"An External load balancer has a public IP address and is for internet-facing traffic, not for internal clients within a VPC.",3:"An External Application Load Balancer is for global HTTP/S traffic, not internal TCP services."}},{id:375,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Quotas",question:"What is the primary purpose of resource quotas in Google Cloud?",options:["To reserve capacity for your project, ensuring resources are always available.","To enforce security policies on resource creation.","To act as a hard limit on your monthly bill.","To prevent unforeseen spikes in usage and protect both the user from unexpected costs and Google's infrastructure."],correct:3,explanation:"Quotas are safety limits. They protect the broader Google Cloud community by preventing resource abuse, and they protect you from yourself. For example, a bug in a script that creates VMs in a loop could generate a massive bill if not for the vCPU quota stopping it. They are a preventative control against runaway resource consumption.",wrongExplanations:{1:"Quotas limit resource count (e.g., number of VMs), not the dollar amount of your bill. You can easily hit your budget without hitting a quota. For cost control, you use budgets and alerts.",2:"Quotas are limits, not reservations. Having a quota for 100 VMs does not guarantee that you will be able to launch 100 VMs if the underlying physical capacity in a zone is exhausted.",3:"Security policies are enforced by IAM and Organization Policies, not by quotas."}},{id:376,domain:"Section 4: Configuring access and security",subdomain:"Encryption",question:"You are using Customer-Managed Encryption Keys (CMEK) with Cloud KMS to protect a BigQuery table. A security administrator temporarily disables the KMS key version used by the table. What is the immediate effect on the BigQuery table?",options:["The table and its data become inaccessible until the key version is re-enabled.","There is no effect until the KMS key is permanently destroyed.","BigQuery automatically re-encrypts the table with a Google-managed key.","The table can still be queried, but no new data can be written to it."],correct:0,explanation:"The core principle of CMEK is that the cloud service (BigQuery) needs to call the KMS API to get the key to decrypt the data for every query. If the key is disabled or destroyed, the API call fails, and BigQuery cannot decrypt the data. This makes the data completely inaccessible, giving you direct control over your data's availability.",wrongExplanations:{1:"Both read and write operations will fail because both require access to the encryption key.",2:"BigQuery will not and cannot change the encryption method of a table automatically. The data remains encrypted with the CMEK.",3:"Disabling the key has an immediate effect. You do not need to wait for it to be destroyed."}},{id:377,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE configurations",question:"You need to ensure that pods running in your GKE cluster can communicate with Google APIs like Cloud Storage and BigQuery without traversing the public internet. The GKE nodes themselves do not have public IP addresses. What should you configure?",options:["Enable Private Google Access on the subnet where the GKE nodes reside.","Deploy an egress proxy in the cluster.","Set up VPC Service Controls.","Configure a Cloud NAT gateway for the subnet."],correct:0,explanation:"Private Google Access is a feature that allows VMs (including GKE nodes) without external IP addresses to reach the public IP addresses of Google Cloud services. It configures special routing within Google's network so that traffic from your private subnet to Google APIs stays within Google's network, providing a secure and private communication path.",wrongExplanations:{1:"Cloud NAT is for allowing private instances to reach the *external internet*. For reaching Google APIs, Private Google Access is the more direct and efficient solution.",2:"VPC Service Controls protect services from data exfiltration, but they do not provide the underlying network path for connectivity.",3:"An egress proxy is a possible solution but is much more complex to set up and manage than the native, built-in Private Google Access feature."}},{id:378,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You want to retain all Admin Activity audit logs for your project for 10 years for compliance reasons. The default retention period is 400 days. What should you do?",options:["This is not possible; logs cannot be retained for more than 400 days.","Modify the retention period of the `_Required` log bucket.","Create a log sink to export the audit logs to a Cloud Storage bucket and configure the bucket's retention policy.","Create a custom log bucket and configure its retention period to 10 years."],correct:2,explanation:'The retention period for the default log buckets (`_Required` and `_Default`) cannot be changed. The standard and recommended way to achieve long-term retention is to create a log sink. The sink can filter for the specific logs you need (e.g., `logName:"logs/cloudaudit.googleapis.com%2Factivity"`) and route them to a destination designed for long-term storage, like a Cloud Storage bucket, where you can set a long retention period.',wrongExplanations:{1:"You cannot modify the retention period of the `_Required` log bucket.",2:"You could route logs to a custom bucket, but for long-term archival, Cloud Storage is generally more cost-effective and is the common pattern.",3:"This is incorrect. While the built-in retention is limited, you can achieve any retention period you need by exporting the logs."}},{id:379,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the difference between a primitive role (Owner, Editor, Viewer) and a predefined role (e.g., `roles/compute.admin`)?",options:["Primitive roles include permissions to manage IAM policies, while predefined roles do not.","There is no difference; predefined roles are just aliases for primitive roles.","Primitive roles can only be applied at the project level, while predefined roles can be applied at any level in the hierarchy.","Primitive roles are broad and apply to all services in a project, while predefined roles provide granular permissions for a specific service."],correct:3,explanation:"This is a fundamental concept in IAM. Primitive roles (Owner, Editor, Viewer) were the original roles and grant sweeping permissions across all services within a project. Predefined roles were created to allow for more granular control and follow the principle of least privilege. For example, `roles/compute.admin` grants full control over Compute Engine, but not over BigQuery or Cloud Storage. It is always a best practice to use predefined roles over primitive roles.",wrongExplanations:{1:"This is incorrect. Both types of roles can be applied at various levels (e.g., project, folder, resource).",2:"This is incorrect. They are distinct sets of permissions.",3:"This is not always true. For example, the Editor role does not grant IAM management permissions, but some predefined roles (like `roles/resourcemanager.projectIamAdmin`) do."}},{id:380,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You have a group of stateless web servers in a managed instance group (MIG). You need to update the application to a new version. The update should be applied gradually with minimal impact, and you need the ability to quickly roll back if the new version has problems. Which update method should you use for the MIG?",options:["An `opportunistic` update.","Stop all instances, update the instance template, and restart them.","A rolling update with the `canary` type.","A `proactive` update."],correct:2,explanation:"A rolling update with the canary method is a sophisticated deployment strategy. The MIG first updates a small subset of instances (the canary) to the new version. You can then monitor its performance. If it's healthy, you can proceed to update the rest of the group. If it's unhealthy, you can easily roll back just the canary instances, minimizing the blast radius of a bad update.",wrongExplanations:{1:"A proactive update replaces all instances as quickly as possible. This is not a gradual rollout.",2:"An opportunistic update only replaces instances when they are manually stopped or recreated for other reasons. This is not suitable for a controlled, timely update.",3:"This 'stop the world' approach would cause significant downtime and is not a recommended practice for high-availability applications."}},{id:381,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Marketplace",question:"A developer needs to quickly deploy a standard LAMP stack (Linux, Apache, MySQL, PHP) for a new prototype. They want to avoid manually installing and configuring each component. What is the fastest way to get this environment running on Google Cloud?",options:["Create a new Compute Engine instance with a base Linux image and manually install the components.","Deploy a pre-configured LAMP stack solution from the Google Cloud Marketplace.","Write a startup script to install Apache, MySQL, and PHP on a new VM.","Containerize the application and deploy it to a GKE cluster."],correct:1,explanation:"The Cloud Marketplace offers thousands of pre-configured, ready-to-go solutions from Google and third-party vendors. You can find and deploy a fully configured LAMP stack with just a few clicks, which is by far the fastest and easiest method for standard application stacks.",wrongExplanations:{1:"Manual installation is time-consuming and error-prone, which is what the developer wants to avoid.",2:"Writing a startup script is a form of automation, but it still requires you to figure out all the installation and configuration commands. A Marketplace solution has this already done.",3:"Containerizing the application is a good practice for production, but for a quick prototype, it is significantly more work than deploying a pre-built Marketplace solution."}},{id:382,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"What is the purpose of a 'metric scope' in Cloud Monitoring?",options:["It is a filter that you apply to a specific chart.","It sets the resolution at which metrics are collected.","It defines which Google Cloud projects' metrics you can see in your workspace.","It defines the time range for the metrics displayed on a dashboard."],correct:2,explanation:"A Cloud Monitoring workspace can be configured to monitor multiple Google Cloud projects. The metric scope is the list of projects whose metrics are visible in that workspace. This allows you to create a centralized monitoring project that has visibility into many other projects, which is a common pattern for large organizations.",wrongExplanations:{1:"The time range is selected using the time-range picker on the dashboard or metrics explorer page.",2:"Metric resolution (the sampling period) is determined by the service generating the metric, not by the Monitoring scope.",3:"A filter applies to a chart, whereas the metric scope applies to the entire workspace."}},{id:383,domain:"Section 4: Configuring access and security",subdomain:"Security Command Center",question:"You have just enabled the Security Command Center (SCC) Standard tier. What is a key capability that SCC provides for your organization?",options:["A managed service for storing and rotating secrets.","A centralized dashboard of security findings and vulnerabilities from various Google Cloud services.","Real-time intrusion detection and prevention for your VPC network.","Automated patching of security vulnerabilities on your Compute Engine instances."],correct:1,explanation:"Security Command Center is a centralized security and risk management platform. Its primary function is to ingest findings from various sources (e.g., Security Health Analytics, Web Security Scanner, Event Threat Detection) and present them in a single pane of glass. This gives you a high-level overview of your organization's security posture.",wrongExplanations:{1:"Intrusion detection (Cloud IDS) is a separate service that can feed its findings *into* SCC, but SCC is the dashboard, not the detection engine itself.",2:"SCC identifies vulnerabilities (e.g., an OS that needs patching), but it does not perform the patching itself. You would use a tool like OS Config management to do the patching.",3:"A managed service for secrets is Secret Manager, not Security Command Center."}},{id:384,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"You have two separate VPCs, `vpc-dev` and `vpc-prod`. You need to allow VMs in `vpc-dev` to communicate with VMs in `vpc-prod` using their internal IP addresses. The VPCs should not have overlapping IP ranges. What should you configure?",options:["A Shared VPC","Cloud VPN between the two VPCs","An external load balancer","VPC Network Peering"],correct:3,explanation:"VPC Network Peering is specifically designed to allow two VPCs to connect and route traffic between them using internal IP addresses. The networks behave as if they are part of the same private networking space. A key requirement is that the peered VPCs cannot have overlapping CIDR ranges.",wrongExplanations:{1:"Shared VPC is a model where a central 'host' project owns the network, and other 'service' projects can launch resources into its subnets. It's for centralizing network management, not for connecting two already existing, distinct VPCs.",2:"While you could technically set up a VPN between two VPCs, it's more complex and less performant than VPC Peering, which uses Google's internal network fabric.",3:"An external load balancer is for internet-facing traffic and does not facilitate internal VPC-to-VPC communication."}},{id:385,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You need to get the logs for a specific pod named `frontend-xyz-123` that is running in your GKE cluster. Which `kubectl` command should you use?",options:["kubectl get pods frontend-xyz-123 --logs","kubectl logs frontend-xyz-123","gcloud container logs frontend-xyz-123","kubectl describe pod frontend-xyz-123"],correct:1,explanation:"`kubectl logs [POD_NAME]` is the standard and direct command for streaming the logs (from stdout/stderr) of a running pod. This is one of the most common commands used for day-to-day GKE operations and troubleshooting.",wrongExplanations:{1:"The `--logs` flag is not a valid flag for the `kubectl get pods` command.",2:"`kubectl describe pod` provides metadata and events related to the pod's state and lifecycle, but it does not show the application logs from inside the container.",3:"`gcloud container` has commands for managing the cluster itself, but for interacting with workloads *inside* the cluster, `kubectl` is the primary tool."}},{id:386,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You need to run a `gcloud` command to create a resource, but you are not sure of the exact syntax and available flags. What is the easiest way to get help and see examples for the `gcloud compute instances create` command directly in your terminal?",options:["gcloud compute instances create --help","gcloud help compute instances create","man gcloud-compute-instances-create","gcloud docs compute instances create"],correct:0,explanation:"The `--help` flag is a universal flag in `gcloud` that can be appended to any command or subcommand. It provides a detailed description of the command, lists all the available flags with explanations, and often includes usage examples right in your terminal. `gcloud help [COMMAND]` is an equivalent way to get the same information.",wrongExplanations:{1:"This is also a correct and equivalent command.",2:"`docs` is not a valid subcommand for `gcloud`.",3:"`gcloud` commands are not documented using traditional `man` pages."}},{id:387,domain:"Section 4: Configuring access and security",subdomain:"Service Accounts",question:"What is the primary difference between a user account and a service account in Google Cloud IAM?",options:["A user account is intended for a human user, while a service account is intended for an application or VM.","User accounts can be granted the Owner role, but service accounts cannot.","User accounts are created in Cloud Identity, while service accounts are created in the IAM section of a project.","User accounts are authenticated with passwords, while service accounts are authenticated with API keys."],correct:0,explanation:"This is the fundamental distinction. User accounts represent people who interact with Google Cloud through the console or CLI. Service accounts represent non-human workloads (applications, scripts, VMs) that need to authenticate and be authorized to call Google Cloud APIs. While both are 'identities' that can be granted IAM roles, their intended use is different.",wrongExplanations:{1:"Service accounts are typically authenticated using service account keys (JSON files) or, preferably, by using the attached identity of the compute resource they are running on. They do not use API keys for authentication.",2:"Service accounts can be granted the Owner role, although it is a very bad practice.",3:"This is true, but it's a detail of their creation process. The most important difference is their intended purpose (human vs. application)."}},{id:388,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Run",question:"You have a containerized web application that you want to deploy on Cloud Run. You want to deploy a new version of the application but only direct 10% of the production traffic to it initially to test it. How can you configure this?",options:["Use Cloud Deployment Manager to manage traffic percentages.","Deploy the new version as a new revision and use traffic splitting to direct 10% of traffic to the new revision and 90% to the old one.","Create a second Cloud Run service and use a load balancer to split traffic.","Deploy the new version to all instances and then quickly roll it back if there are issues."],correct:1,explanation:"Cloud Run has built-in support for revisions and traffic splitting. When you deploy a new version, it creates a new, immutable revision. You can then configure the service to split traffic between different revisions by percentage. This is the standard, idiomatic way to perform canary releases and gradual rollouts on Cloud Run.",wrongExplanations:{1:"This is overly complex and expensive. Cloud Run's native traffic splitting feature is designed for this and does not require a separate load balancer.",2:"This is a 'big bang' or 'all-at-once' deployment, not a gradual 10% test. The goal is to limit the blast radius of a potentially bad deployment.",3:"Cloud Deployment Manager is an Infrastructure-as-Code service for provisioning resources; it does not manage the live traffic routing of a Cloud Run service."}},{id:389,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Storage",question:"You are using a Cloud Storage bucket with object versioning enabled. A user overwrites an important file with a new, incorrect version. How do you revert to the previous version of the file?",options:["The previous version is lost once it is overwritten.","Delete the current version of the object; the previous version will automatically become the current version.","Restore the object from a daily backup.","List the noncurrent versions of the object, and copy the desired previous version over the current one."],correct:3,explanation:"When versioning is enabled, overwriting an object does not delete the old data. Instead, it makes the previous version 'noncurrent' and uploads the new data as the 'live' version. To revert, you can simply find the desired noncurrent version and either copy it to a new object or copy it over the live version to effectively restore it.",wrongExplanations:{1:"Deleting the current version makes the previous version noncurrent, but it also creates a 'delete marker' as the new live object. It does not automatically promote the old version. You must perform a restore/copy action.",2:"Cloud Storage does not have automatic backups. Versioning is the feature that protects against overwrites.",3:"This is incorrect; the entire purpose of versioning is to preserve old versions when objects are overwritten or deleted."}},{id:390,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Identity",question:"Your company does not use Google Workspace but wants to manage users for Google Cloud without the cost of a full Workspace subscription. Users need to be able to authenticate with a corporate identity (e.g., `user@yourcompany.com`). Which service should you use?",options:["Service accounts for each user","Cloud Identity Free","Standard consumer Gmail accounts","IAM custom roles"],correct:1,explanation:"Cloud Identity is Google's Identity as a Service (IDaaS) product. The free tier allows you to create and manage users and groups with your company's domain, providing the core identity management needed for Google Cloud IAM without the additional collaboration features and cost of Google Workspace.",wrongExplanations:{1:"Using consumer Gmail accounts is not a best practice for a corporate environment as it provides no centralized management or control over the user identities.",2:"IAM roles define permissions, they do not create or manage user identities.",3:"Service accounts are for applications, not human users."}},{id:391,domain:"Section 4: Configuring access and security",subdomain:"Security best practices",question:"A developer needs to connect to a Compute Engine instance that has no public IP address for a debugging session. According to Google's zero-trust security principles, what is the most secure method to provide this access?",options:["Temporarily add a public IP to the instance and then remove it.","Create a bastion host with a public IP in the same VPC.","Use IAP TCP Forwarding.","Set up a Cloud VPN connection from the developer's machine to the VPC."],correct:2,explanation:"Identity-Aware Proxy (IAP) for TCP Forwarding allows you to tunnel SSH, RDP, and other TCP traffic to a VM without requiring a public IP or a bastion host. Access is controlled by IAM permissions, not network-level controls, allowing you to grant access to specific users without exposing your instances to the internet. This is a core component of a zero-trust model.",wrongExplanations:{1:"A bastion host is a traditional approach, but it creates another publicly-exposed VM that must be secured and managed. IAP is the more modern, managed, and secure solution.",2:"Temporarily adding a public IP is risky and creates a window of exposure. It should be avoided.",3:"Setting up a full VPN for a single developer's debugging session is overly complex and not scalable. IAP provides on-demand, fine-grained access."}},{id:392,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"You need a managed database for a new web application that stores user session data. The data is simple key-value pairs, access needs to be extremely fast (sub-millisecond latency), and the data is temporary and can be lost without issue. Which service is the best fit?",options:["Cloud Bigtable","Cloud SQL","Firestore","Memorystore for Redis"],correct:3,explanation:"Memorystore for Redis is a fully managed in-memory data store service. It is designed for use cases that require extremely low latency, such as caching, session management, and real-time analytics. Since the session data is temporary, the in-memory nature of Redis is a perfect fit.",wrongExplanations:{1:"Firestore is a NoSQL document database. While fast, it is a durable, disk-based storage and typically has higher latency than an in-memory store like Redis.",2:"Cloud SQL is a relational database and is overkill, as well as too slow, for a simple, high-speed session store.",3:"Cloud Bigtable is a wide-column NoSQL database designed for very large analytical and operational workloads, not for low-latency key-value caching."}},{id:393,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Profiler",question:"You've used Cloud Profiler and the flame graph shows that a large amount of time is being spent in a function called `waitForNetworkResponse`. What does this likely indicate about your application's performance?",options:["The function is performing a very complex mathematical calculation.","The application is spending significant time waiting for responses from external services (I/O wait).","The function has a bug and is in an infinite loop.","The function is consuming a large amount of memory."],correct:1,explanation:"Cloud Profiler can track various metrics, including 'Wall Time'. If a significant portion of wall time is spent in a function related to network calls, it means the application's execution is blocked waiting for a response from another service. This is a classic I/O bottleneck, and it highlights an opportunity to optimize by making calls in parallel or reducing dependency on a slow external service.",wrongExplanations:{1:"An infinite loop would typically manifest as high CPU time, not just high wall time.",2:"A complex calculation would also manifest as high CPU time.",3:"To analyze memory consumption, you would look at the 'Heap' or 'Allocated Heap' profile types, not the wall time profile."}},{id:394,domain:"Section 4: Configuring access and security",subdomain:"Cloud Armor",question:"Your web application is experiencing a SQL injection (SQLi) attack. The application is served by an external Application Load Balancer. Which feature of Cloud Armor can help you mitigate this type of attack?",options:["A geo-based access control rule.","An IP allowlist/denylist rule.","The pre-configured WAF rules for SQLi.","A rate-limiting rule."],correct:2,explanation:"Cloud Armor acts as a Web Application Firewall (WAF). It includes pre-configured rules based on the ModSecurity Core Rule Set that are specifically designed to detect and block common web-based attacks, including SQL injection (SQLi), Cross-Site Scripting (XSS), and others. Enabling these rules provides an immediate layer of defense at the network edge.",wrongExplanations:{1:"A geo-based rule blocks traffic from a country; it does not inspect the traffic for SQLi payloads.",2:"A rate-limiting rule can help against volumetric attacks, but it won't stop a single, well-crafted SQLi request.",3:"An IP-based rule is only effective if the attack is coming from a small, known set of IP addresses, which is rarely the case."}},{id:395,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"Your project contains a mix of production and development resources. You need to be able to see a cost breakdown for just the development resources. What is the best way to achieve this?",options:["Create a separate billing account for development resources.","Move all development resources to a separate project.","Manually track the cost of development resources in a spreadsheet.","Apply a label (e.g., `env:dev`) to all development resources and filter the billing reports by that label."],correct:3,explanation:"Labels are key-value pairs that you can attach to resources for organization and cost allocation. The Cloud Billing reports are fully integrated with labels, allowing you to group and filter your costs by any label you define. This is the most flexible and standard way to get cost breakdowns within a single project.",wrongExplanations:{1:"While moving resources to a separate project is a valid strategy for isolation, it's a much heavier operation than simply applying labels, which can be done without re-provisioning resources. For simple cost tracking, labels are preferred.",2:"A project can only be linked to one billing account at a time. Creating a separate billing account is unnecessary and adds complexity.",3:"Manual tracking is inefficient and error-prone."}},{id:396,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"What is the purpose of a Shared VPC?",options:["To allow a central host project to own and manage a network that service projects can use.","To share a single public IP address among multiple VM instances.","To extend your on-premises network into Google Cloud.","To connect two VPCs from different organizations."],correct:0,explanation:"Shared VPC is a centralized networking model. A 'host project' owns the VPC network, subnets, and routes. Other 'service projects' can then be attached to the host project, allowing them to create resources (like VMs) that use the subnets from the host project. This is ideal for large organizations that want a central network administration team to manage a consistent and secure network, while allowing developer teams to manage their own resources in separate projects.",wrongExplanations:{1:"Connecting VPCs from different organizations is typically done with VPC Peering or a VPN.",2:"Sharing a public IP is the function of a load balancer, not a Shared VPC.",3:"Extending an on-premises network is done with Cloud Interconnect or Cloud VPN."}},{id:397,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You have deployed an application to GKE using a Deployment manifest. You now need to expose the application to other services within the cluster using a stable DNS name. Which Kubernetes resource should you create?",options:["An Ingress","A Service (of type ClusterIP)","A Pod","A StatefulSet"],correct:1,explanation:"A Kubernetes Service provides a stable abstraction over a set of pods. It gets a stable IP address and a DNS name within the cluster. By default, a Service is of type `ClusterIP`, which means it's only reachable from within the cluster. This is the standard way to enable service-to-service communication.",wrongExplanations:{1:"An Ingress is for exposing services to traffic *outside* the cluster, typically HTTP/S traffic. For internal communication, a Service is sufficient.",2:"A Pod is the workload itself; it does not provide a stable endpoint as pods are ephemeral and their IPs can change.",3:"A StatefulSet is a workload API object for stateful applications, but the Service is still the resource that provides the stable network endpoint for it."}},{id:398,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You grant a user the `roles/storage.objectCreator` role on a bucket. You then grant them the `roles/storage.objectViewer` role at the project level. What are the user's effective permissions on objects in that bucket?",options:["They have no permissions, as the roles conflict.","They can only create objects in that bucket.","They can create and view objects in that bucket.","They can only view objects in that bucket."],correct:2,explanation:"IAM policies are a union of all policies that apply to a resource. The user gets the permissions from the bucket-level policy (`storage.objectCreator`) plus the permissions from the project-level policy (`storage.objectViewer`) which are inherited by the bucket. Therefore, their effective permissions are the combination of both roles.",wrongExplanations:{1:"The project-level role is inherited, so they also get viewer permissions.",2:"The bucket-level role also applies, so they also get creator permissions.",3:"IAM policies are additive. There are no conflicts; permissions are combined."}},{id:399,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to create a group of 10 identical Compute Engine VMs for a web frontend. You want to manage them as a single logical unit and ensure that if one VM fails, it is automatically recreated. What should you create?",options:["An unmanaged instance group.","A managed instance group (MIG).","An instance template and then create 10 VMs from it.","A GKE cluster with 10 nodes."],correct:1,explanation:"A managed instance group (MIG) is the correct solution. It uses an instance template to create a set of identical VMs and then manages them for you. Key features include autoscaling, autohealing (recreating failed instances), and rolling updates, which perfectly match the requirements.",wrongExplanations:{1:"An unmanaged instance group is just a collection of heterogeneous VMs. It provides no autohealing or lifecycle management.",2:"This is a manual process. While you would use an instance template, the MIG is the resource that provides the management and autohealing capabilities.",3:"A GKE cluster is for running containerized workloads, not for managing a group of individual VMs directly."}},{id:400,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You have just run `gcloud init` and authenticated. You now want to set your default compute zone to `europe-west1-b` so you don't have to specify the `--zone` flag for every `gcloud compute` command. Which command should you run?",options:["gcloud compute set-zone europe-west1-b","gcloud projects set-default-zone europe-west1-b","gcloud config set default_zone europe-west1-b","gcloud config set compute/zone europe-west1-b"],correct:3,explanation:"The `gcloud config set` command is used to set properties in your active gcloud configuration. Properties are grouped into sections. The default zone and region for compute commands are in the `compute` section, so the correct property name is `compute/zone`.",wrongExplanations:{1:"`default_zone` is not a valid property name.",2:"`gcloud compute set-zone` is not a valid command.",3:"`gcloud projects set-default-zone` is not a valid command."}},{id:401,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You want to be notified whenever a specific error message, `FATAL: connection limit exceeded`, appears in your PostgreSQL logs in Cloud Logging. You need to create an alert for this. What is the first step you should take?",options:["Create an uptime check for the database.","Create a log sink to export the logs to Cloud Storage.","Create a logs-based metric that counts the occurrences of the error message.","Enable Data Access audit logs for the database."],correct:2,explanation:'Cloud Monitoring alerts on metrics, not directly on log entries. To alert on something in a log, you must first create a logs-based metric. This metric will be a counter that increments every time a log entry matching your filter (e.g., `textPayload: "FATAL: connection limit exceeded"`) is ingested. Once you have this metric, you can create a standard metric-based alerting policy for it.',wrongExplanations:{1:"An uptime check probes network connectivity; it cannot read the content of log files.",2:"A log sink is for routing logs, not for creating alerts.",3:"Data Access audit logs track who is accessing data, not application-level error messages from the database engine."}},{id:402,domain:"Section 4: Configuring access and security",subdomain:"VPC Security",question:"What is the purpose of the implied `deny all egress` firewall rule in a VPC network?",options:["It blocks all incoming traffic from the internet.","It blocks any outgoing traffic that is not explicitly allowed by another firewall rule with a higher priority.","It denies traffic between different subnets within the VPC.","It prevents VMs from being created with public IP addresses."],correct:1,explanation:"Every VPC network has two implied firewall rules that cannot be deleted, each with the lowest possible priority (65535): an `allow all egress` rule and a `deny all ingress` rule. However, if you create any custom egress rule, you might think of the default behavior as 'deny unless allowed'. The `deny all egress` rule, priority 65535, blocks any outgoing traffic that isn't matched by a higher-priority (lower number) rule. It acts as a fail-safe.",wrongExplanations:{1:"Preventing public IPs is the job of an Organization Policy, not a firewall rule.",2:"Blocking incoming traffic is the job of the implied `deny all ingress` rule.",3:"The implied `allow all egress` and the default `allow internal` rules permit traffic between subnets. The `deny all egress` rule applies to traffic leaving the VPC."}},{id:403,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You need to provide a user with a time-limited URL to upload a large file directly to a Cloud Storage bucket, without giving them permanent IAM permissions or making the bucket public. What should you generate?",options:["A signed URL (V4).","A public access URL for the bucket.","A service account key.","An access control list (ACL) for the user."],correct:0,explanation:"Signed URLs provide a way to grant time-limited access to a specific Cloud Storage resource. You can generate a URL that grants a specific user or service account permission to perform an action (like `PUT` for an upload) on a specific object for a defined period. The user can then use this URL to perform the action without needing any other Google Cloud credentials.",wrongExplanations:{1:"Giving a user a service account key grants them permanent (until the key is revoked) permissions, which is not what is required.",2:"ACLs are a legacy way to manage permissions and are not ideal for providing temporary, one-off access. Uniform bucket-level access and signed URLs are the modern approach.",3:"A public URL would allow anyone to access the bucket, which is insecure."}},{id:404,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You have a fleet of VMs and you want to ensure they all have the Cloud Monitoring agent installed and running correctly to collect detailed system metrics (like memory and disk usage). What is the most efficient way to manage the agent's installation and ensure it stays up-to-date across the entire fleet?",options:["Manually SSH into each VM and run the installation script.","Use the OS Config agent to manage the installation and version of the Monitoring agent via a policy.","Use a startup script on each VM to check for and install the agent on every boot.","Create a custom Compute Engine image with the agent pre-installed."],correct:1,explanation:"OS Config is a suite of tools for managing operating system configuration at scale. You can create a guest policy that defines the desired state for a package (like the `google-cloud-ops-agent`). The OS Config agent on each VM will then ensure that this package is installed and maintained at the specified version, providing a centralized and automated way to manage the fleet.",wrongExplanations:{1:"Manual SSH is not scalable or efficient for a fleet of VMs.",2:"A custom image is a good start, but it doesn't solve the problem of keeping the agent updated after the VMs have been created. OS Config manages the lifecycle of the agent.",3:"A startup script runs on every boot and can be slow and inefficient. OS Config is a more robust, state-based management tool."}},{id:405,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"At which level of the Google Cloud resource hierarchy can a billing account be attached?",options:["Resource (e.g., a VM)","VPC Network","Organization and Project","Folder"],correct:2,explanation:"A billing account is what pays for the resources consumed. It can be linked directly to an Organization, in which case all projects under that organization can be linked to it. It can also be linked directly to individual projects that are not part of an organization. It cannot be attached to folders or individual resources.",wrongExplanations:{1:"Folders are for grouping projects and applying policies; they do not have a direct billing relationship.",2:"Billing is not managed at the individual resource level.",3:"Billing is not managed at the network level."}},{id:406,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You want to see a history of all changes made to the IAM policy of a project, including who made the change and when. Where would you find this information?",options:["In the Admin Activity audit logs for the project.","In the IAM section of the Cloud Console.","In the Data Access audit logs.","In the project's activity feed."],correct:0,explanation:"Modifying an IAM policy (e.g., granting a user a new role) is a significant administrative action. All such actions are recorded in the Admin Activity audit logs, which are enabled by default. These logs capture the API call (`SetIamPolicy`), the identity that made the call, and the timestamp.",wrongExplanations:{1:"The activity feed provides a high-level summary of recent actions but may not have the full historical detail that the audit logs provide.",2:"The IAM page shows the *current* policy, not the history of changes to it.",3:"Data Access logs are for tracking access to data (e.g., reading a file), not for administrative changes."}},{id:407,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE Autopilot",question:"What is a key security benefit of using a GKE Autopilot cluster compared to a GKE Standard cluster?",options:["It automatically encrypts all secrets in the cluster.","The underlying nodes are managed by Google, reducing the attack surface and management burden for the user.","It requires the use of VPC-native clusters.","All pods are automatically deployed into a secure sandbox."],correct:1,explanation:"In Autopilot mode, Google manages the nodes, including the OS, runtime, and networking. This means Google is responsible for security patching and hardening of the nodes. This significantly reduces the attack surface that the user is responsible for, as they cannot SSH into nodes or modify their low-level configuration.",wrongExplanations:{1:"While Autopilot does implement various security measures, the term 'secure sandbox' is not a standard feature. Both modes use standard Linux container isolation.",2:"Both Standard and Autopilot clusters support encryption of secrets at rest, but it's not an automatic feature unique to Autopilot.",3:"Both Standard and Autopilot clusters are VPC-native. This is not a differentiating security benefit."}},{id:408,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Observability",question:"What are the three core pillars of Google Cloud's operations suite (formerly Stackdriver)?",options:["Billing, IAM, and Networking","Logging, Monitoring, and Trace","Compute, Storage, and Databases","Security, Compliance, and Auditing"],correct:1,explanation:"Google Cloud's operations suite is built around the three fundamental pillars of observability: Cloud Logging (for collecting and analyzing log data), Cloud Monitoring (for collecting and visualizing metrics and creating alerts), and Cloud Trace (for distributed tracing to understand request latency). Error Reporting, Profiler, and Debugger are also part of the suite but these three are the core.",wrongExplanations:{1:"These are core infrastructure services, not observability tools.",2:"These are core infrastructure categories, not observability tools.",3:"These are security concepts, not the pillars of the operations suite."}},{id:409,domain:"Section 4: Configuring access and security",subdomain:"Secret Manager",question:"Your application running on a Compute Engine VM needs to connect to a third-party API using an API key. According to best practices, how should you store and retrieve this API key?",options:["Store the key in Secret Manager and grant the VM's service account the Secret Manager Secret Accessor role.","Hardcode the key as a string in the application's source code.","Store the key in the VM's custom metadata.","Store the key in a file on the VM's boot disk."],correct:0,explanation:"Secret Manager is a dedicated service for storing secrets like API keys, passwords, and certificates. It provides versioning, auditing, and fine-grained access control. The best practice is to store the secret in Secret Manager and grant the service account of the application that needs it the specific IAM role to access it (`roles/secretmanager.secretAccessor`). The application can then retrieve the secret at runtime.",wrongExplanations:{1:"Storing secrets on disk in plaintext is insecure. If the VM is compromised, the secret is immediately exposed.",2:"Hardcoding secrets in source code is a major security anti-pattern and makes rotation extremely difficult.",3:"While custom metadata is more secure than putting secrets in code, it's still not as secure as using a dedicated secret management service like Secret Manager, which provides much better auditing and access control."}},{id:410,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Run",question:"You have a Cloud Run service that needs to connect to a Cloud SQL database instance that has only a private IP address. How can the Cloud Run service establish this connection?",options:["This type of connection is not possible.","Configure a Serverless VPC Access connector.","Use the Cloud SQL Auth Proxy in your container.","Assign a public IP address to the Cloud SQL instance."],correct:1,explanation:"Serverless VPC Access connectors create a bridge between your serverless environment (like Cloud Run or Cloud Functions) and your VPC network. By routing the Cloud Run service's traffic through the connector, the service can access resources in the VPC, such as a Cloud SQL instance, using their internal, private IP addresses.",wrongExplanations:{1:"Assigning a public IP to the database is a security risk and is often against company policy. The goal is to connect privately.",2:"The Cloud SQL Auth Proxy is used for secure connections, often over the public internet or from environments like GKE. For private IP connections from Cloud Run, the VPC connector is the required networking component.",3:"This is incorrect; it is a very common and supported pattern."}},{id:411,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"Your company has a policy that all Cloud Storage buckets must be created in the `EU` multi-region to comply with data residency regulations. How can you enforce this and prevent buckets from being created in other locations?",options:["Create an IAM role that only allows creating buckets in the `EU`.","Write a Cloud Function that is triggered when a new bucket is created and deletes it if it's in the wrong location.","Set an Organization Policy with the `storage.location` constraint, allowing only the `EU` value.","Trust your developers to follow the documented guidelines."],correct:2,explanation:"Organization Policies are the correct tool for enforcing resource constraints. The `gcp.resourceLocations` or the more specific `storage.location` constraint can be used to define an allowed list of locations for resource creation. By setting this at the organization or folder level, any attempt to create a bucket in a non-allowed location will be blocked by the API.",wrongExplanations:{1:"IAM controls *who* can perform an action, but not *where* they can perform it. This is not the right tool for location constraints.",2:"This is a reactive approach. It's better to prevent the non-compliant resource from being created in the first place, which is what an Organization Policy does.",3:"Relying on trust is not an adequate compliance control. An automated, preventative measure is required."}},{id:412,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Backup strategies",question:"What is a key benefit of using Cloud Storage for backups of on-premises data compared to using traditional on-premises tape backups?",options:["Geographic redundancy, durability, and lower operational overhead.","Lower upfront hardware costs.","Faster restore times for large datasets.","Better integration with on-premises applications."],correct:0,explanation:"Cloud Storage provides 99.999999999% (11 nines) of durability and automatically replicates data across multiple locations (for regional and multi-regional buckets). This provides incredible data protection against disasters. It also eliminates the operational overhead of managing physical tapes, drives, and off-site storage logistics.",wrongExplanations:{1:"Restoring large datasets from the cloud can be limited by network bandwidth, and may be slower than restoring from a local tape drive if the network connection is slow. The primary benefits are durability and accessibility.",2:"While true, this is a part of the broader benefit of lower operational overhead. The durability and redundancy are more significant technical benefits.",3:"Cloud storage does not inherently integrate better with on-premises applications than on-premises storage. The key benefits are related to the nature of the cloud platform itself."}},{id:413,domain:"Section 4: Configuring access and security",subdomain:"Cloud Audit Logs",question:"You have enabled Data Access audit logs for your Cloud Storage bucket. Which of the following actions would be recorded in these logs?",options:["A user deleting the bucket.","A user changing the bucket's storage class.","A user modifying the IAM policy of the bucket.","A user reading the contents of a file (`storage.objects.get`)."],correct:3,explanation:"Data Access audit logs are specifically for tracking when data is created, read, or modified. Actions like `storage.objects.get` (reading a file), `storage.objects.create` (writing a file), and `storage.objects.list` are typical examples of what would be captured. Administrative changes are captured in the Admin Activity logs.",wrongExplanations:{1:"Changing the bucket's configuration is an administrative action and would be in the Admin Activity logs.",2:"Deleting the bucket is an administrative action and would be in the Admin Activity logs.",3:"Modifying the IAM policy is a critical administrative action and would be in the Admin Activity logs."}},{id:414,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Filestore",question:"You are migrating a legacy application from on-premises to Google Cloud. The application requires a shared file system (NFS) that can be mounted by multiple Compute Engine instances simultaneously. Which Google Cloud service provides a fully managed NFS server?",options:["Cloud Storage with FUSE","Cloud Filestore","A self-managed NFS server on a Compute Engine VM","Persistent Disk in Multi-Writer mode"],correct:1,explanation:"Cloud Filestore is Google's fully managed Network File System (NFS) service. It provides a familiar file system interface and is designed for applications that require a shared file system. It's the direct, managed solution for this common lift-and-shift migration scenario.",wrongExplanations:{1:"Cloud Storage FUSE allows you to mount a Cloud Storage bucket as a file system, but it does not provide true POSIX compliance or the performance of a dedicated NFS solution. It's best for different use cases.",2:"Persistent Disk in Multi-Writer mode allows a disk to be attached to multiple VMs, but it requires the use of a clustered file system (like GFS2) and does not provide an NFS interface.",3:"While you could build your own NFS server, it would not be a managed service. You would be responsible for its availability, performance, and maintenance. Filestore is the managed solution."}},{id:415,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You are creating an alerting policy. You want to receive notifications through multiple channels: email to your SRE team's mailing list and a message to a specific Slack channel. What do you need to configure?",options:["Write a Cloud Function that is triggered by the alert and sends the notifications to both channels.","Create two separate notification channels, one for email and one for Slack, and attach both to the alerting policy.","Configure a single notification channel that points to Slack, and set up a rule in Slack to forward the message to email.","This is not possible; an alerting policy can only have one notification channel."],correct:1,explanation:"Cloud Monitoring alerting policies support attaching multiple notification channels. The correct procedure is to configure each desired channel (e.g., an Email channel, a Slack channel, a PagerDuty channel) separately under the main Monitoring settings, and then when you create the alerting policy, you can select checkboxes for all the channels you want to notify.",wrongExplanations:{0:"While this might work, it's a workaround. The native solution is to use multiple channels directly.",2:"This is incorrect. Multiple channels are fully supported.",3:"This is overly complex. The functionality is built directly into Cloud Monitoring."}},{id:416,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cost management",question:"Your company gives all developers their own 'sandbox' projects. You want to ensure that if a developer forgets to shut down a large VM, they don't accidentally run up a huge bill. What is the most effective way to automatically control spending in these projects?",options:["Grant developers a custom role that prevents them from creating large VMs.","Rely on low default quotas to limit the number of resources they can create.","Send a daily email to all developers reminding them to shut down their resources.","Set a billing budget with a notification, and link it to a Pub/Sub topic that triggers a Cloud Function to disable billing for the project."],correct:3,explanation:"This is a key cost control pattern. You can create a budget and set an alert threshold (e.g., 100% of the budget). Instead of just sending an email, you can have this alert publish a message to a Pub/Sub topic. This message can then trigger a Cloud Function that programmatically disables billing for that project, which shuts down all running resources and prevents any further charges. This is the most reliable and automated way to enforce a hard spending limit.",wrongExplanations:{1:"This restricts experimentation and might prevent them from testing something they legitimately need to. Controlling the total cost is more flexible.",2:"Quotas limit resource count, not cost. A developer could still run a very expensive type of resource for a long time without hitting a quota.",3:"A reminder email is not an automated control and is not reliable."}},{id:417,domain:"Section 4: Configuring access and security",subdomain:"IAM Recommender",question:"You are a new administrator for a Google Cloud project that has been running for a year. You suspect that many users have overly broad permissions. What is the easiest way to identify IAM roles that have been granted but are not being used?",options:["Remove all roles and see who complains.","Ask each user what permissions they think they need.","Manually review the Admin Activity audit logs for every user.","Use IAM Recommender to get insights on unused and excessive permissions."],correct:3,explanation:"IAM Recommender is an intelligent service that analyzes your IAM usage patterns over time. It can automatically identify permissions that have been granted but not used within the last 90 days and provide recommendations to remove or replace overly permissive roles with more granular ones. This is the most efficient and data-driven way to enforce the principle of least privilege.",wrongExplanations:{1:"Manually reviewing logs would be an incredibly time-consuming and difficult task. IAM Recommender automates this analysis.",2:"Users often don't know the exact permissions they need or may ask for more than is necessary. A data-driven approach is better.",3:"This is a disruptive and unprofessional way to manage permissions."}},{id:418,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Build",question:"You want to automate the process of building a Docker container image from a Dockerfile in your source code repository every time you push a new commit to the `main` branch. The built image should be pushed to Artifact Registry. Which service should you use?",options:["Cloud Functions","Cloud Build","Cloud Run","Cloud Deployment Manager"],correct:1,explanation:"Cloud Build is a fully managed continuous integration, delivery, and deployment (CI/CD) platform. It can execute builds based on triggers, such as a commit to a Git repository. A common use case is to define a build pipeline in a `cloudbuild.yaml` file that uses the Docker build step to create an image and then pushes it to Artifact Registry.",wrongExplanations:{1:"Cloud Functions is for running event-driven code, not for orchestrating a container build pipeline.",2:"Cloud Deployment Manager is for provisioning infrastructure, not for building software artifacts.",3:"Cloud Run is for running containers, not for building them."}},{id:419,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"You are troubleshooting a performance issue on a Windows Server Compute Engine instance. You need to access the graphical user interface of the server to use a diagnostic tool. How can you connect to the instance?",options:["Use the serial console to access the command prompt.","Generate Windows credentials and connect using an RDP client.","View a screenshot of the instance.","Connect using SSH from the Cloud Console."],correct:1,explanation:"The standard way to connect to the graphical desktop of a Windows Server instance is by using the Remote Desktop Protocol (RDP). In Google Cloud, you would first generate a username and password for the instance, and then you can use any RDP client (like the one built into Windows or others on macOS/Linux) to connect to the instance's IP address.",wrongExplanations:{1:"SSH is for command-line access to Linux instances, not for graphical access to Windows.",2:"The serial console on a Windows instance provides a command-line interface (Special Administration Console), not the full graphical desktop.",3:"A screenshot is a static image and does not provide an interactive session."}},{id:420,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"Your team is adopting Infrastructure as Code. You need to create a Terraform configuration to provision a new VPC network with three subnets in different regions. How can Gemini Cloud Assist accelerate this process?",options:["Ask Gemini: 'Generate a Terraform configuration for a VPC with subnets in us-central1, europe-west1, and asia-east1'.","Use Gemini to analyze the cost of the VPC network.","Ask Gemini to apply the Terraform configuration for you.","Use Gemini to write unit tests for your Terraform code."],correct:0,explanation:"Gemini Cloud Assist is adept at generating code and configuration in various languages, including HCL for Terraform. By describing the desired infrastructure in natural language, you can get a valid and well-structured Terraform configuration file as a starting point, which you can then customize and apply. This significantly reduces the time spent looking up syntax and resource definitions.",wrongExplanations:{1:"Cost analysis is typically done using the Google Cloud Pricing Calculator or by analyzing billing data. Gemini's primary role here is code generation.",2:"Gemini can generate the configuration files, but it does not execute commands like `terraform apply`. You would still run that yourself.",3:"While a powerful AI, generating specific unit tests for IaC is a more advanced task. Its core strength lies in generating the primary configuration."}},{id:421,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud NGFW",question:"You are using Cloud NGFW network firewall policies. You have a default 'allow all egress' rule with a priority of 1000. You need to add a rule to explicitly block all VMs from making outbound connections to a known malicious IP range, `203.0.113.0/24`. What priority should you assign to this new 'deny' rule?",options:["The same priority, 1000.","A lower number than 1000 (e.g., 900).","Priority does not matter for deny rules.","A higher number than 1000 (e.g., 1100)."],correct:1,explanation:"In Google Cloud firewall policies, rules are evaluated in order of priority, from the lowest number (highest priority) to the highest number (lowest priority). To ensure your specific 'deny' rule is evaluated *before* the general 'allow' rule, it must have a higher priority, which means a lower number.",wrongExplanations:{1:"If the deny rule has a lower priority (higher number) than the allow rule, traffic to the malicious IP range would be permitted by the allow rule first, and the deny rule would never be evaluated.",2:"If two rules have the same priority, their behavior is not guaranteed. You should always use distinct priorities.",3:"Priority is critical for all rules to ensure a deterministic and predictable firewall policy."}},{id:422,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"Which of the following is NOT a feature or capability of the Database Center?",options:["Executing SQL queries directly against your databases.","Seeing an overview of performance metrics like CPU utilization for the entire database fleet.","Viewing a centralized list of all database instances across multiple projects.","Identifying database instances with security recommendations from Security Command Center."],correct:0,explanation:"Database Center is an observability and management dashboard. It provides high-level views of performance, health, and security. It is not a query execution tool or an IDE. To run queries, you would use the specific tool for that database, such as the Cloud SQL Studio, the `bq` command-line tool, or a standard database client.",wrongExplanations:{1:"A centralized inventory is a core feature of Database Center.",2:"Fleet-level performance overviews are a key benefit.",3:"Integration with Security Command Center to show security posture is a key feature."}},{id:423,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You have created a custom IAM role for your application's service accounts. Later, you realize you added a permission that is too broad and you need to remove it. What is the process for updating the custom role?",options:["Delete the custom role and recreate it with the correct permissions.","You cannot edit a custom role. You must create a new role and grant it to all the service accounts.","Edit the existing custom role to remove the unwanted permission. The change will apply to all identities that have been granted the role.","Run `gcloud iam roles update` and provide only the permissions you want to keep."],correct:2,explanation:"Custom IAM roles are mutable. You can edit them after they have been created to add or remove permissions. When you update a role's definition, the changes are propagated and take effect for all principals (users, groups, service accounts) that have been assigned that role.",wrongExplanations:{1:"Custom roles are editable, so this is incorrect.",2:"Deleting the role would revoke access for all service accounts. Editing the existing role is the correct, non-disruptive procedure.",3:"While this describes the command, the key concept is that existing roles can be edited, and the change applies everywhere."}},{id:424,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Config Connector",question:"You are using Config Connector to manage a Cloud SQL instance. You define the desired state of the instance in a YAML file and apply it to your GKE cluster. What happens if an engineer later goes into the Cloud Console and manually changes a setting on that Cloud SQL instance, such as the machine type?",options:["The manual change will persist, and Config Connector will update its status to show a discrepancy.","Config Connector will delete and recreate the Cloud SQL instance to match the desired state.","Config Connector will not be aware of the manual change.","Config Connector's controllers will detect the drift and automatically revert the change to match the state defined in the YAML file."],correct:3,explanation:"Config Connector operates on the principle of continuous reconciliation. It periodically checks the state of the Google Cloud resources it manages against the desired state declared in your Kubernetes manifests. If it detects any manual changes (drift), it will automatically take action to bring the resource back into compliance with your declared configuration.",wrongExplanations:{1:"The default behavior is to actively enforce the desired state, not just report on it.",2:"Deletion and recreation is a drastic step that Config Connector would only take if necessary. For a simple setting change, it would perform an update.",3:"Drift detection and correction is a core feature of Config Connector."}},{id:425,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Console",question:"You want to quickly find a specific Compute Engine instance named `instance-123` among hundreds of other resources in your project. What is the most efficient way to do this in the Google Cloud Console?",options:["Use Cloud Shell to run `gcloud compute instances list` and grep for the name.","Use the search bar at the top of the Cloud Console.","Go to the Compute Engine page and manually look through the list of instances.","Check the project's activity logs."],correct:1,explanation:"The search bar at the top of the Cloud Console is a powerful, unified search tool. It allows you to quickly find resources by name, ID, label, and other attributes across all services in your project. This is much faster than navigating to a specific service's page and searching there.",wrongExplanations:{1:"Manual searching is inefficient for a large number of resources.",2:"Using the CLI is a valid approach, but the question asks for the most efficient way *in the Cloud Console*.",3:"Activity logs show actions taken on resources, but they are not a tool for finding a resource's current state."}},{id:426,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"A developer is complaining that their queries to a Cloud SQL instance are slow. Using Query Insights, you identify their specific query and see that its top 'Wait State' is 'Lock Wait'. What does this indicate?",options:["The query is waiting for data to be read from disk.","The database instance does not have enough memory.","The query is waiting for another transaction to release a lock on a row or table it needs to access.","The query is actively using the CPU to process data."],correct:2,explanation:"'Lock Wait' specifically means that the query is blocked because it is trying to access a resource (like a row, a page, or a whole table) that is currently locked by another active transaction. This is a common source of performance issues in transactional databases and points to problems with transaction contention or long-running transactions.",wrongExplanations:{1:"Waiting for disk is categorized as 'I/O Wait'.",2:"Active processing is categorized as 'CPU'.",3:"While low memory can cause performance issues (like more I/O), the specific 'Lock Wait' state points directly to resource locking contention."}},{id:427,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"You have set up a VPC Service Controls perimeter. A VM inside the perimeter needs to call a third-party API on the public internet, but the perimeter is blocking all egress traffic to non-supported services. How can you allow this specific outbound traffic while maintaining the perimeter's security?",options:["Create a VPC firewall rule to allow the traffic.","Configure an egress policy for the service perimeter that allows traffic to the required external host.","Temporarily disable the service perimeter.","Use Cloud NAT to provide an outbound path."],correct:1,explanation:"VPC Service Controls are designed to be configurable. An egress policy is a specific feature that allows you to define granular exceptions to the perimeter's egress blocking. You can specify which identities are allowed to call which external services (defined by hostnames, IP ranges, etc.), providing a secure and auditable way to allow necessary external communication.",wrongExplanations:{1:"A VPC firewall rule will not override a VPC Service Controls policy. The perimeter policy is enforced at a higher level.",2:"Disabling the perimeter would completely remove the security boundary and is not a recommended practice.",3:"Cloud NAT provides a network path, but it does not override the denial from the VPC Service Controls policy."}},{id:428,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"BigQuery",question:"You have a very large (petabyte-scale) BigQuery table. You need to run a query that processes all the data in the table every morning. To manage costs and ensure predictable performance, which pricing model should you use for this project?",options:["On-demand pricing.","Capacity-based pricing (slots reservations/commitments).","Flat-rate pricing.","Storage-based pricing."],correct:1,explanation:"On-demand pricing charges you per byte processed, which can be very expensive and have variable performance for large, recurring queries. Capacity-based pricing (often called flat-rate) allows you to reserve a specific amount of query processing capacity (slots) for a fixed price. This provides predictable costs and performance, making it ideal for large, enterprise-scale data warehousing workloads.",wrongExplanations:{1:"On-demand pricing would be very costly and unpredictable for this use case.",2:"Storage-based pricing refers to the cost of storing data, not the cost of querying it.",3:"Flat-rate is the older term for what is now called capacity-based pricing. This is also correct, but 'capacity-based' is the more modern term."}},{id:429,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Error Reporting",question:"Your application is logging errors to Cloud Logging with correctly formatted JSON, including an `error` field and a stack trace. You have enabled Error Reporting, but the errors are not appearing in the Error Reporting dashboard. What is a likely cause?",options:["The IAM permissions for the Error Reporting service are incorrect.","You need to install the Error Reporting agent on your VMs.","The log entries are not formatted as expected by Error Reporting, such as missing a `serviceContext` or a correctly formatted `message` field with a stack trace.","Error Reporting only works for applications written in specific languages."],correct:2,explanation:"For Error Reporting to automatically parse errors from Cloud Logging, the log entries must be in a specific JSON format. It looks for a `message` field containing the stack trace and a `serviceContext` object that identifies the application. If these fields are missing or incorrectly formatted, Error Reporting will not be able to group the errors correctly.",wrongExplanations:{1:"There is no general 'Error Reporting agent'. It works by integrating with services or by parsing structured logs.",2:"If logs are appearing in Cloud Logging, it's unlikely to be a permissions issue for the Error Reporting service itself, but rather a formatting issue in the logs.",3:"While there are client libraries for specific languages, Error Reporting can parse errors from any language as long as the logs are structured correctly."}},{id:430,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"A project's spending has been unexpectedly shut down after its budget was exceeded. Developers in the project report they can no longer use any billable services. What action must be taken to re-enable services for the project?",options:["Increase the budget amount in the project's budget settings.","Unlink and relink the project to the billing account in the Cloud Console.","Grant the developers the 'Billing Account User' role.","The services will re-enable automatically at the start of the next billing cycle."],correct:1,explanation:"When billing is programmatically disabled for a project (often via a Cloud Function triggered by a budget alert), the link between the project and its billing account is severed. To restore services, you must manually re-enable billing by relinking the project to a valid billing account.",wrongExplanations:{1:"Simply increasing the budget amount does not re-enable billing if it has been disabled. The link must be restored first.",2:"IAM roles control who can manage billing, but they do not re-enable a disabled billing link.",3:"Services do not automatically re-enable. Manual intervention is required."}},{id:431,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to provide a set of sensitive configuration files to a new Compute Engine instance at boot time. These files should not be stored in the boot disk image or in a public repository. What is the most secure method to provide this data?",options:["Store the files in a public Cloud Storage bucket and download them with a startup script.","SSH into the instance after it boots and manually copy the files.","Embed the files directly into the startup script.","Pass the data as custom metadata to the instance during creation."],correct:3,explanation:"Instance metadata is a secure way to pass small amounts of configuration data to an instance. The data is accessible only from within the instance itself via the metadata server, and it's not part of the persistent disk image. This is the standard method for securely bootstrapping an instance.",wrongExplanations:{1:"Storing sensitive files in a public bucket is a major security risk.",2:"Manual copying is not automated and is inefficient for scalable deployments.",3:"Embedding large or sensitive files in a startup script is cumbersome and exposes the data in the instance's metadata, but using the dedicated metadata feature is cleaner."}},{id:432,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You need to view logs from all projects within a specific Folder in a single, centralized view. What should you do?",options:["Navigate to each project individually to view its logs.","This is not possible; logs can only be viewed on a per-project basis.","Create a Cloud Monitoring workspace and add all the projects to its metric scope.","Create a log sink at the Folder level to route all logs to a single BigQuery dataset or Cloud Storage bucket."],correct:3,explanation:"Log sinks can be configured at any level of the resource hierarchy (Organization, Folder, or Project). Creating a sink at the Folder level and setting its `includeChildren` property to true will capture all log entries from all projects within that folder and route them to a single, specified destination for centralized analysis.",wrongExplanations:{1:"A Monitoring workspace is for aggregating metrics, not logs.",2:"This is inefficient and does not provide the required centralized view.",3:"This is incorrect. Aggregated sinks are a core feature of Cloud Logging."}},{id:433,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"A service account in `project-a` needs to be able to read objects from a Cloud Storage bucket in `project-b`. What is the best practice for granting this cross-project access?",options:["Make the bucket in `project-b` public.","Set up VPC Peering between the projects' VPCs.","In `project-b`, grant the `Storage Object Viewer` role to the service account from `project-a` on the specific bucket.","Create a new service account in `project-b` with the required role and download its key to the application in `project-a`."],correct:2,explanation:"IAM principals (including service accounts) are global. You can grant a role to any principal on any resource, regardless of which project the principal or resource belongs to. The best practice is to directly grant the service account from `project-a` the specific, least-privilege role it needs on the resource in `project-b`.",wrongExplanations:{1:"Downloading and managing service account keys is a security risk and should be avoided. Direct role grants are more secure.",2:"Making the bucket public is a major security vulnerability.",3:"VPC Peering enables network connectivity, but it does not grant IAM permissions. Authorization is still handled by IAM."}},{id:434,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud SQL",question:"You are configuring a Cloud SQL for MySQL instance for high availability. What feature should you enable to ensure the instance can survive a zonal failure?",options:["Enable high availability (HA) configuration.","Increase the instance's vCPU and memory.","Create a read replica in another region.","Enable automated backups."],correct:0,explanation:"The high availability (HA) configuration for Cloud SQL provisions a primary instance in one zone and a standby instance in a different zone within the same region. Data is synchronously replicated between them. If the primary zone fails, Cloud SQL automatically fails over to the standby instance with no data loss, providing resilience against zonal failures.",wrongExplanations:{1:"Backups are for disaster recovery (restoring data), not for providing high availability (automatic failover).",2:"A cross-region read replica is for disaster recovery and read scaling in another region, not for automatic HA failover within the same region.",3:"Increasing instance size (vertical scaling) improves performance but does not improve availability."}},{id:435,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"Your managed instance group (MIG) is configured with an autoscaler. The autoscaler is not scaling up the group even though the instances are reporting high CPU utilization. What is a likely reason for this?",options:["The MIG's size is already at the maximum number of instances (`maxReplicas`) defined in the autoscaler policy.","The health check for the instance group is failing.","The autoscaler is configured to scale based on a schedule, not CPU.","You have not enabled the Cloud Monitoring API."],correct:0,explanation:"Every autoscaler policy has a minimum and maximum number of replicas. Even if the scaling signal (like CPU) exceeds the target, the autoscaler will not create new instances if the group has already reached its configured maximum size. This is a common issue to check when troubleshooting autoscaling.",wrongExplanations:{1:"This is a possible reason, but the most common constraint is the maximum size limit.",2:"Failing health checks cause the MIG to recreate instances (autohealing), but they do not prevent the autoscaler from adding new instances if the existing healthy instances are overloaded.",3:"The Monitoring API is enabled by default in most projects, and autoscaling relies on it. It's less likely to be the issue than a simple configuration limit."}},{id:436,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You want to find all Compute Engine instances in your project that have a specific network tag, `backend-server`. Which `gcloud` command should you use?",options:["gcloud compute instances list --tags backend-server","gcloud compute instances search --tags backend-server",'gcloud compute instances list --filter="tags.items=backend-server"',"gcloud compute instances list | grep backend-server"],correct:2,explanation:"The `gcloud` command-line tool has a powerful `--filter` flag that allows you to apply server-side filtering based on resource properties. The syntax for filtering on tags, which is a list, is `tags.items=[VALUE]`.",wrongExplanations:{1:"`search` is not a valid subcommand for `gcloud compute instances`.",2:"There is no `--tags` flag for the list command; filtering is done with the `--filter` flag.",3:"Using `grep` is client-side filtering. It is less efficient than using `--filter`, which performs the filtering on the server before returning the results."}},{id:437,domain:"Section 4: Configuring access and security",subdomain:"Organization policies",question:"Your company wants to restrict which public container images can be run on your GKE clusters. You only want to allow images from Google's curated base images (gcr.io/google-containers) and your company's private Artifact Registry. How can you enforce this?",options:["Use a Kubernetes admission controller like OPA Gatekeeper to validate image sources.","Manually review all Kubernetes manifests before they are deployed.","Use the 'Define allowed container image repositories' Organization Policy constraint (`constraints/container.trustedImageProjects`).","Create a custom IAM role that only allows pulling from specific repositories."],correct:2,explanation:"This is the exact use case for the `container.trustedImageProjects` Organization Policy constraint. It allows you to specify a list of Google Cloud project IDs from which container images can be deployed. Any attempt to deploy a pod with an image from an untrusted source will be blocked.",wrongExplanations:{1:"While an admission controller can also enforce this, the Organization Policy is a higher-level, simpler, and fully managed Google Cloud native solution.",2:"IAM controls permissions for *users* to perform actions, but it does not control the content (like the image source) of the resources they create.",3:"Manual review is not a scalable or reliable security control."}},{id:438,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You need to upload a very large file (50 GB) to a Cloud Storage bucket from a machine with an unstable internet connection. What is the most reliable way to perform this upload?",options:["Use the Cloud Console's file upload button.","Mount the bucket using Cloud Storage FUSE and copy the file.","Write a custom script to split the file into 1 GB chunks and upload them individually.","Use `gsutil` with the `-m` (parallel) and resumable upload feature."],correct:3,explanation:"`gsutil` is the command-line tool for Cloud Storage. By default, it performs resumable uploads for large files. If the upload is interrupted, you can run the same command again, and it will resume from where it left off. The `-m` flag enables parallel uploads, which can significantly speed up the process for large files by uploading multiple chunks simultaneously.",wrongExplanations:{1:"The Cloud Console upload is not designed for very large files or unstable connections and is likely to fail and require a full restart.",2:"Manually splitting the file is complex. `gsutil` handles this chunking and reassembly for you automatically with its parallel upload feature.",3:"Using FUSE over an unstable connection for a large file copy would be very slow and prone to errors."}},{id:439,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You want to create an alert that notifies you if the 99th percentile latency of your external Application Load Balancer exceeds 500ms. Which metric should you use for the alerting policy?",options:["The load balancer's `request_count` metric.","The backend instances' `cpu/utilization` metric.","A custom metric that your application writes to the Monitoring API.","The load balancer's `request_latencies` metric, using percentile aggregation."],correct:3,explanation:"The external Application Load Balancer automatically exports detailed metrics to Cloud Monitoring, including a distribution metric for request latencies. When creating an alerting policy, you can choose to base the condition on a specific percentile of this distribution (e.g., 50th, 95th, 99th), which is the correct way to monitor tail latency.",wrongExplanations:{1:"CPU utilization is an indirect and often unreliable indicator of request latency.",2:"Request count tells you the volume of traffic, not the performance of individual requests.",3:"While you could create a custom metric, it's unnecessary work. The load balancer provides this metric out of the box."}},{id:440,domain:"Section 4: Configuring access and security",subdomain:"Secret Manager",question:"Your application needs to access a new version of a database password that is stored in Secret Manager. The application code already fetches the secret by its name. What is the simplest way to roll out the new password to the application without changing its code?",options:["Create a new secret with the new password and update the IAM policy.","Store the new password in the application's environment variables.","Add the new password as a new version of the secret and configure the application's secret reference to point to the 'latest' alias.","Delete the old version of the secret."],correct:2,explanation:"Secret Manager supports versioning for secrets. You can add a new password as a new version. By having your application reference the secret using the 'latest' alias, it will automatically pick up the new version the next time it fetches the secret. This allows for seamless secret rotation without code changes or redeployments.",wrongExplanations:{1:"Creating a completely new secret would require changing the application's code to reference the new secret's name.",2:"Deleting the old version before all instances of the application have picked up the new one would cause an outage.",3:"Using environment variables is less secure than using Secret Manager, as they can be more easily exposed."}},{id:441,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You are logged into Cloud Shell and want to see which Google Cloud project is currently configured as the default for your `gcloud` commands. Which command should you run?",options:["gcloud info","gcloud projects list","gcloud auth list","gcloud config get-value project"],correct:3,explanation:"The `gcloud config` command group is used to manage your gcloud configurations. The `get-value` subcommand allows you to retrieve the value of a specific property. To get the currently configured project, you would ask for the `project` property.",wrongExplanations:{1:"`gcloud projects list` shows all projects you have access to, not the one that is currently configured as the default.",2:"`gcloud auth list` shows the accounts you are logged in with, not the default project.",3:"`gcloud info` provides general information about your gcloud installation, which includes the project but `get-value` is the more direct command."}},{id:442,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE",question:"You are deploying a containerized application to a GKE cluster. The application needs to mount a persistent disk that can be read and written to by a single pod at a time. The data must persist even if the pod is deleted and recreated. Which Kubernetes objects do you need to create?",options:["A PersistentVolumeClaim and a Deployment that references the claim.","A StatefulSet with a volumeClaimTemplates section.","A hostPath volume.","An Ephemeral Volume attached to the pod."],correct:0,explanation:"This describes the standard 'ReadWriteOnce' access mode for persistent storage in Kubernetes. The developer creates a PersistentVolumeClaim (PVC) to request storage. The GKE cluster will dynamically provision a Persistent Disk to satisfy this claim. The Deployment can then reference the PVC in its volume mounts, allowing the pods to use the disk. The data will persist independently of the pod's lifecycle.",wrongExplanations:{1:"A StatefulSet is used for stateful applications where each pod needs its own unique, persistent volume. For a single shared volume, a Deployment with a PVC is simpler.",2:"An Ephemeral Volume's lifecycle is tied to the pod. If the pod is deleted, the data is lost.",3:"A hostPath volume mounts a directory from the underlying node, which is not durable storage. If the pod is rescheduled to a different node, the data will not be available."}},{id:443,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"You have a MIG with autohealing enabled. The health check is configured to check for a response on TCP port 80 every 30 seconds. A VM in the group fails its health check three consecutive times. What action will the MIG take?",options:["It will send an alert to Cloud Monitoring but take no action.","It will stop the VM and attempt to restart it.","It will remove the VM from the load balancer's backend service but leave it running.","It will automatically delete the unhealthy VM and create a new one to replace it."],correct:3,explanation:"The purpose of autohealing is to maintain the health and availability of the application. When a VM is confirmed to be unhealthy (by failing a specified number of consecutive health checks), the MIG's autohealing policy will automatically recreate the instance. This involves deleting the failed VM and creating a new one based on the same instance template.",wrongExplanations:{1:"The standard autohealing action is to recreate, not just restart, the instance to ensure it comes up in a clean state.",2:"The MIG takes direct action. Sending an alert is a separate function you can configure in Cloud Monitoring, but it's not part of the autohealing process itself.",3:"While the load balancer will stop sending traffic to an unhealthy instance, the MIG's autohealing policy will go further and actually replace the instance."}},{id:444,domain:"Section 4: Configuring access and security",subdomain:"Security Command Center",question:"Security Command Center has reported a 'Public Cloud Storage ACL' finding for one of your buckets. What does this finding indicate?",options:["The bucket has an Access Control List (ACL) that grants access to `allUsers` or `allAuthenticatedUsers`, making it publicly accessible.","The bucket is not encrypted with a Customer-Managed Encryption Key (CMEK).","The bucket does not have Uniform Bucket-Level Access enabled.","The bucket does not have logging enabled for data access."],correct:0,explanation:"Security Health Analytics, a built-in service in Security Command Center, automatically scans for common misconfigurations. One of the most critical findings is when a bucket is made public through legacy ACLs. The 'Public Cloud Storage ACL' finding specifically identifies buckets that grant broad access to `allUsers` (anyone on the internet) or `allAuthenticatedUsers` (any authenticated Google account).",wrongExplanations:{1:"While CMEK usage might be a best practice, this specific finding is about public access, not the encryption method.",2:"Not using UBLA is a related misconfiguration that can lead to this, but the finding's title directly refers to the public ACL itself.",3:"While access logging is a good security practice, this finding is about access control, not auditing."}},{id:445,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Functions",question:"You have written a Cloud Function that is triggered by messages on a Pub/Sub topic. The function processes the message and can sometimes take up to 15 minutes to complete. When you deploy the function, it fails with a timeout error. What is the most likely cause?",options:["The Pub/Sub topic has too many messages.","The function's timeout setting is at its default value, which is less than 15 minutes.","The service account for the function does not have the correct permissions.","The function has a memory leak."],correct:1,explanation:"Cloud Functions have a configurable timeout, which defaults to 60 seconds. The maximum allowed timeout for an HTTP-triggered function is 9 minutes (or 60 minutes for Cloud Functions 2nd gen and event-driven functions). Since the processing takes 15 minutes, it is exceeding the default timeout, and you must explicitly increase it during deployment.",wrongExplanations:{1:"A high volume of messages would just trigger more concurrent instances of the function; it wouldn't cause a single invocation to time out.",2:"A permissions error would likely cause the function to fail immediately with an access denied error, not a timeout.",3:"A memory leak would likely cause the function to crash with an out-of-memory error, not a timeout."}},{id:446,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"How are IAM policies inherited through the Google Cloud resource hierarchy?",options:["Policies are not inherited; they must be applied at each level explicitly.","Policies are inherited downwards. A policy set at a Folder applies to all Projects and resources within that Folder.","Policies are inherited upwards. A policy set on a Project applies to its parent Folder and the Organization.","Only policies set at the Organization level are inherited."],correct:1,explanation:"The resource hierarchy allows for top-down inheritance of policies. If you grant a user the Viewer role at the Organization level, they will have Viewer permissions on all Folders and Projects within that Organization. Similarly, a policy on a Folder applies to all Projects under it. This allows for efficient management of broad access controls.",wrongExplanations:{1:"Inheritance is strictly top-down, not bottom-up.",2:"Inheritance is a key feature of the hierarchy.",3:"Policies can be set and inherited from any level (Organization, Folder, Project)."}},{id:447,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You need to perform maintenance on a specific node in your GKE Standard cluster. You want to safely evict all the user pods running on that node and prevent new pods from being scheduled on it, without terminating the node itself. Which `kubectl` command should you use first?",options:["kubectl stop node [NODE_NAME]","kubectl delete node [NODE_NAME]","kubectl drain [NODE_NAME]","kubectl cordon [NODE_NAME]"],correct:2,explanation:"The `kubectl drain` command is the standard, safe procedure for taking a node out of service. It performs two actions: first, it marks the node as unschedulable (cordons it), so no new pods are placed there. Second, it respects pod disruption budgets and gracefully evicts the existing pods, allowing them to be rescheduled on other available nodes.",wrongExplanations:{1:"`kubectl cordon` only marks the node as unschedulable; it does not evict the existing pods.",2:"`kubectl delete node` removes the node object from the Kubernetes API, which can lead to GKE's cluster manager recreating the underlying VM. It is a destructive action.",3:"`stop node` is not a valid `kubectl` command."}},{id:448,domain:"Section 4: Configuring access and security",subdomain:"VPC Security",question:"You have a VPC network with a default-allow-internal firewall rule. You need to create a more restrictive security posture where two subnets, `subnet-a` and `subnet-b`, cannot communicate with each other at all, but both can still communicate with a third shared `subnet-c`. What is the most effective way to do this?",options:["Delete the default-allow-internal firewall rule.","Move `subnet-a` and `subnet-b` to separate VPC networks.","Configure VPC Service Controls to isolate the subnets.","Create a deny-all firewall rule with a priority higher than the default-allow-internal rule, targeting traffic between `subnet-a` and `subnet-b` using network tags or service accounts."],correct:3,explanation:"Firewall rules are evaluated by priority. The default-allow-internal rule has a low priority (65534). To override it for specific traffic, you create a new rule with a higher priority (a lower number, e.g., 1000). This new rule would have a 'deny' action, with a source of one subnet and a destination of the other, effectively blocking communication between them while not affecting their ability to communicate with other subnets.",wrongExplanations:{1:"Moving subnets to new VPCs is a very disruptive and complex solution for a simple traffic isolation requirement.",2:"You cannot delete the default firewall rules. Also, deleting it would break all internal communication, which is not what is required.",3:"VPC Service Controls protect managed services; they do not control network traffic between subnets within a VPC."}},{id:449,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You are creating a new Compute Engine instance and want to ensure that it is created on a physical server that is not shared with any other customer's VMs. Which feature should you use?",options:["Confidential Computing","Shielded VMs","Sole-tenant nodes","A compute-optimized machine type"],correct:2,explanation:"Sole-tenant nodes are physical Compute Engine servers dedicated to your project. They ensure that your VMs have exclusive access to the underlying hardware and are not co-located with workloads from other customers. This is often used for security, compliance (e.g., licensing), or performance isolation requirements.",wrongExplanations:{1:"Shielded VMs provide verifiable integrity of the boot process, but they still run on multi-tenant hardware.",2:"Confidential Computing encrypts data while it is in use (in memory), but the VMs still run on multi-tenant hardware.",3:"Machine type determines the vCPU and memory configuration, not the tenancy of the underlying physical host."}},{id:450,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Shell",question:"What is a key feature of Google Cloud Shell?",options:["It is a downloadable client for managing Google Cloud from your local machine.","It is a browser-based shell environment with `gcloud` and other common utilities pre-installed and authenticated.","It is a tool for monitoring and debugging applications.","It is a dedicated, persistent VM for running production workloads."],correct:1,explanation:"Cloud Shell provides a temporary Compute Engine VM instance accessible directly from your browser. It comes with the `gcloud` CLI, Docker, Terraform, `kubectl`, and many other development tools pre-installed. Your authentication credentials and project context are automatically configured, making it the quickest way to start managing your Google Cloud resources.",wrongExplanations:{1:"The downloadable client is the Google Cloud SDK. Cloud Shell is browser-based.",2:"Cloud Shell is for interactive, administrative tasks, not for hosting production applications. Its instance is ephemeral.",3:"Tools for monitoring and debugging are Cloud Monitoring, Logging, etc. Cloud Shell is a management environment."}},{id:451,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You want to find all log entries from a specific Compute Engine instance. Which field in the Logs Explorer should you filter by?",options:["jsonPayload.instanceName","source_instance","resource.labels.instance_id","gce_instance"],correct:2,explanation:"In Cloud Logging, logs from Google Cloud services have a `resource` field that describes the source of the log. For Compute Engine, the resource type is `gce_instance`, and it has labels that identify the specific instance, such as `instance_id` and `zone`. Filtering on `resource.labels.instance_id` is the standard and correct way to isolate logs from a particular VM.",wrongExplanations:{1:"`source_instance` is not a standard field.",2:"`jsonPayload` contains the actual log message content. While it might contain the instance name, filtering on the resource label is the correct way to query the log's metadata.",3:"`gce_instance` is the resource *type*, not the field you would filter on for a specific instance ID."}},{id:452,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You have a Cloud Storage bucket that will store data that is accessed frequently for the first 30 days, then infrequently for the next 90 days, and then rarely after that. You want to automatically manage the storage costs of this data. What should you configure?",options:["A separate bucket for each access pattern.","A Cloud Function to move the objects between storage classes.","Object versioning.","A lifecycle management policy on the bucket."],correct:3,explanation:"Cloud Storage lifecycle management is a feature that lets you define rules to automatically take action on objects based on their age or other conditions. You can create a rule to transition objects from Standard to Nearline storage after 30 days, and then from Nearline to Coldline or Archive after another 90 days. This automates the cost optimization process.",wrongExplanations:{1:"A Cloud Function is a valid but overly complex solution. Lifecycle policies are the built-in, declarative way to achieve this.",2:"Object versioning is for protecting against deletes/overwrites, not for managing storage class and cost.",3:"Using separate buckets is a manual and difficult-to-manage approach. A single bucket with a lifecycle policy is the correct design."}},{id:453,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the purpose of the `iam.serviceAccountUser` role?",options:["It is a role that you assign to service accounts to let them use services.","It allows a user to manage the IAM policies of a service account.","It allows a user to run jobs and act as (impersonate) a service account.","It allows a user to create and manage service accounts."],correct:2,explanation:"The Service Account User role is a critical part of service account security. It does not grant permissions to manage the service account itself, but rather to *use* it. A user with this role can impersonate the service account to get short-lived credentials, or they can attach the service account to a resource like a Compute Engine VM.",wrongExplanations:{1:"The role for managing service accounts is `iam.serviceAccountAdmin`.",2:"You assign roles like `Storage Object Viewer` to a service account to let it use other services. The `serviceAccountUser` role is granted *to users*, not to the service account itself.",3:"Managing the IAM policy of a service account is done by the `iam.serviceAccountAdmin` role."}},{id:454,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"You are creating a new VPC network. What is a key reason to choose a 'Custom mode' VPC over an 'Auto mode' VPC?",options:["Custom mode provides higher network throughput.","Custom mode automatically creates a set of useful firewall rules for you.","Custom mode is required for using Compute Engine.","Custom mode allows you to define your own IP address ranges for subnets, which is essential for avoiding conflicts with on-premises networks."],correct:3,explanation:"Auto mode VPCs automatically create a subnet in each Google Cloud region with a predefined, non-configurable IP range. This can easily lead to IP range overlap if you need to connect your VPC to an on-premises network via VPN or Interconnect. Custom mode gives you full control over which subnets are created and what their IP ranges are, which is a best practice for all production environments.",wrongExplanations:{1:"Auto mode creates the default firewall rules. In Custom mode, you start with a minimal set of rules and must create your own.",2:"Both modes support Compute Engine. Auto mode is often used by beginners for simplicity.",3:"The VPC mode does not affect the available network throughput."}},{id:455,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"An alerting policy has been configured with a condition and a notification channel. The condition of the alert is met, and an incident is created in Cloud Monitoring. After 24 hours, the condition is still met. What is the default behavior of the alerting policy?",options:["The incident will automatically close after 24 hours.","A new notification will be sent every 5 minutes until the incident is acknowledged.","A new, separate incident will be created.","The incident will remain open, but no new notifications will be sent unless the incident is closed and re-opened."],correct:3,explanation:"By default, an alerting policy sends a notification only when an incident is first opened. It will not send repeated notifications for an ongoing, open incident. The incident will stay open as long as the condition is met. To get repeated notifications, you would need to configure that explicitly in the policy's documentation/notification settings.",wrongExplanations:{1:"This describes a 'nagging' or re-notification feature, which is not the default behavior.",2:"Incidents only close when the condition is no longer met or if they are manually closed. There is a maximum auto-close duration of 7 days if data stops arriving, but not 24 hours for an active condition.",3:"A new incident is only created if the previous one was closed and the condition is met again."}},{id:456,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"Your company has negotiated a special discounted rate for a specific Google Cloud service. What is the name for this type of discount?",options:["A Private Pricing Agreement.","A Sustained Use Discount.","A Committed Use Discount.","A Promotional Credit."],correct:0,explanation:"A Private Pricing Agreement (or Private Offer) is a custom deal negotiated between a customer and Google (or a partner via the Marketplace) that provides a special price for a specific product, which is not publicly available.",wrongExplanations:{1:"Sustained Use Discounts are automatic discounts applied to Compute Engine for running instances for a significant portion of the month.",2:"Committed Use Discounts are discounts you receive in exchange for committing to a certain level of resource usage (e.g., vCPUs or RAM) for a 1 or 3-year term.",3:"Promotional credits are one-time credits applied to an account, often for free trials."}},{id:457,domain:"Section 4: Configuring access and security",subdomain:"Cloud KMS",question:"What is the primary function of Cloud Key Management Service (KMS)?",options:["To manage cryptographic keys and perform encryption and decryption operations.","To store SSL certificates for load balancers.","To manage SSH keys for Compute Engine instances.","To store and manage service account keys."],correct:0,explanation:"Cloud KMS is a centralized service for creating, importing, managing, and using cryptographic keys. Other Google Cloud services (like Cloud Storage, BigQuery, and Persistent Disk) can integrate with KMS to use these keys for encrypting data (a pattern known as Customer-Managed Encryption Keys or CMEK). You can also call the KMS API directly to perform encryption/decryption.",wrongExplanations:{1:"Service account keys are managed in IAM, although the best practice is to store sensitive keys in a service like Secret Manager, which can use KMS for encryption.",2:"SSH keys are managed as part of Compute Engine's metadata.",3:"SSL certificates are managed in Certificate Manager or directly on the load balancer."}},{id:458,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Artifact Registry",question:"Your organization is standardizing on using Artifact Registry to store all of its container images. You need to create a new repository to store Docker images for your web-app. What type of repository should you create?",options:["An npm repository.","A generic artifact repository.","A Docker repository.","A Maven repository."],correct:2,explanation:"Artifact Registry is a universal repository manager that supports multiple package formats. When you create a repository, you must specify the format of the artifacts it will store. For Docker container images, you must choose the 'Docker' format. This configures the repository to understand the Docker V2 API for `docker push` and `docker pull` commands.",wrongExplanations:{1:"Maven is a repository format for Java artifacts.",2:"npm is a repository format for Node.js packages.",3:"While Artifact Registry is a generic manager, you must specify the format for each repository you create."}},{id:459,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Health",question:"You have received an email from Google Cloud stating that one of your Compute Engine instances is scheduled for host maintenance in the next week. The instance's live migration setting is turned on. What do you need to do?",options:["You must request a maintenance extension to avoid downtime.","You need to migrate the instance's persistent disk to a new host.","No action is required. Google will automatically live migrate the instance to a new host with no downtime.","You must manually stop and restart the instance during the maintenance window."],correct:2,explanation:"Live migration is a feature of Compute Engine that allows Google to move a running VM from one physical host to another without any impact on the application or the need for a reboot. When live migration is enabled (the default for most instances), Google handles the maintenance automatically, and you typically do not need to take any action.",wrongExplanations:{1:"This is only necessary if live migration is turned off for the instance.",2:"The persistent disk is network-attached and does not need to be manually migrated; it will be reattached automatically after the live migration.",3:"An extension is not necessary because live migration is designed to be a zero-downtime event."}},{id:460,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"AlloyDB",question:"Which of the following is a primary advantage of using AlloyDB for PostgreSQL over a self-managed PostgreSQL instance on a Compute Engine VM?",options:["Fully managed service with automated backups, patching, and high availability.","Complete control over the operating system and database configuration.","Ability to install any third-party PostgreSQL extension.","Lower cost for small, non-critical workloads."],correct:0,explanation:"AlloyDB is a fully managed database service. This means Google handles the complex and time-consuming administrative tasks such as provisioning infrastructure, setting up high availability, managing backups, and applying security patches. This allows teams to focus on their application rather than on database administration.",wrongExplanations:{1:"Complete control is an advantage of a self-managed instance, not AlloyDB. The trade-off for a managed service is giving up some control for operational ease.",2:"For very small workloads, a small Compute Engine VM might be cheaper, but it comes with a high operational cost. AlloyDB is designed for performance and availability, which comes at a higher price point than a minimal VM.",3:"While AlloyDB supports many popular extensions, a self-managed instance gives you the freedom to install any extension, which might be a requirement for some legacy applications. This is a point of flexibility for self-managed, not an advantage of AlloyDB."}},{id:461,domain:"Section 4: Configuring access and security",subdomain:"Cloud NGFW policies",question:"A network administrator wants to create a global network firewall policy that can be applied to all VPCs across the entire organization. What should they configure?",options:["A network firewall policy in each VPC.","A hierarchical firewall policy at the Organization root.","A Shared VPC network.","A Cloud Armor policy."],correct:1,explanation:"Hierarchical firewall policies are designed for this exact use case. They are configured at the Organization or Folder level in the resource hierarchy and are inherited by all VPCs below them. This allows for centralized enforcement of baseline security rules (e.g., blocking known malicious IPs) across the entire enterprise.",wrongExplanations:{1:"Configuring policies in each VPC is decentralized and does not meet the requirement for a single, global policy.",2:"Cloud Armor is a WAF for load balancers, not a general-purpose firewall for VPCs.",3:"A Shared VPC centralizes network management but does not in itself apply a global firewall policy."}},{id:462,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"You are trying to diagnose a networking issue between two VMs. You suspect a firewall rule is blocking traffic. How can Gemini Cloud Assist help you troubleshoot this in the Cloud Console?",options:["Look at the VPC flow logs.","Ask Gemini to write a network diagnostic script.","Run `gcloud compute ssh` into the VM.","Ask Gemini: 'Why can't my VM named 'frontend-1' connect to 'backend-1' on port 5432?'"],correct:3,explanation:"Gemini Cloud Assist is context-aware. When troubleshooting in the console, you can ask it natural language questions about your resources. It can analyze your current project's configuration, including firewall rules and network tags, to provide a specific explanation for why traffic might be blocked and suggest the exact firewall rule change needed to fix it.",wrongExplanations:{1:"SSHing into the VM allows you to test connectivity (e.g., with `ping` or `telnet`), but it doesn't directly tell you *why* it's failing at the cloud network level.",2:"VPC flow logs are a valid but more complex troubleshooting tool. You would have to manually search and interpret the logs. Gemini can automate this analysis for you.",3:"While Gemini can write scripts, its primary troubleshooting value in the console is its ability to directly analyze your project's configuration and provide an immediate answer."}},{id:463,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"When using Query Insights, you notice that a particular query has a very high 'Rows Scanned' value compared to the 'Rows Returned' value. What does this indicate?",options:["The query is inefficient and is reading much more data than necessary, likely due to a missing index or un-optimized `WHERE` clause.","The database table is well-clustered.","The query is performing well because it is scanning data quickly.","The user who ran the query has overly broad permissions."],correct:0,explanation:"A large discrepancy between rows scanned and rows returned is a key indicator of an inefficient query. It means the database had to read a large number of rows from disk to find the small number of rows that actually matched the query's conditions. This often points to a full table scan where a more efficient index scan could have been used.",wrongExplanations:{1:"High scan count is a sign of poor performance, not good performance.",2:"If the table were well-clustered for this query, the number of rows scanned would be much closer to the number of rows returned.",3:"Query Insights is a performance tool; it provides no information about IAM permissions."}},{id:464,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"Your team is building a new application and prefers a flexible, schema-less data model. The application will store JSON-like documents and needs to scale automatically from zero to millions of users. Which database service is the best choice?",options:["Memorystore","Cloud SQL","Cloud Spanner","Firestore"],correct:3,explanation:"Firestore is a fully managed, serverless, NoSQL document database. It is designed for storing, syncing, and querying data for mobile, web, and IoT applications. Its key features include a flexible, JSON-like data model, automatic scaling, and real-time data synchronization, which perfectly match the requirements.",wrongExplanations:{1:"Cloud SQL is a relational database and requires a predefined schema. It does not use a document-based data model.",2:"Cloud Spanner is a relational database with a schema, not a schema-less document store.",3:"Memorystore is an in-memory key-value store, not a durable document database suitable for being the primary data store for an application."}},{id:465,domain:"Section 4: Configuring access and security",subdomain:"Security Command Center",question:"You want to proactively scan your container images stored in Artifact Registry for known security vulnerabilities (CVEs) before they are deployed to GKE. Which feature, which integrates with Security Command Center, should you enable?",options:["Container Analysis","Cloud Armor","Web Security Scanner","Binary Authorization"],correct:0,explanation:"Container Analysis is the service that provides vulnerability scanning for container images in Artifact Registry or Container Registry. It scans your images for known CVEs in OS packages and application libraries. The findings are then surfaced in the Security Command Center dashboard, allowing you to identify and remediate vulnerabilities early in the development lifecycle.",wrongExplanations:{1:"Cloud Armor is a WAF for protecting running applications from network attacks; it does not scan container images at rest.",2:"Binary Authorization is a deployment-time security control that *enforces* policies based on attestations (e.g., ensuring an image has been scanned and has no critical vulnerabilities), but the scanning itself is done by Container Analysis.",3:"Web Security Scanner is for scanning running web applications for vulnerabilities like XSS, not for scanning container images."}},{id:466,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Deployment Manager",question:"You want to define a set of Google Cloud resources (a VM, a firewall rule, and a Cloud Storage bucket) as code in a declarative template file. You want to use Google's native Infrastructure as Code service. Which service should you use?",options:["Terraform","Cloud Build","Config Connector","Cloud Deployment Manager"],correct:3,explanation:"Cloud Deployment Manager is Google Cloud's native infrastructure deployment service that allows you to specify all the resources needed for your application in a declarative format using YAML. You can create templates and reuse them to provision infrastructure in a repeatable and predictable way.",wrongExplanations:{1:"Terraform is a very popular third-party IaC tool, but the question specifically asks for Google's *native* service.",2:"Config Connector is for managing GCP resources via the Kubernetes API, which is a different workflow.",3:"Cloud Build is a CI/CD service for building and deploying code, not for declarative infrastructure management."}},{id:467,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Load balancing",question:"You have an application running on GKE that needs to be exposed to the internet via HTTP. You want to manage traffic using native Kubernetes resources. Which resource should you create to have GKE automatically provision and configure an external Application Load Balancer?",options:["A Kubernetes Service of type LoadBalancer.","A Gateway object from the Gateway API.","A Cloud Armor policy.","A Kubernetes Ingress object."],correct:3,explanation:"In GKE, creating a Kubernetes Ingress object is the standard way to expose HTTP and HTTPS routes from outside the cluster to services within the cluster. The GKE Ingress controller watches for these Ingress objects and automatically provisions and configures a Google Cloud external Application Load Balancer to handle the routing.",wrongExplanations:{1:"Creating a Service of type LoadBalancer will provision an external *Network* Load Balancer (Layer 4), which is not suitable for HTTP routing and does not provide features like path-based routing or SSL termination.",2:"A Cloud Armor policy attaches to a load balancer but does not create it.",3:"The Gateway API is a newer, more expressive set of APIs for service networking, but Ingress is the long-standing and more commonly tested resource for this function."}},{id:468,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You want to be alerted if your project's daily spending, as reported by Cloud Billing, exceeds $100. How can you set up this alert?",options:["This cannot be done in Cloud Monitoring; you must create a budget and alert in Cloud Billing.","Create a logs-based metric from the billing audit logs.","Create an alerting policy in Cloud Monitoring based on the `billing/quota` metric.","Write a custom application that calls the Billing API every minute and sends an email."],correct:0,explanation:"While Cloud Monitoring is the primary tool for performance metrics, cost control and alerting is the specific responsibility of the Cloud Billing service. The correct and simplest way to achieve this is to go to the Billing section, create a budget for your project, and set an alert threshold to send a notification when spending reaches a certain amount.",wrongExplanations:{1:"There is no such metric for direct cost alerting in Cloud Monitoring.",2:"Billing data is not typically sent to logs in a way that would be suitable for creating logs-based metrics for alerting.",3:"This is an overly complex, custom solution for a feature that is provided out-of-the-box by Cloud Billing."}},{id:469,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the purpose of a Google Group in the context of IAM?",options:["To act as a service account for a group of applications.","To create a shared email inbox for a team.","To define a custom set of permissions.","To grant a set of permissions to multiple users at once, simplifying user management."],correct:3,explanation:"Using Google Groups is a best practice for managing IAM policies. Instead of assigning roles to individual users one by one, you can create a group (e.g., `gcp-project-admins@yourcompany.com`), grant the necessary roles to that group, and then simply add or remove users from the group. This decouples user management from policy management and is much more scalable.",wrongExplanations:{1:"While a Google Group does provide a mailing list address, its primary function in IAM is as a collection of principals for role assignment.",2:"A custom set of permissions is a custom role, not a group.",3:"A group is a collection of user accounts, not a non-human identity like a service account."}},{id:470,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to run a high-performance computing (HPC) workload that requires the lowest possible network latency between the participating VMs. Which feature should you use when creating the VMs?",options:["Use a premium network service tier.","Place the VMs in a sole-tenant node group.","Choose VMs with the highest vCPU count.","Create the VMs as a compact placement policy."],correct:3,explanation:"A compact placement policy is a feature that instructs Compute Engine to place the specified VMs on physical hardware that is in close proximity to each other. This reduces the network distance between the VMs, resulting in significantly lower inter-VM network latency, which is critical for tightly coupled HPC applications.",wrongExplanations:{1:"Sole-tenancy provides hardware isolation, but it does not guarantee that the nodes within the group are physically close to each other for low latency.",2:"The premium network tier optimizes for traffic between your VMs and the internet, not for latency *between* VMs within a zone.",3:"A high vCPU count provides more processing power but does not affect the network latency between instances."}},{id:471,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"A security administrator wants to prevent developers from creating service account keys, as they want to enforce the best practice of attaching service accounts to resources directly. Which Organization Policy constraint should they enforce?",options:["`compute.vmExternalIpAccess`","`iam.disableServiceAccountKeyCreation`","`iam.allowedServiceAccountKeys`","`iam.serviceAccountUser`"],correct:1,explanation:"The `iam.disableServiceAccountKeyCreation` constraint is designed for this exact purpose. When this boolean constraint is enforced on a project, folder, or organization, it blocks all API calls that attempt to create new external service account keys, forcing developers to use more secure methods like impersonation or attaching service accounts to VMs.",wrongExplanations:{1:"This is not a valid constraint name.",2:"This constraint restricts the creation of VMs with external IPs; it has nothing to do with service account keys.",3:"This is an IAM role, not an Organization Policy constraint."}},{id:472,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"What is a log sink inclusion filter used for?",options:["To specify which log entries should be excluded from the sink.","To specify which log entries should be exported by the sink.","To define the IAM permissions for the sink's destination.","To set the retention period for the exported logs."],correct:1,explanation:"A log sink routes log entries to a destination. The inclusion filter uses the Logging query language to define which logs should be captured and routed. For example, you could create a filter `severity >= ERROR` to create a sink that only exports logs with a severity of ERROR or higher.",wrongExplanations:{1:"To exclude logs, you use an exclusion filter. The inclusion filter defines what to send.",2:"IAM permissions for the destination are managed on the sink's service account, not in the filter.",3:"Retention is configured at the destination (e.g., the Cloud Storage bucket), not in the sink's filter."}},{id:473,domain:"Section 4: Configuring access and security",subdomain:"Identity-Aware Proxy (IAP)",question:"You have configured IAP to protect a web application running on App Engine. What happens when a user who is not on the IAP access list tries to access the application's URL?",options:["The request will reach the application, which is then responsible for denying access.","IAP will block the request and show the user a Google-branded access denied screen, without the request ever reaching the application.","The user will see a generic 404 Not Found error.","The user will be prompted to request access from the project owner."],correct:1,explanation:"IAP acts as an authenticating and authorizing proxy. It sits in the request path before your application. It intercepts all requests, checks the user's identity against the IAM policy, and if the user is not authorized, it blocks the request immediately. The application code is never executed for unauthorized users.",wrongExplanations:{1:"This is incorrect. The core benefit of IAP is that it prevents unauthorized requests from ever reaching your backend.",2:"This is not a feature of IAP. Access management is done through standard IAM role grants.",3:"The user will see a 403 Access Denied error, not a 404 Not Found error."}},{id:474,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"You need to store a massive dataset (terabytes to petabytes) of wide-column data, such as time-series data from IoT devices. The workload requires very high throughput for both reads and writes, and low latency is critical. Which database is designed for this use case?",options:["Cloud SQL","Cloud Bigtable","BigQuery","Firestore"],correct:1,explanation:"Cloud Bigtable is a fully managed, scalable NoSQL wide-column database. It is the same database that powers many core Google services like Search and Maps. It's specifically designed for large analytical and operational workloads with high throughput and low latency, making it ideal for time-series, IoT, and financial data.",wrongExplanations:{1:"BigQuery is an analytical data warehouse designed for SQL queries and analysis, not for low-latency point reads and writes required by a live application.",2:"Cloud SQL is a relational database and cannot handle the scale or data model required.",3:"Firestore is a document database and is not optimized for the massive scale and wide-column data model that Bigtable excels at."}},{id:475,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Error Reporting",question:"Cloud Error Reporting has grouped several different error messages from your application into a single error group. What is the most likely reason for this?",options:["The errors all came from the same user.","The errors all originated from the same location in the application's source code (i.e., they have similar stack traces).","The errors all have the same severity level.","The errors all occurred at the same time."],correct:1,explanation:"The primary mechanism Error Reporting uses for grouping is by analyzing the stack trace. It identifies the root cause of an exception and groups different error messages that share a similar causal stack trace. This is powerful because a single bug (e.g., a null pointer exception) can manifest with slightly different error messages, and Error Reporting correctly identifies them as a single issue.",wrongExplanations:{1:"Time is a factor in viewing errors, but it's not the primary grouping mechanism.",2:"Severity is not used for grouping.",3:"User information is not used for grouping."}},{id:476,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"You are the billing administrator for your organization. A team has created a new project, but they are unable to enable any APIs or create resources because it is not linked to a billing account. What should you do in the Cloud Console?",options:["Submit a support ticket to Google to have the project linked.","Create a new billing account just for this project.","Navigate to the Billing section, select the project, and link it to your organization's active billing account.","Grant the project's creator the 'Billing Account Administrator' role."],correct:2,explanation:"A project must be associated with an active billing account before it can use any billable Google Cloud services. As the billing administrator, the standard procedure is to go to the Billing Account management page, find the unbilled project, and explicitly link it to the appropriate billing account.",wrongExplanations:{1:"Granting a role to the user doesn't link the project. The linking is a separate administrative action.",2:"Creating a new billing account is unnecessary if the organization already has one. This would also complicate billing management.",3:"This is a self-service action that a billing administrator can and should perform directly."}},{id:477,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"Which of the following services can be protected by a VPC Service Controls perimeter?",options:["Compute Engine and Google Kubernetes Engine","BigQuery, Cloud Storage, and Cloud SQL","Cloud Identity and IAM","Cloud DNS and Cloud Load Balancing"],correct:1,explanation:"VPC Service Controls are designed to protect Google-managed services that store data and have public API endpoints. This includes major data services like BigQuery, Cloud Storage, Pub/Sub, Cloud SQL, and Spanner. It does not apply to foundational infrastructure like Compute Engine or networking services, which are controlled by VPC firewalls and IAM.",wrongExplanations:{1:"Compute Engine and GKE are not directly protected by perimeters. You protect the data services they access.",2:"These are networking services and are not protected by perimeters.",3:"These are identity and access management services and are not protected by perimeters."}},{id:478,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You have a Compute Engine instance that runs a nightly batch job. You want to ensure the instance is only running when it is needed to save costs. What is the most efficient way to automate this?",options:["Keep the instance running at all times but use an E2 machine type.","Configure an autoscaler to set the instance group size to 0 when CPU is low.","Manually start and stop the instance every day from the Cloud Console.","Use Cloud Scheduler to send a message to a Pub/Sub topic that triggers a Cloud Function to start and stop the instance."],correct:3,explanation:"This is a common serverless automation pattern. Cloud Scheduler is a managed cron service that can trigger an event on a schedule. You can have it publish a message to two different Pub/Sub topics (e.g., `start-instance` and `stop-instance`) at the desired times. These topics can then trigger two simple Cloud Functions that use the Compute Engine API to start or stop the target instance. This is a fully automated and serverless solution.",wrongExplanations:{1:"Keeping the instance running is the opposite of the goal, which is to save costs by only running it when needed.",2:"Manual operations are not automated and are prone to being forgotten.",3:"An autoscaler is for managing a group of instances based on load, not for scheduling a single instance to turn on and off at specific times."}},{id:479,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You are trying to troubleshoot a pod in GKE that is stuck in the `ImagePullBackOff` state. What is the most likely cause of this issue?",options:["The cluster's CNI (Container Network Interface) plugin is misconfigured.","The GKE node cannot pull the container image specified in the pod manifest, likely due to an incorrect image name or insufficient permissions.","The pod does not have enough CPU or memory resources requested to start.","A readiness probe for the pod is failing."],correct:1,explanation:"The `ImagePullBackOff` status means that Kubernetes tried to pull the container image from the specified registry but failed. It will keep retrying with an increasing back-off delay. Common reasons for this failure include a typo in the image name or tag, or the GKE node's service account not having permission to read from the image repository (e.g., a private Artifact Registry).",wrongExplanations:{1:"Insufficient resources would likely result in a `Pending` state (if no node can fit it) or a crash after starting, not an image pull failure.",2:"A CNI issue would likely affect pod networking after the container starts, not the image pull process.",3:"A failing readiness probe would cause the pod to not receive traffic, but it wouldn't prevent the container image from being pulled in the first place."}},{id:480,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the purpose of an IAM Condition?",options:["To trigger an alert when a user performs a specific action.","To define the set of permissions included in a custom role.","To add constraints to a role binding, such as making it temporary or only allowing access to resources with specific names or labels.","To enforce a security posture across an organization, such as restricting resource locations."],correct:2,explanation:"IAM Conditions add an extra layer of attribute-based access control to role bindings. A role binding grants a role to a principal on a resource. A condition adds a constraint to that binding, for example, by specifying that the access is only valid for a certain time period, or only applies to VMs whose names start with `prod-`, or only from a specific IP range.",wrongExplanations:{1:"The set of permissions in a role is its definition. A condition applies to the *binding* of that role, not the role itself.",2:"Enforcing broad constraints like resource location is the job of Organization Policies.",3:"Alerting is done through Cloud Audit Logs and Cloud Monitoring, not IAM Conditions."}},{id:481,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cost management",question:"You are running a predictable, steady-state workload on Compute Engine 24/7. What is the best way to significantly reduce your compute costs for this workload?",options:["Choose the E2 machine series.","Purchase a 1-year or 3-year Committed Use Discount (CUD) for the required vCPU and memory.","Rely on automatic Sustained Use Discounts.","Use Spot VMs for all instances."],correct:1,explanation:"Committed Use Discounts provide the largest discounts (up to 70%) in exchange for committing to pay for a certain amount of vCPU and memory for a 1 or 3-year term, regardless of whether you use them. For a workload that is stable and runs 24/7, a CUD provides a guaranteed, significant cost reduction.",wrongExplanations:{1:"Spot VMs are for interruptible, fault-tolerant workloads, not for a steady-state workload that needs to run 24/7.",2:"Sustained Use Discounts are automatic but provide a much smaller discount than a CUD.",3:"While the E2 series is cost-effective, the discount from a CUD on a standard machine series will be much greater."}},{id:482,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Build",question:"Your `cloudbuild.yaml` file defines several steps that need to be run in a specific order. How does Cloud Build handle the execution of these steps?",options:["It runs all steps in parallel to speed up the build.","You must explicitly define dependencies between steps using the `waitFor` keyword.","It runs the steps in alphabetical order based on the step's `id`.","By default, it runs the steps serially in the order they are defined in the file."],correct:3,explanation:"By default, the steps in a `cloudbuild.yaml` file are executed serially, one after the other, in the order they appear in the list. The build will only proceed to the next step if the previous one completes successfully.",wrongExplanations:{1:"To run steps in parallel, you must explicitly configure dependencies using `waitFor`.",2:"The `waitFor` keyword is used to explicitly control dependencies and allow for parallel execution, but the default behavior is serial.",3:"Execution order is based on the list order in the YAML file, not on the `id` field."}},{id:483,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"What is an 'uptime check' in Cloud Monitoring used for?",options:["To check if your project has exceeded its quota.","To periodically send a request to a URL, IP, or TCP port to verify that a service is responsive.","To verify that a user has the correct IAM permissions.","To monitor the CPU and memory utilization of a VM."],correct:1,explanation:"Uptime checks are a form of black-box monitoring. They simulate a user's request from various locations around the world to check if your application is available and responding correctly. You can configure checks for HTTP, HTTPS, and TCP endpoints, and then create alerting policies based on the success or failure of these checks.",wrongExplanations:{1:"CPU and memory are system metrics collected by the Monitoring agent (white-box monitoring), not by an uptime check.",2:"Quota monitoring is a separate feature within Monitoring.",3:"IAM permissions are not checked by Monitoring."}},{id:484,domain:"Section 4: Configuring access and security",subdomain:"Binary Authorization",question:"You want to enforce a policy in your GKE cluster that only allows container images that have been approved by your QA team to be deployed. How can you automate this enforcement at deployment time?",options:["Use Cloud Armor to block deployments.","Use Binary Authorization with attestations.","Use an Organization Policy to restrict image repositories.","Use IAM roles to restrict who can deploy."],correct:1,explanation:"Binary Authorization is a deployment-time security control. You can create a policy that requires one or more 'attestations' for an image before it can be deployed. Your CI/CD pipeline can be configured so that after the QA team validates a build, it creates a cryptographic signature (an attestation) for that specific image digest. The Binary Authorization enforcer in GKE will then block any deployment attempt of an image that lacks this required attestation.",wrongExplanations:{1:"Restricting repositories is a good first step, but it doesn't verify that a specific image within that repository has passed QA.",2:"Cloud Armor is a network security tool, not a deployment control tool.",3:"IAM controls who can perform the deploy action, but it doesn't validate the content of what they are deploying."}},{id:485,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"What happens when you enable 'Requester Pays' on a Cloud Storage bucket?",options:["The user who downloads the data from the bucket pays for the network egress charges, not the bucket owner.","The user who uploads data to the bucket pays for the storage costs.","All users must have a service account to access the bucket.","The bucket becomes publicly accessible."],correct:0,explanation:"Ordinarily, the owner of a bucket pays for all costs associated with it, including network egress when someone downloads data. The 'Requester Pays' feature flips this for network and data access charges. The person or service making the request must have a billing-enabled project and they will be billed for the data they download. This is often used for sharing large datasets where you want to provide the data but not pay for everyone's download costs.",wrongExplanations:{1:"Storage costs are always paid by the bucket owner.",2:"Access is still controlled by IAM. Requester Pays only changes the billing model for access charges.",3:"It does not make the bucket public; IAM permissions still apply."}},{id:486,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"You have been given the `roles/resourcemanager.projectCreator` role at the Organization level. What does this allow you to do?",options:["Create new Folders within the Organization.","Become the Owner of any existing project in the Organization.","Create new Google Cloud projects within the Organization.","Manage the billing account for the Organization."],correct:2,explanation:"The `roles/resourcemanager.projectCreator` role grants the single permission `resourcemanager.projects.create`. This allows the user to create new projects. By default, when a user creates a project, they are automatically granted the Owner role for that new project.",wrongExplanations:{1:"Creating folders requires the `roles/resourcemanager.folderCreator` role.",2:"This role does not grant any permissions on existing projects.",3:"Managing billing requires a `billing. role."}},{id:487,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Health",question:"You are looking at the 'Recommendations' page in the Google Cloud Console and see a recommendation to 'Rightsize VM instance' for one of your VMs. What does this mean?",options:["The VM's operating system is out of date and needs to be patched.","The VM has been consistently underutilized, and you could save money by switching to a smaller machine type.","The VM is running in a region with high carbon emissions, and you should move it to a greener region.","The VM does not have the Monitoring agent installed."],correct:1,explanation:"The Active Assist Recommender uses machine learning to analyze your resource usage patterns. A 'rightsizing' recommendation for a VM means it has observed that the VM's CPU and/or memory has been very low for a significant period. It will suggest a smaller, cheaper machine type that could handle the workload, thus saving you money.",wrongExplanations:{1:"OS patching recommendations come from the OS Config service.",2:"Carbon footprint recommendations are a different category of recommendation.",3:"Agent installation is a different type of recommendation, often related to observability."}},{id:488,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your company has acquired another organization that uses Google Cloud. You need to migrate their 50 projects into your existing organization while preserving all IAM bindings, billing configurations, and organizational policies. What is the recommended approach?",options:["Export all resources from each project to Cloud Storage, delete projects, recreate them in your organization, then re-import resources and reconfigure IAM","Use the Resource Manager API to move the projects from their organization to yours, then apply your organization's policies","Create new projects in your organization and use VPC Network Peering to connect the projects across organizations","Keep the projects in the original organization and use Shared VPC to connect the two organizations"],correct:1,explanation:"The Resource Manager API supports moving projects between organizations using `gcloud projects move` or the API directly. This preserves all project-level IAM bindings, resources, and configurations while allowing you to apply your organization's policies after the move. This is the officially supported migration path.",wrongExplanations:{0:"Exporting and re-importing would be extremely complex, time-consuming, error-prone, and would result in downtime. Many resources don't support export/import, and you'd lose resource history and IDs.",2:"VPC Network Peering doesn't solve organizational structure or billing consolidation. Projects would remain in separate organizations with separate billing and policy management.",3:"Shared VPC cannot span organizations and doesn't address the need for unified organizational structure, billing, and policy management."}},{id:489,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"You are designing the folder structure for a large enterprise with 5 business units, each with 3 departments, and each department has development, staging, and production environments. Security policies must be enforced at the business unit level, while billing needs to be tracked by department. What folder hierarchy should you implement?",options:["Organization > Folders (per environment: dev/staging/prod) > Folders (per department) > Projects","Organization > Folders (per business unit) > Folders (per department) > Projects (per environment)","Organization > Projects (with labels for business unit, department, and environment)","Organization > Folders (per department) > Folders (per environment) > Projects"],correct:1,explanation:"This structure allows applying organization policies at the business unit level (top folder), tracking billing by department (middle folder), and separating environments within projects. Policies inherit downward, and you can use labels on projects for additional filtering. This follows Google Cloud's recommended hierarchy for large enterprises.",wrongExplanations:{0:"Organizing by environment at the top level makes it impossible to apply business unit-level policies effectively, and billing tracking by department becomes difficult when departments are nested under environments.",2:"Without folders, you cannot enforce policies at organizational levels. Labels alone don't support policy inheritance, and managing hundreds of projects directly under the organization becomes unmanageable.",3:"Starting with departments before business units prevents applying business unit-level policies, which is a key requirement. This structure also makes cross-department initiatives at the business unit level difficult to manage."}},{id:490,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your organization's security team requires that all projects in the 'Finance' folder must have Compute Engine default service accounts disabled, but projects in the 'Development' folder need them enabled for rapid prototyping. How should you implement this requirement?",options:["Set an organization policy constraint `constraints/compute.disableSerialPortAccess` on the Finance folder","Apply the organization policy constraint `constraints/iam.disableServiceAccountCreation` at the Organization level","Set an organization policy constraint `constraints/compute.disableDefaultServiceAccount` on the Finance folder only","Create a custom IAM role without service account permissions and assign it to all Finance folder projects"],correct:2,explanation:"The `constraints/compute.disableDefaultServiceAccount` policy prevents the automatic creation and attachment of the default Compute Engine service account. Applying it only to the Finance folder allows you to enforce this security requirement there while leaving Development folder projects unaffected.",wrongExplanations:{0:"The serial port access constraint controls console access to VM serial ports for debugging, not service account behavior. This doesn't address the requirement.",1:"Disabling service account creation at the organization level would affect all folders including Development. This violates the requirement to allow defaults in the Development folder.",3:"IAM roles control user permissions, not the automatic creation of default service accounts. This approach wouldn't prevent the default service account from being created when VMs are launched."}},{id:491,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"A project manager needs to create new projects under a specific folder but should not be able to delete projects or modify folder-level IAM policies. What is the minimum set of permissions required?",options:["Grant `roles/resourcemanager.folderAdmin` on the folder","Grant `roles/resourcemanager.projectCreator` on the folder and `roles/owner` on created projects","Grant `roles/resourcemanager.projectCreator` on the folder","Grant `roles/editor` at the organization level"],correct:2,explanation:"The `roles/resourcemanager.projectCreator` role grants the permission to create projects within a folder. When a user creates a project, they automatically become the Owner of that project, allowing them to manage it. This role doesn't grant permissions to delete projects or modify folder IAM, meeting the requirement for least privilege.",wrongExplanations:{0:"The folderAdmin role includes permissions to modify folder IAM policies and manage all resources under the folder, which exceeds the requirements and violates the principle of least privilege.",1:"Users automatically receive Owner role on projects they create, so explicitly granting Owner is redundant. Only projectCreator at the folder level is needed.",3:"Editor at the organization level grants far too many permissions across the entire organization, including the ability to modify resources in all folders and projects, which violates least privilege principles."}},{id:492,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your company wants to enforce a policy that prevents all projects in the organization from creating external IP addresses for Compute Engine instances, except for projects in the 'DMZ' folder which hosts internet-facing services. How should you configure this?",options:["Apply `constraints/compute.vmExternalIpAccess` with DENY at the Organization level, then create an ALLOW exception on the DMZ folder","Apply `constraints/compute.vmExternalIpAccess` with DENY at the Organization level; folder-level exceptions cannot be made for this constraint","Create a Cloud Armor security policy that blocks external IP assignment","Use VPC firewall rules to block external traffic at the organization level"],correct:0,explanation:"Organization policy constraints support inheritance and can be overridden at lower levels of the hierarchy. By setting DENY at the organization level, you block external IPs everywhere. Then setting ALLOW on the DMZ folder creates an exception for just those projects. This is the correct way to implement exceptions in the resource hierarchy.",wrongExplanations:{1:"While some organization policy constraints cannot be overridden (enforcement-only constraints), `compute.vmExternalIpAccess` is a list constraint that supports inheritance and can be configured differently at different levels of the hierarchy.",2:"Cloud Armor is a web application firewall for protecting applications from DDoS and other attacks. It doesn't control whether compute instances can have external IP addresses assigned at creation time.",3:"VPC firewall rules control network traffic to and from instances but don't prevent the assignment of external IP addresses. An instance can have an external IP even if firewall rules block all traffic to it."}},{id:493,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"You need to grant a security auditor read-only access to view all IAM policies, resources, and configurations across your entire organization, but they should not be able to view the actual data stored in databases or storage buckets. What role should you assign at the organization level?",options:["roles/viewer","roles/iam.securityReviewer","roles/browser","roles/iam.organizationRoleViewer"],correct:1,explanation:"`roles/iam.securityReviewer` is specifically designed for security auditors. It grants read access to IAM policies, organization policies, and security-related configurations across the organization without granting access to actual data in Cloud Storage, databases, or Compute Engine instances. This follows the principle of least privilege for auditing scenarios.",wrongExplanations:{0:"The Viewer role grants read access to most resources, including the ability to list and get details about data in Cloud Storage buckets and database instances, which violates the requirement to not access actual data.",2:"The Browser role only allows listing projects and folders in the resource hierarchy. It doesn't provide access to IAM policies, security configurations, or resource metadata needed for security auditing.",3:"`roles/iam.organizationRoleViewer` only grants permissions to view custom roles at the organization level. It doesn't provide access to IAM policies, bindings, or other security configurations across projects."}},{id:494,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Cloud Identity",question:"Your company does not use Google Workspace but wants to manage user identities centrally for Google Cloud access. You have 500 employees using Active Directory on-premises for authentication. What is the most cost-effective solution that provides centralized identity management?",options:["Purchase Google Workspace for all 500 employees to get centralized identity management","Use Cloud Identity Free edition and federate with your Active Directory using Google Cloud Directory Sync (GCDS)","Have each employee create their own Gmail account and grant access individually","Use Cloud Identity Premium edition with third-party SAML SSO to Active Directory"],correct:1,explanation:"Cloud Identity Free provides centralized identity management for Google Cloud without requiring Google Workspace licenses. GCDS syncs users from Active Directory to Cloud Identity, and you can use SAML federation for single sign-on. This is the most cost-effective solution for companies that only need Google Cloud access without Google Workspace productivity tools.",wrongExplanations:{0:"Google Workspace includes productivity tools (Gmail, Docs, Drive) that cost more and aren't needed if you only want Google Cloud identity management. Cloud Identity is designed specifically for this use case.",2:"Individual Gmail accounts cannot be managed centrally, don't support organizational policies, and create security and compliance issues. This approach doesn't scale and violates enterprise identity management best practices.",3:"Cloud Identity Premium costs more than the Free edition and is only needed for advanced features like mobile device management, automated user provisioning, and advanced security. Basic SAML SSO is available in the Free edition."}},{id:495,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Cloud Identity",question:"You are configuring Cloud Identity for your organization. Several contractors need temporary access to Google Cloud resources for a 3-month project. What is the best way to manage these temporary users?",options:["Create Cloud Identity accounts for contractors and manually delete them after 3 months","Use guest user invitations which automatically expire after a specified period","Grant the contractors access using their personal Gmail accounts","Create a service account that all contractors share for the project duration"],correct:1,explanation:"Cloud Identity supports guest users who can be invited with automatic expiration dates. This is the recommended approach for temporary access as it ensures accounts are automatically cleaned up, reduces administrative overhead, and maintains proper audit trails. Guest users can authenticate using their own email address via federation.",wrongExplanations:{0:"Manual deletion is error-prone, creates administrative burden, and risks leaving orphaned accounts if someone forgets to delete them. It doesn't provide automatic cleanup or policy enforcement for temporary access.",2:"Personal Gmail accounts cannot be centrally managed, don't support organizational policies, and create audit and compliance issues. If a contractor leaves, you can't disable their personal Gmail account.",3:"Sharing a service account among multiple users is a security anti-pattern. It prevents individual accountability, makes audit logging useless, and if compromised, affects all contractors. Service accounts are for applications, not humans."}},{id:496,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Cloud Identity",question:"Your security team requires multi-factor authentication (MFA) for all users accessing Google Cloud Console, but you don't want to require MFA for programmatic access via gcloud CLI. How should you configure this?",options:["Enable 2-Step Verification enforcement in Cloud Identity for all users; create an organization policy to allow programmatic access","Use Context-Aware Access to create policies that require MFA for Console access but allow CLI access without MFA","Configure 2-Step Verification in Cloud Identity and instruct users to disable it when using gcloud CLI","This is not possible; if MFA is enabled, it applies to all access methods including CLI"],correct:1,explanation:"Context-Aware Access allows you to create granular policies based on access context including device, location, and access method. You can require MFA for Console access while allowing CLI access without MFA, or require different security levels based on other factors. This provides flexibility while maintaining security where needed.",wrongExplanations:{0:"Organization policies control resource configuration and access, not authentication methods. They cannot be used to selectively enforce or bypass MFA based on access method.",2:"Having users enable and disable MFA manually defeats the purpose of enforced security policies, creates inconsistent security posture, and is impractical to manage. This approach introduces significant security risks.",3:"While basic 2-Step Verification applies to all access methods, Context-Aware Access policies can differentiate between access methods and apply different requirements. This option is incorrect because the capability does exist."}},{id:497,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Cloud Identity",question:"Your organization uses Cloud Identity and has 5 different groups representing teams (data-engineering, ml-research, devops, security, finance). You need to grant the data-engineering group access to BigQuery in multiple projects. What is the most maintainable approach?",options:["Add individual users from the data-engineering team to each project's IAM policy with BigQuery roles","Grant the data-engineering@yourcompany.com group the necessary BigQuery roles in each project's IAM policy","Create a service account for the data-engineering team and share the key file","Create individual project-specific groups for each project where data-engineering needs access"],correct:1,explanation:"Using Cloud Identity groups in IAM policies is the recommended best practice. When you add or remove users from the group, permissions are automatically updated across all projects. This centralized management is more maintainable, reduces errors, and follows the principle of least privilege by role.",wrongExplanations:{0:"Managing individual users in multiple IAM policies creates maintenance overhead, is error-prone, and doesn't scale. When team members join or leave, you must update every project's IAM policy manually.",2:"Service accounts are for applications, not groups of users. Sharing service account keys is a serious security anti-pattern, prevents individual auditing, and if the key is compromised, all team members are affected.",3:"Creating project-specific groups defeats the purpose of centralized group management. When users join or leave the data-engineering team, you'd need to update multiple groups, increasing administrative burden and error risk."}},{id:498,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Budgets and alerts",question:"Your finance team wants to receive email alerts when project costs reach 50%, 80%, and 100% of the monthly budget, and they want to automatically disable billing when 110% is reached to prevent runaway costs. How should you configure this?",options:["Create a budget with alert thresholds at 50%, 80%, 100%, and 110%; configure Pub/Sub topic to receive alerts; create a Cloud Function triggered by Pub/Sub to disable billing at 110%","Configure budget alerts at 50%, 80%, and 100%; use organization policies to set a hard spending limit at 110%","Create budget alerts at the required thresholds; billing will automatically stop when 100% is reached","Set spending limits directly on the billing account for automatic enforcement"],correct:0,explanation:"Budgets provide alerting but don't automatically stop billing to prevent service disruption. To automatically disable billing, you must create a Pub/Sub topic that receives budget notifications, then trigger a Cloud Function to call the Billing API to disable billing when the threshold is reached. This is the officially documented approach for programmatic billing control.",wrongExplanations:{1:"Organization policies control resource configurations and access, not spending limits. There is no organization policy that can enforce hard spending limits or automatically disable billing.",2:"Budget alerts only send notifications; they never automatically stop billing. Google Cloud continues to accrue charges even after budgets are exceeded to prevent service disruption.",3:"There are no native spending limits or caps on Google Cloud billing accounts. The only way to stop billing is to programmatically disable it via the API or manually disable services."}},{id:499,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Cost allocation",question:"Your organization has multiple teams sharing projects, and you need to allocate costs back to each team for chargeback purposes. Teams are identified by a 'team' label on all their resources. What is the most effective approach for tracking and reporting costs by team?",options:["Export billing data to BigQuery and query costs grouped by the 'team' label","Create separate projects for each team and track costs by project","Use Cloud Monitoring to create custom metrics for cost tracking by label","Configure budget alerts with filters for the 'team' label"],correct:0,explanation:"Exporting billing data to BigQuery allows you to query and analyze costs with full SQL capabilities. Labels are included in the billing export, enabling you to `GROUP BY` the team label for accurate chargeback reporting. BigQuery provides the flexibility for complex cost analysis, custom dashboards, and integration with BI tools.",wrongExplanations:{1:"While separate projects enable easier cost tracking, reorganizing existing resources into new projects is disruptive, may not be feasible for shared resources, and doesn't leverage the label-based organization already in place.",2:"Cloud Monitoring tracks operational metrics (CPU, memory, etc.), not billing costs. Cost data comes from Cloud Billing, not the monitoring agent. This tool cannot be used for cost allocation.",3:"Budget alerts are for threshold notifications, not detailed cost analysis or reporting. They don't provide the querying capabilities needed for chargeback reporting and cannot generate detailed cost breakdowns by label."}},{id:500,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Billing exports",question:"You need to analyze historical billing data for the past year to identify cost optimization opportunities and create trend analysis. The billing console only shows the last 12 months in limited detail. What should you do?",options:["Use the Cloud Billing Reports in the Console to export to PDF monthly","Configure billing export to BigQuery; BigQuery retains data indefinitely and allows complex analysis","Configure billing export to Cloud Storage as CSV files for archival","Use the Billing API to retrieve historical data and store it in Cloud SQL"],correct:1,explanation:"Billing export to BigQuery is the recommended approach for detailed billing analysis. BigQuery stores data indefinitely, provides powerful SQL querying for analysis, supports partitioning for efficient queries, and integrates well with visualization tools like Data Studio and Looker. The schema includes granular details not available in Console reports.",wrongExplanations:{0:"PDF exports from the Console provide static reports unsuitable for programmatic analysis, trend analysis, or data manipulation. You'd need to manually extract data from PDFs, which is error-prone and doesn't scale.",2:"While CSV exports to Cloud Storage provide archival, they require additional processing to query and analyze. You'd need to load data into a database or analysis tool. BigQuery provides both storage and query capabilities in one service.",3:"The Billing API is designed for programmatic access to billing information, not bulk historical exports. Using it to backfill data into Cloud SQL creates unnecessary complexity, and Cloud SQL isn't optimized for the large-scale analytical queries needed for billing analysis."}},{id:501,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Budgets and alerts",question:"Your development team frequently experiments with new services, causing unpredictable cost spikes. You want to set up budget alerts that notify different people based on the severity: alerts at 50% go to the team lead, alerts at 90% go to the department head, and alerts at 100% go to finance. How should you configure this?",options:["Create three separate budgets with different thresholds and different notification email addresses for each","Create one budget with multiple threshold rules at 50%, 90%, and 100%; configure a Pub/Sub topic with Cloud Function to route notifications based on threshold","Create one budget with three thresholds; add all three recipients to the same email notification list","Budget notifications cannot be customized per threshold; all recipients receive all alerts"],correct:1,explanation:"A single budget can have multiple thresholds, but all thresholds send to the same email recipients by default. To route different thresholds to different people, publish to a Pub/Sub topic, then use a Cloud Function to parse the alert payload (which includes threshold info) and send appropriately targeted notifications via email, Slack, etc. This is the recommended approach for complex notification routing.",wrongExplanations:{0:"Multiple budgets for the same project/resources creates confusion and management overhead. You'd receive redundant alerts, and keeping thresholds synchronized becomes difficult. One budget with threshold-based routing is cleaner.",2:"All recipients would receive all alerts regardless of threshold level. This creates alert fatigue and violates the requirement to notify different people at different severity levels.",3:"This is incorrect; while default email notifications go to all recipients, using Pub/Sub with Cloud Functions allows threshold-specific routing as described in the correct answer."}},{id:502,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Billing accounts",question:"Your company has three subsidiaries, each with their own Google Cloud organization and billing account. Corporate leadership wants to consolidate billing to get volume discounts but maintain separate cost tracking for each subsidiary. What is the recommended approach?",options:["Keep three separate billing accounts; volume discounts automatically apply across accounts in the same corporate entity","Create one master billing account with subaccounts for each subsidiary","Merge all three organizations into one; use folders per subsidiary and a single billing account with labels for tracking","Link all three organizations to a single billing account; use projects with labels to track costs per subsidiary"],correct:3,explanation:"A single billing account can be linked to multiple organizations. This enables volume-based discounts across all usage while maintaining organizational separation. Using labels on projects allows detailed cost tracking per subsidiary through billing exports to BigQuery. This approach balances cost optimization with organizational requirements.",wrongExplanations:{0:"Separate billing accounts do not receive combined volume discounts. Discounts are calculated per billing account. This option fails to meet the requirement for consolidated billing to achieve volume pricing.",1:"Google Cloud billing accounts don't have a concept of 'subaccounts'. You can have multiple billing accounts, but they remain separate for discount purposes.",2:"Merging organizations is disruptive, complex, and may not be feasible due to subsidiary legal independence. It requires migrating all resources and reconfiguring IAM, policies, and organizational structure."}},{id:503,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"You are a new Google Cloud engineer configuring a VPC network. You want to use Gemini Cloud Assist to help you write firewall rules that allow HTTPS traffic from specific IP ranges. How should you interact with Gemini to get the most accurate assistance?",options:["Ask Gemini: 'Write firewall rules' and let it generate default configurations","Provide context: 'I need to allow HTTPS traffic from 203.0.113.0/24 to instances with tag web-servers in my VPC'. Ask Gemini to generate the gcloud command","Use Gemini to write the rules directly in the Console UI; it has access to your current project context automatically","Describe your requirements in natural language; Gemini will automatically apply the configuration to your project"],correct:1,explanation:"Gemini Cloud Assist works best with specific, contextual prompts. Providing details like the traffic type (HTTPS), source IP range, target tags, and your intent helps Gemini generate accurate gcloud commands like 'gcloud compute firewall-rules create allow-https --direction=INGRESS --priority=1000 --network=default --action=ALLOW --rules=tcp:443 --source-ranges=203.0.113.0/24 --target-tags=web-servers'. You then review and execute the suggestions. Gemini provides assistance but doesn't automatically apply changesyou maintain control.",wrongExplanations:{0:"Generic prompts without context lead to generic answers that may not fit your specific requirements. Gemini needs details about your traffic patterns, IP ranges, instance targets, and protocols to provide useful guidance.",2:"While Gemini integrates with the Console, it doesn't automatically read your current project context or VPC configuration. You must provide the relevant details in your prompt for accurate assistance.",3:"Gemini provides recommendations and generated commands/configurations, but it never automatically applies changes to your resources. This is a safety featureyou must review and explicitly execute any suggested changes."}},{id:504,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"Your team is troubleshooting why a Cloud Function is failing to write to a Cloud Storage bucket. You want to use Gemini Cloud Assist to diagnose the IAM permissions issue. What is the most effective approach?",options:["Ask Gemini to check the IAM permissions for you and provide a list of missing roles","Provide Gemini with error messages from the logs, the service account being used, and the bucket name; ask it to suggest possible IAM misconfigurations","Request that Gemini automatically fix the IAM permissions by granting the necessary roles","Use Gemini to generate a script that will programmatically audit and fix IAM issues"],correct:1,explanation:"Gemini excels at analyzing error messages and suggesting likely causes when given context. By sharing the error logs, the service account, and the resource involved, Gemini can suggest which IAM roles or permissions might be missing (e.g., roles/storage.objectCreator). You then validate and apply the recommendations. This diagnostic workflow leverages Gemini's knowledge while keeping you in control.",wrongExplanations:{0:"Gemini cannot directly access your project's IAM policies or resources. It can't 'check' your actual permissions. It provides guidance based on the information you share, not by inspecting your live environment.",2:"Gemini does not have the capability to modify your Google Cloud resources or IAM policies. It provides recommendations that you must manually apply or script yourself.",3:"While Gemini can help generate scripts for IAM auditing, asking it to automatically fix issues isn't possible. You'd still need to review, customize, and execute any script Gemini provides, with your own validation of what changes are safe."}},{id:505,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"You are writing a deployment script using Terraform and want Gemini Cloud Assist to help you understand best practices for managing state files in Cloud Storage. How should you phrase your request to get actionable guidance?",options:["Ask: 'Tell me about Terraform state files'","Ask: 'How do I configure Terraform to use Cloud Storage backend for state with encryption and state locking?'","Ask Gemini to analyze your current Terraform configuration file and automatically suggest improvements","Request Gemini to create a complete Terraform project structure with best practices"],correct:1,explanation:"A specific, task-oriented question yields actionable guidance. By asking about the Cloud Storage backend with specific requirements (encryption, state locking), you get targeted advice on backend configuration blocks, enabling encryption with CMEK or Google-managed keys, and using state locking. This allows you to immediately apply the knowledge to your tf files.",wrongExplanations:{0:"This is too broad and will result in general educational content about state files rather than specific implementation guidance for your use case with Cloud Storage. More specificity leads to more useful assistance.",2:"Gemini cannot directly access or analyze files in your project or local environment unless you paste the content into your prompt. It doesn't have automatic file system access.",3:"While Gemini can generate project scaffolding, asking for a 'complete' project without specifying requirements may produce generic output that doesn't match your actual needs. Iterative, specific prompts work better."}},{id:506,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"Your organization has compliance requirements that prohibit sending any project-specific resource names, IDs, or configurations to external AI services. You still want to use Gemini Cloud Assist for learning Google Cloud concepts. What should you do?",options:["Use Gemini with generic, hypothetical examples rather than your actual project details","Disable Gemini Cloud Assist entirely; it cannot be used without sending project data","Enable Gemini but use organization policies to prevent data exfiltration","Request that Google configure Gemini to not log your organization's prompts"],correct:0,explanation:"Gemini Cloud Assist processes the prompts you provide, so you control what information is shared. By using hypothetical scenarios, generic examples, and avoiding actual project-specific identifiers, you can learn from Gemini while maintaining compliance. For example, instead of 'my-prod-bucket-12345', use 'my-bucket'. This approach balances utility with data protection.",wrongExplanations:{1:"You can use Gemini effectively without sharing sensitive project details by asking questions with generic examples and hypothetical scenarios. Complete disablement isn't necessary.",2:"Organization policies control resource configurations and access patterns, not the content of prompts sent to Gemini. They cannot filter or redact information within your natural language questions.",3:"Gemini Cloud Assist follows Google's standard data handling practices. You cannot customize logging on a per-organization basis. The solution is to control what you include in your prompts, not to request backend configuration changes."}},{id:507,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"You are using Gemini Cloud Assist in the Cloud Console to help configure a Compute Engine instance. Gemini suggests a gcloud command to create the instance. Before executing it, what should you do?",options:["Execute the command immediately; Gemini commands are pre-validated","Review the command to ensure it matches your requirements (machine type, region, disk size, network) and understand what it will create before executing","Copy the command and ask Gemini to verify it again to ensure accuracy","Test the command in a different project first to avoid impacting production"],correct:1,explanation:"Always review AI-generated commands before execution. Verify that parameters match your requirements (correct machine type, region, disk size, tags, service account, etc.). Understand what resources will be created and their cost implications. Gemini provides helpful starting points, but you are responsible for validating and executing commands safely. This is a critical best practice for using AI assistance.",wrongExplanations:{0:"Gemini-generated commands should be treated as suggestions, not pre-validated commands safe to execute blindly. They may not perfectly match your environment, naming conventions, or specific requirements. Always review before execution.",2:"While you can refine prompts to get better suggestions, asking Gemini to verify its own output isn't meaningful. You, as the engineer, must validate the command against your actual requirements and environment. Human review is essential.",3:"Testing in a different project is good practice for complex changes, but the first step is always to review and understand what the command does. For simple resource creation, review and execution in the correct environment is often sufficient."}},{id:508,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Organization policies",question:"Your security team requires that all VM instances in your organization must not have external IP addresses, except for instances in a specific 'dmz' folder used for public-facing services. How should you configure organization policies to enforce this?",options:["Set an organization policy at the organization level to deny `constraints/compute.vmExternalIpAccess`; create an exception policy at the 'dmz' folder to allow it","Set an organization policy at the organization level to allow `constraints/compute.vmExternalIpAccess`; create a deny policy at all folders except 'dmz'","Use IAM conditions to restrict who can assign external IPs; don't use organization policies","Configure firewall rules to block external IP assignment at the organization level"],correct:0,explanation:"Organization policies use inheritance with the ability to override at lower levels. Setting a deny policy at the organization level blocks external IPs everywhere by default. Creating an allow policy at the 'dmz' folder level overrides the inherited deny policy specifically for that folder. This is the standard pattern for 'deny everywhere except specific locations'.",wrongExplanations:{1:"This approach is backwards. Allowing at the organization level permits external IPs everywhere, then you'd need deny policies at multiple folders (every folder except dmz). This is harder to maintain and error-prone as new folders are added.",2:"IAM conditions control who can perform actions, not what resources can be configured. You could restrict who can create instances with external IPs, but organization policies are the proper mechanism to enforce resource configuration constraints.",3:"Firewall rules control network traffic, not resource configuration. They don't prevent external IP assignment; they control which traffic can reach those IPs. Organization policies are the correct tool for enforcing resource configuration requirements."}},{id:509,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Organization policies",question:"Developers in your organization want to experiment with Cloud Run, but your organization policy `constraints/run.allowedIngress` is set to 'internal-only' at the organization level. Developers need to allow public ingress for testing. What is the correct approach to enable this?",options:["Ask developers to create Cloud Run services in a separate project outside the organization","Have an admin create an organization policy override at the project or folder level to allow 'all' ingress for the development environment","Developers can override organization policies by using gcloud with --allow-unauthenticated flag","Disable the organization policy temporarily for all projects during the testing phase"],correct:1,explanation:"Organization policies support hierarchical overrides. An administrator with the appropriate permissions can create a policy at a lower level (project or folder) that overrides the inherited policy, allowing 'all' ingress for specific development projects while keeping 'internal-only' enforced for production. This maintains security boundaries while enabling development workflows.",wrongExplanations:{0:"Creating projects outside the organization circumvents governance and security controls. All company projects should be within the organization hierarchy to maintain visibility, billing, and policy enforcement. This defeats the purpose of centralized management.",2:"Developers cannot override organization policies through command-line flags. Organization policies are enforced by the Resource Manager API and cannot be bypassed by individual users or service accounts, regardless of what gcloud flags are used.",3:"Disabling organization-wide policies removes protections from all projects, including production. This is a security risk and violates the principle of least privilege. Selective overrides at the project/folder level are the correct approach."}},{id:510,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Organization policies",question:"You need to enforce that all new Cloud Storage buckets must be created in either 'us-central1' or 'us-east1' regions only, with no exceptions. Which organization policy constraint should you configure?",options:["constraints/gcp.resourceLocations with allowedValues set to 'us-central1' and 'us-east1'","constraints/storage.bucketRegions with allowedValues set to 'us-central1' and 'us-east1'","constraints/gcp.allowedRegions with values 'us-central1' and 'us-east1'","constraints/compute.vmRegions applies to all resources including storage"],correct:0,explanation:"The `constraints/gcp.resourceLocations` policy is the universal constraint for enforcing resource location restrictions across multiple Google Cloud services, including Cloud Storage, Compute Engine, BigQuery, and others. Setting allowedValues to the specific regions restricts resource creation to only those locations. This is the correct and most comprehensive approach.",wrongExplanations:{1:"There is no organization policy constraint named 'storage.bucketRegions'. The constraint for location restrictions is `gcp.resourceLocations`, which applies broadly across services.",2:"There is no constraint named 'gcp.allowedRegions'. The correct constraint name is `gcp.resourceLocations`, which controls where resources can be created.",3:"The constraint `compute.vmRegions` does not exist, and even if it did, Compute Engine constraints apply only to Compute Engine resources, not to Cloud Storage buckets. `gcp.resourceLocations` is the cross-service solution."}},{id:511,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Organization policies",question:"Your company policy requires that all projects must have labels for 'environment' and 'cost-center' before any resources can be created. How can you enforce this using organization policies?",options:["Use the constraint `constraints/gcp.requireLabels` to enforce required labels on projects","Organization policies cannot enforce labels; use Cloud Asset Inventory to audit missing labels","Use IAM conditions that check for labels before granting resource creation permissions","Set up Cloud Functions triggered by project creation to automatically add default labels"],correct:0,explanation:"The `constraints/gcp.requireLabels` organization policy allows you to specify required label keys that must be present on resources. You can configure this at the organization level to enforce that all projects (and optionally other resources) must have specified labels before they can be created or updated. This is the direct, policy-based enforcement mechanism.",wrongExplanations:{1:"While Cloud Asset Inventory can audit for missing labels after resources are created, it doesn't prevent creation of non-compliant resources. Organization policies provide preventive enforcement, blocking resource creation that violates policies.",2:"IAM conditions control access permissions, not resource configuration requirements. You cannot use IAM to check for resource labels as a precondition for operations. Organization policies are the mechanism for configuration constraints.",3:"Cloud Functions can add labels post-creation, but this is reactive, not preventive. Resources could be created without labels, and functions might fail or be delayed. Organization policies prevent non-compliant resources from being created in the first place."}},{id:512,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - APIs and services",question:"You are creating a new project for a machine learning workload. Your application needs to use Cloud Storage, BigQuery, and Vertex AI. When you try to create a BigQuery dataset via the Console, you receive an error that the BigQuery API is not enabled. What should you do?",options:["Enable the BigQuery API in the project using the Console (APIs & Services > Library) or gcloud services enable bigquery.googleapis.com","BigQuery is always enabled; check your IAM permissions instead","File a support ticket with Google to enable BigQuery for your project","Wait 24 hours for automatic API propagation after project creation"],correct:0,explanation:"Most Google Cloud APIs are not enabled by default in new projects. You must explicitly enable each API you want to use through the APIs & Services page in the Console or using `gcloud services enable <api-name>`. Enabling an API is immediate and allows your project to create resources and make API calls for that service. This is a standard setup step for new projects.",wrongExplanations:{1:"BigQuery API is not enabled by default. While IAM permissions are necessary to use BigQuery, the API itself must be enabled at the project level before any BigQuery operations can be performed, regardless of permissions.",2:"API enablement is self-service and happens instantly. You don't need to contact Google support to enable standard Google Cloud APIs. Support tickets are for issues like quota increases beyond standard limits or account-level problems.",3:"API enablement is immediate, not a background process. Once you enable an API through the Console or gcloud, you can immediately use that service. There is no waiting period for propagation."}},{id:513,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - APIs and services",question:"Your development team enables and disables various APIs frequently during experimentation. You notice that previously disabled APIs still appear in billing reports with small charges. What is the most likely explanation?",options:["Billing data is cached and will correct itself after 24-48 hours","Disabling an API doesn't delete existing resources created by that API; those resources continue to incur costs until deleted","Google Cloud charges a 'disabled API' maintenance fee for previously enabled APIs","There is a bug in the billing system; file a support case to get charges refunded"],correct:1,explanation:"Disabling an API prevents new API calls and resource creation for that service, but it does not delete existing resources. For example, disabling the Compute Engine API doesn't stop running VMs. Those resources continue to exist and incur costs until explicitly deleted. Always delete resources before disabling APIs if you want to stop charges completely.",wrongExplanations:{0:"Billing data may have slight delays, but the presence of charges for disabled APIs is not a caching issue. It reflects actual ongoing costs from resources that still exist after the API was disabled.",2:"There is no maintenance fee for disabled APIs. Google Cloud only charges for resources you actively use or have provisioned. Disabled APIs have no associated cost unless resources from those services still exist.",3:"This is expected behavior, not a bug. The billing system correctly reflects charges for resources that continue to run even after their associated API is disabled. To stop charges, delete the resources, not just disable the API."}},{id:514,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - APIs and services",question:"Your application uses the Cloud Vision API to analyze images. You deployed to a new project and are getting 'API quota exceeded' errors despite very low usage. The default quota should be sufficient. What should you check first?",options:["Verify that the Cloud Vision API is enabled in the new project","Check if organization policies are restricting API usage","Request a quota increase through the Console (IAM & Admin > Quotas)","Verify that your service account has the necessary IAM roles"],correct:0,explanation:"The most common cause of 'quota exceeded' errors in a new project when usage is low is that the API hasn't been enabled. The error message can be misleading. Before requesting quota increases or investigating complex issues, always verify that the required API is enabled in the project using `gcloud services list --enabled` or the Console.",wrongExplanations:{1:"While organization policies can restrict service usage, they typically result in 'policy constraint' errors, not quota errors. The first and most common issue is that the API isn't enabled at all.",2:"Requesting quota increases is premature if the API isn't even enabled yet. Default quotas for Cloud Vision are generous for low usage. Always verify basic enablement before requesting increases.",3:"IAM permissions issues result in 'permission denied' errors, not quota errors. If the API isn't enabled, you get quota-like errors because the project has zero quota allocation for that service."}},{id:515,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"Your application needs to create 50 Compute Engine instances across multiple regions for a load test, but you're hitting the 'QUOTA_EXCEEDED' error for 'CPUS_ALL_REGIONS' when trying to create the 25th instance. The default quota is 24 CPUs. What should you do?",options:["Request a quota increase for 'CPUS_ALL_REGIONS' through the Console (IAM & Admin > Quotas) with justification for the load test","Delete existing instances in other projects to free up quota","Switch to a different region that might have available quota","Contact Google Cloud support to get an emergency quota override"],correct:0,explanation:"Quotas are per-project limits that can be increased through self-service requests in the Console. Navigate to IAM & Admin > Quotas, filter for 'CPUS_ALL_REGIONS', select the quota, and click 'Edit Quotas'. Provide justification (load testing) and the requested amount. Many increases are approved automatically or within hours. This is the standard process for planned capacity needs.",wrongExplanations:{1:"Quotas are per-project, not per-account or organization. Deleting instances in other projects doesn't free up quota for your current project. Each project has independent quota allocations.",2:"CPUS_ALL_REGIONS is an aggregate quota across all regions. Switching regions won't help because this quota applies to total CPU usage regardless of where instances are created. You need to increase the quota itself.",3:"Support contact is unnecessary for routine quota increases. The self-service quota increase system is designed for this. Support is needed only for extremely large increases, denied requests requiring review, or time-sensitive production emergencies."}},{id:516,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"You requested a quota increase for 'IN_USE_ADDRESSES' (static external IPs) from 8 to 50, but the request was automatically denied. Your justification stated 'for future use'. What should you do?",options:["File a support ticket to escalate the denied quota increase","Request a smaller increase (e.g., to 15) first, then request more later as needed","Resubmit the request with detailed justification explaining the specific use case, architecture, and why 50 addresses are needed","Wait 30 days and resubmit the same request; automatic approval may happen"],correct:2,explanation:"Quota increases require clear, specific justification, especially for large increases. 'For future use' is insufficient. Explain your architecture (e.g., 'need 50 static IPs for 50 regional load balancers serving different customer-facing applications'). Include details about your business need, timeline, and why the amount is justified. Better justifications lead to faster approvals.",wrongExplanations:{0:"Escalating to support is premature. First, improve your justification and resubmit. Most denied requests are due to vague or insufficient justification, not technical issues requiring support intervention.",1:"While incremental increases are valid, requesting a smaller amount when you actually need 50 delays your project and creates extra work. The better approach is to justify the actual need properly so the full increase can be approved.",3:"Resubmitting the same request with the same insufficient justification will likely be denied again. The issue is the quality of justification, not a waiting period. Fix the justification and resubmit immediately."}},{id:517,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"Your project suddenly starts failing to create new Pub/Sub topics with a quota error, but you've only created 100 topics and the documented default quota is 10,000 topics per project. What is the most likely cause?",options:["The quota documentation is outdated; the actual default is 100 topics","You're hitting a different quota limit, such as 'topics per region' rather than 'topics per project'","There may be soft limits or rate limits being enforced; check the detailed quota page in the Console for all Pub/Sub quotas","Your billing account is past due, causing quota restrictions"],correct:2,explanation:"Pub/Sub has multiple quotas: topics per project, publish throughput, subscription throughput, outstanding messages, etc. The error might be due to a different quota than the one you're checking. Review the detailed quotas page (IAM & Admin > Quotas, filter for Pub/Sub) to see all limits and current usage. Rate limits or throughput limits might be the actual constraint, not topic count.",wrongExplanations:{0:"Google Cloud quota documentation is generally accurate and kept up to date. The default quota for Pub/Sub topics per project is indeed much higher than 100. The issue is likely a different quota being hit.",1:"Pub/Sub topics are global resources within a project, not regional. There isn't a 'topics per region' quota. The quotas relate to the project as a whole or to message throughput rates.",3:"While past-due billing can lead to resource suspension, it typically results in broader service disruption and different error messages, not quota errors. Quota errors usually indicate hitting specific service limits."}},{id:518,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing Compute Engine resources - GKE cluster operations",question:"Your GKE cluster is running out of capacity during peak hours. You want the cluster to automatically scale up when CPU utilization exceeds 70% and scale down during off-peak hours. What should you do?",options:["Enable Cluster Autoscaler on the node pool with appropriate minimum and maximum node counts using 'gcloud container clusters update CLUSTER --enable-autoscaling --min-nodes=3 --max-nodes=10 --zone=ZONE'","Create a Cloud Function triggered by Cloud Monitoring alerts to add nodes via the GKE API when CPU is high","Manually add nodes before peak hours and remove them afterward using gcloud commands","Enable Horizontal Pod Autoscaler (HPA); it will automatically add nodes as needed"],correct:0,explanation:"GKE Cluster Autoscaler automatically adjusts the number of nodes in a node pool based on resource requests of pods and utilization patterns. Enable it using 'gcloud container clusters update CLUSTER --enable-autoscaling --min-nodes=3 --max-nodes=10 --zone=ZONE' or per node pool with 'gcloud container node-pools update POOL --enable-autoscaling --min-nodes=3 --max-nodes=10'. When pods can't be scheduled due to insufficient resources, it adds nodes (up to the maximum). When nodes are underutilized, it removes them (down to the minimum). This is the built-in, recommended solution for automatic cluster scaling.",wrongExplanations:{1:"While technically possible, creating custom automation with Cloud Functions is complex, error-prone, and reinvents functionality that Cluster Autoscaler already provides. You'd need to handle edge cases, race conditions, and coordinate with Kubernetes scheduling.",2:"Manual scaling doesn't provide automatic responsiveness to changing demand, requires operational overhead, and can't react quickly to unexpected spikes. It defeats the purpose of cloud elasticity and wastes engineering time.",3:"HPA scales the number of pod replicas, not nodes. If there aren't enough nodes to run the scaled pods, HPA won't add nodes. You need Cluster Autoscaler for node-level scaling and HPA for pod-level scalingthey work together."}},{id:519,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing Compute Engine resources - GKE cluster operations",question:"You need to upgrade your GKE cluster from version 1.27 to 1.28 with minimal disruption to your production workload. The cluster has multiple node pools. What is the recommended approach?",options:["Upgrade the control plane first, then upgrade each node pool sequentially; GKE will drain nodes gracefully during the upgrade","Create a new cluster with version 1.28, migrate workloads using a blue-green deployment strategy, then delete the old cluster","Upgrade all node pools simultaneously with the control plane for faster completion","Take a snapshot of all nodes, upgrade in place, and restore if there are issues"],correct:0,explanation:"GKE upgrades follow a two-phase process: control plane first, then node pools. The control plane upgrade is automatic and has no downtime. Node pool upgrades drain pods gracefully to other nodes (following PodDisruptionBudgets), upgrade nodes, and return them to service. Upgrading node pools sequentially (one at a time) minimizes risk and maintains capacity throughout the upgrade.",wrongExplanations:{1:"Blue-green cluster migration is valid but much more complex and resource-intensive than in-place upgrades. It requires double the resources temporarily, complex network/DNS cutover, and data migration. Reserve this approach for major version jumps or when testing significant changes.",2:"You cannot upgrade control plane and all node pools simultaneously. Control plane must upgrade first. Additionally, upgrading all node pools at once could cause capacity issues if many nodes are draining simultaneously, potentially causing service disruption.",3:"Node snapshots don't capture Kubernetes state, pod configurations, or workload data. GKE upgrades are designed to be forward-only with testing beforehand. The recommended rollback approach is to create a new node pool with the old version and migrate pods if issues arise."}},{id:520,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing Compute Engine resources - GKE cluster operations",question:"Your GKE application is experiencing intermittent connectivity issues. You suspect it's related to node health. How can you investigate which nodes are having problems and view their logs?",options:[`Use 'kubectl get nodes' to see node status, then 'kubectl describe node <node-name>' for details; check Cloud Logging with 'gcloud logging read "resource.type=k8s_node"' for node logs`,"SSH into each node and check /var/log/syslog for errors","Use 'gcloud compute instances list' to see VM status; GKE nodes are just Compute Engine VMs","Enable Cloud Monitoring for GKE and create a custom dashboard showing node health metrics"],correct:0,explanation:`'kubectl get nodes' shows the status of all nodes (Ready, NotReady, etc.). 'kubectl describe node NODE_NAME' provides detailed information about conditions, resource usage, and recent events. For deeper investigation, use 'gcloud logging read "resource.type=k8s_node AND resource.labels.cluster_name=CLUSTER_NAME" --limit=50' to view node-level logs including kubelet logs. First get cluster credentials with 'gcloud container clusters get-credentials CLUSTER_NAME --zone=ZONE'. This is the standard Kubernetes troubleshooting workflow integrated with Google Cloud logging.`,wrongExplanations:{1:"GKE nodes are managed Compute Engine instances, but SSH access may be disabled or restricted for security. More importantly, kubectl and Cloud Logging provide better visibility into Kubernetes-specific issues without needing direct SSH access to VMs.",2:"While GKE nodes are Compute Engine VMs, managing them primarily through Compute Engine APIs bypasses Kubernetes-aware tooling. You'd miss Kubernetes-specific status information, pod events, and the integration between node and cluster state.",3:"Monitoring dashboards are useful for visualization but don't provide the immediate diagnostic information needed during troubleshooting. Start with kubectl commands for real-time node status, then use monitoring for trends and alerting."}},{id:521,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing Compute Engine resources - GKE cluster operations",question:"Your GKE cluster has a node pool with preemptible VMs to reduce costs. You're experiencing issues with pods being evicted when nodes are preempted. How should you handle this to ensure application availability?",options:["Mix preemptible and non-preemptible nodes; use node affinity and tolerations to run critical workloads on non-preemptible nodes and fault-tolerant batch jobs on preemptible nodes","Replace all preemptible nodes with Spot VMs; they have longer lifespans","Disable preemption by setting the node pool to standard (non-preemptible) VMs; there's no way to handle preemption gracefully","Increase the number of pod replicas; more replicas compensate for preempted nodes"],correct:0,explanation:"The best practice is to use preemptible VMs for appropriate workloads (batch processing, stateless jobs, fault-tolerant services) and non-preemptible VMs for critical services requiring high availability. Use Kubernetes node affinity, tolerations, and pod topology spread constraints to schedule workloads appropriately. This balances cost savings with reliability requirements.",wrongExplanations:{1:"Spot VMs in GKE are preemptible VMsthe terms are used interchangeably. Both can be terminated with 30 seconds notice. Spot VMs don't have longer lifespans; the expected runtime is the same. The solution is workload placement strategy, not VM type.",2:"While switching to non-preemptible VMs eliminates preemption, you lose the cost savings (up to 80% discount). The issue isn't that preemption is unhandleableit's that you need to architect for it appropriately by choosing which workloads run on preemptible nodes.",3:"Increasing replicas helps with availability but doesn't solve the root issue. If all replicas run on preemptible nodes, they can all be preempted simultaneously during high demand periods. You need workload separation between preemptible and non-preemptible nodes."}},{id:522,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing Compute Engine resources - GKE cluster operations",question:"Your team wants to deploy a StatefulSet with 10 replicas that requires persistent storage. Each pod needs its own 100GB persistent volume. What is the recommended way to provision storage for this StatefulSet in GKE?",options:["Create a StorageClass with the pd-standard or pd-ssd provisioner, reference it in the StatefulSet's volumeClaimTemplates; GKE will automatically create PersistentVolumes for each pod","Manually create 10 PersistentVolumes before deploying the StatefulSet","Use a single large PersistentVolume and share it across all pods in the StatefulSet","Create a Cloud Filestore instance and mount it as a shared volume for all pods"],correct:0,explanation:"StatefulSets with volumeClaimTemplates automatically create PersistentVolumeClaims for each pod replica. When you specify a StorageClass (using GKE's built-in provisioners like pd-standard, pd-ssd, or pd-balanced), GKE automatically provisions PersistentVolumes (Persistent Disks) for each claim. This is the declarative, automated approach for StatefulSet storage, ensuring each pod gets its own independent persistent storage.",wrongExplanations:{1:"Manually creating PersistentVolumes defeats the purpose of StatefulSets and volumeClaimTemplates. It doesn't scale well, creates operational overhead, and you'd need to manually create new PVs when scaling the StatefulSet. Dynamic provisioning automates this.",2:"Persistent Disks (pd-standard, pd-ssd, pd-balanced) are read-write by a single node only. Sharing a single PersistentVolume across multiple pods in a StatefulSet would cause mount conflicts and data corruption. Each StatefulSet pod needs its own PV.",3:"Filestore provides shared NFS storage, which is useful for ReadWriteMany scenarios, but StatefulSets typically need independent storage per pod (ReadWriteOnce). Shared storage can lead to data conflicts if the application isn't designed for concurrent access."}},{id:523,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing Compute Engine resources - GKE cluster operations",question:"After deploying a new application version to your GKE cluster, you notice that some pods are in 'CrashLoopBackOff' state. How should you troubleshoot this issue?",options:["Use kubectl describe pod <pod-name> to see events and status, then kubectl logs <pod-name> to view application logs; check for configuration errors or missing dependencies","Delete the pods; Kubernetes will recreate them and the issue may resolve","Increase resource requests (CPU/memory) for the pods; crashes are usually due to insufficient resources","Restart the entire GKE cluster to clear the error state"],correct:0,explanation:"CrashLoopBackOff means the container is starting, crashing, and Kubernetes is restarting it with exponential backoff. kubectl describe pod shows recent events (exit codes, reasons) and kubectl logs shows application output, which usually reveals the cause (configuration errors, missing environment variables, application bugs, failed health checks). This diagnostic approach identifies the root cause.",wrongExplanations:{1:"Deleting pods won't fix the underlying issue. Kubernetes will recreate them with the same configuration, and they'll enter CrashLoopBackOff again. You need to diagnose and fix the root cause (code bug, misconfiguration, etc.) before the pods can run successfully.",2:"While resource constraints can cause OOMKilled errors (Out of Memory), they're just one possible cause of crashes. Blindly increasing resources without investigating logs might waste resources or miss the real issue (application bugs, incorrect configuration, etc.).",3:"Restarting the cluster is extreme, unnecessary, and won't fix application-level issues. CrashLoopBackOff indicates an issue with the pod's container (code, configuration), not cluster infrastructure. Cluster restarts cause unnecessary downtime."}},{id:524,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Google Kubernetes Engine resources - Cloud Monitoring",question:"Your application running on Compute Engine is experiencing high latency. You want to identify which metric (CPU, memory, disk I/O, or network) is the bottleneck. What should you do?",options:["View the VM instance in Cloud Monitoring; check the default metrics for CPU utilization, memory usage, disk I/O, and network throughput on the instance dashboard","SSH into the VM and run top, iostat, and netstat commands to gather metrics","Enable Cloud Trace to identify performance bottlenecks","Install the Cloud Monitoring agent (Ops Agent) to collect detailed system metrics"],correct:0,explanation:"Cloud Monitoring automatically collects default metrics for Compute Engine instances without requiring agent installation. The instance dashboard provides CPU, memory, disk I/O, and network metrics out of the box. This is the quickest way to identify resource bottlenecks. For more detailed application-level metrics, you can then install the Ops Agent, but start with the built-in metrics for infrastructure-level investigation.",wrongExplanations:{1:"While SSH and command-line tools work, they provide only point-in-time snapshots and require manual correlation. Cloud Monitoring provides historical data, visualization, and correlation across metrics over time, making it easier to identify patterns and trends.",2:"Cloud Trace is for distributed tracing of application requests to identify latency in microservices architectures. It doesn't show infrastructure metrics like CPU or memory usage. Use Cloud Monitoring for infrastructure metrics and Cloud Trace for request flow analysis.",3:"The Ops Agent provides additional detailed metrics and logs, but default Compute Engine metrics are sufficient for identifying basic resource bottlenecks. Install the agent for application-level metrics, custom metrics, or enhanced logging, but start with built-in metrics first."}},{id:525,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Google Kubernetes Engine resources - Cloud Monitoring",question:"You want to be alerted when your Cloud SQL database's CPU utilization exceeds 80% for more than 5 minutes. How should you configure this in Cloud Monitoring?",options:["Create an alerting policy with a metric condition on 'cloudsql.googleapis.com/database/cpu/utilization', set threshold to 80%, duration to 5 minutes, and configure notification channels","Create a Cloud Function that queries Cloud Monitoring API every minute and sends alerts via email when CPU exceeds 80%","Enable automatic alerting in Cloud SQL settings; it sends alerts for high CPU by default","Use Cloud Scheduler to run a script every 5 minutes that checks CPU utilization and sends notifications"],correct:0,explanation:"Cloud Monitoring alerting policies are the declarative, built-in way to create alerts based on metric thresholds. You specify the metric (Cloud SQL CPU utilization), the condition (greater than 80%), the duration (5 minutes), and notification channels (email, SMS, Pub/Sub, PagerDuty, etc.). The alerting system continuously evaluates the policy and triggers notifications when conditions are met.",wrongExplanations:{1:"Building custom alerting with Cloud Functions is complex, costly (function invocations), and error-prone compared to using Cloud Monitoring's built-in alerting. You'd need to handle state management, notification deduplication, and recovery notificationsall features provided by Cloud Monitoring.",2:"Cloud SQL does not have built-in automatic alerting. You must explicitly create alerting policies in Cloud Monitoring for Cloud SQL metrics. Default monitoring exists, but alerting requires manual configuration.",3:"Cloud Scheduler with custom scripts is similar to the Cloud Function approachunnecessarily complex and lacking features like notification deduplication, incident management, and integration with multiple notification channels. Use Cloud Monitoring's native capabilities."}},{id:526,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Google Kubernetes Engine resources - Cloud Monitoring",question:"You deployed a custom application to Compute Engine and want to export custom application metrics (e.g., 'requests_per_second', 'orders_completed') to Cloud Monitoring for dashboards and alerting. What is the recommended approach?",options:["Use the Cloud Monitoring API or client libraries to write custom metrics from your application code with the 'custom.googleapis.com' metric prefix","Write metrics to a local file on the VM; Cloud Monitoring agent will automatically detect and upload them","Store metrics in Cloud SQL and query them periodically from Cloud Monitoring","Use the Ops Agent's custom metrics configuration file to define and collect your application metrics"],correct:0,explanation:"The Cloud Monitoring API and client libraries (available in multiple languages) allow you to programmatically write custom metrics from your application. Custom metrics use the 'custom.googleapis.com' prefix. This is the standard approach for application-generated metrics. Your code records metric values, and Cloud Monitoring stores them for visualization and alerting.",wrongExplanations:{1:"The Ops Agent collects system metrics and logs, but it doesn't automatically detect arbitrary application metric files. You can configure it to parse logs for metrics (log-based metrics), but direct API integration is more appropriate for structured custom metrics.",2:"Cloud Monitoring doesn't query external databases for metrics. It's designed to receive metrics pushed via its API. Using Cloud SQL as an intermediary adds unnecessary complexity and latency. The proper integration pattern is application  Cloud Monitoring API.",3:"The Ops Agent can collect metrics from third-party applications using plugins (like JMX, Apache, etc.), but for custom application-specific metrics from your own code, using the Cloud Monitoring API directly is simpler and more flexible."}},{id:527,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Google Kubernetes Engine resources - Cloud Monitoring",question:"Your team wants to create a dashboard showing key metrics across multiple projects (Compute Engine VMs, Cloud Storage buckets, Cloud SQL instances). How can you build a unified dashboard in Cloud Monitoring?",options:["Create a metrics scope in one project and add other projects as monitored projects; create dashboards in the scoping project that include metrics from all monitored projects","Create separate dashboards in each project and switch between them","Export all metrics to BigQuery and build dashboards in Looker Studio","Use Cloud Monitoring API to aggregate metrics programmatically and display them in a custom web application"],correct:0,explanation:"Cloud Monitoring uses metrics scopes (formerly called Workspaces) to aggregate monitoring data from multiple projects. You designate one project as the scoping project and add others as monitored projects. Dashboards and alerts in the scoping project can access metrics from all monitored projects, providing a unified view. This is the built-in multi-project monitoring solution.",wrongExplanations:{1:"Separate dashboards per project require constant switching and don't provide a unified view. You can't correlate metrics across projects or create alerts that span multiple projects. Metrics scopes are designed specifically to solve this problem.",2:"While exporting to BigQuery and using Looker Studio is possible for advanced analytics, it's overkill for operational dashboards. Cloud Monitoring provides built-in multi-project support through metrics scopes, which is simpler, real-time, and purpose-built for monitoring.",3:"Building a custom web application is unnecessary complexity. Cloud Monitoring's metrics scopes provide native multi-project aggregation with dashboards, alerting, and all the features of the monitoring console without custom development."}},{id:528,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Google Kubernetes Engine resources - Cloud Monitoring",question:"You notice that your alerting policy for low disk space is triggering false positives during nightly backup jobs when disk usage temporarily spikes. How can you reduce false positives while still catching genuine disk space issues?",options:["Increase the alert threshold duration so the condition must persist longer before triggering (e.g., from 1 minute to 10 minutes)","Disable the alerting policy during backup windows using a maintenance window or notification channel schedule","Lower the threshold percentage to only alert on more severe conditions","Create multiple alerting policies for different times of day with different thresholds"],correct:0,explanation:"Increasing the duration threshold ensures that transient spikes don't trigger alerts. If disk usage exceeds the threshold for 10 minutes instead of 1 minute, temporary usage during backups won't trigger alerts, but sustained high usage indicating a real problem will. This is the simplest and most effective approach for reducing transient alert noise.",wrongExplanations:{1:"Disabling alerts during backups is riskyif a genuine disk space issue occurs during the backup window, you won't be notified. Adjusting duration thresholds is safer because alerts still fire if problems persist, even during backup times.",2:"Lowering the threshold (e.g., from 80% to 90%) means you'll only be alerted when the problem is more severe, giving you less time to react. This doesn't solve the false positive issue from transient spikes; it just makes you aware of problems later.",3:"Creating multiple time-based alerting policies is complex to maintain and error-prone. Duration-based thresholds naturally filter out transient spikes regardless of when they occur, without needing to anticipate specific time windows."}},{id:529,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Managing App Engine and Cloud Run resources - Cloud Logging",question:"Your application is logging errors, but you need to quickly find all error-level logs from the last hour for a specific Cloud Run service named 'api-service'. What is the most efficient way to query these logs?",options:["Use Cloud Logging with filter: resource.type='cloud_run_revision' AND resource.labels.service_name='api-service' AND severity>=ERROR AND timestamp>='-1h'","SSH into the Cloud Run container and read /var/log/application.log","Export all logs to BigQuery and run a SQL query to filter errors","Use gcloud logging read with appropriate filters for resource type, service name, severity, and timestamp"],correct:0,explanation:"Cloud Logging's query interface (Console or API) supports powerful filtering by resource type, labels, severity, timestamp, and more. The filter syntax allows you to precisely target logs from specific services with specific severities within a time range. This provides immediate results without any export or additional setup. Both the Console and gcloud commands support this filter syntax.",wrongExplanations:{1:"Cloud Run containers are serverless and ephemeralyou cannot SSH into them. Logs are automatically sent to Cloud Logging, which is the only way to access them. Container instances scale down to zero and are replaced, making local log files inaccessible.",2:"Exporting to BigQuery is useful for long-term analysis and complex queries across large datasets, but for quick operational queries like 'show me errors from the last hour', Cloud Logging's native query capabilities are faster and simpler.",3:"This is actually correct! gcloud logging read supports the same filter syntax. However, the Console UI (option A) is typically more user-friendly for ad-hoc queries and provides better visualization. Both approaches are valid; the question emphasizes using Cloud Logging's native capabilities."}},{id:530,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Managing App Engine and Cloud Run resources - Cloud Logging",question:"Your compliance team requires that all Cloud Logging logs for your production project must be retained for 7 years, but the default retention is only 30 days. How should you meet this requirement?",options:["Create a log sink that exports logs to Cloud Storage with a lifecycle policy to retain for 7 years, or to BigQuery with appropriate table expiration","Increase Cloud Logging retention settings to 7 years in the project settings","Export logs daily using a scheduled Cloud Function and store them in Cloud Storage","Configure log retention policies in Cloud Logging to 2555 days (7 years)"],correct:0,explanation:"Cloud Logging retains logs for a maximum of 30 days (default) or up to 3650 days (10 years) if you configure custom retention, but for long-term compliance, log sinks are recommended. Create a sink to export logs to Cloud Storage (for cost-effective long-term storage) or BigQuery (for queryable archives). Configure lifecycle policies or table expiration to match retention requirements. This separates operational logs from compliance archives.",wrongExplanations:{1:"While Cloud Logging does support custom retention up to 3650 days (which covers 7 years), this approach stores logs in Cloud Logging's infrastructure, which is more expensive than Cloud Storage for long-term archival. Log sinks to Cloud Storage are more cost-effective for compliance.",2:"Manual export via Cloud Functions adds complexity, potential points of failure, and doesn't guarantee completeness if the function fails. Log sinks are real-time, automatic, and designed for this exact use case with guarantees of delivery.",3:"Cloud Logging does support custom retention up to 3650 days, but this is less cost-effective than exporting to Cloud Storage for compliance archiving. The best practice is to use Cloud Logging for operational logs (30-90 days) and sinks for long-term compliance storage."}},{id:531,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Managing App Engine and Cloud Run resources - Cloud Logging",question:"You want to route different types of logs to different destinations: audit logs to BigQuery for analysis, application errors to Pub/Sub for alerting, and all logs to Cloud Storage for archival. How should you configure this?",options:["Create multiple log sinks in Cloud Logging, each with different inclusion filters and different destinations (BigQuery, Pub/Sub, Cloud Storage)","Create one sink to Cloud Storage, then use Cloud Functions to route copies to BigQuery and Pub/Sub based on log type","Export all logs to one destination, then process and redistribute them using Dataflow","Cloud Logging only supports one sink per project; choose the most important destination"],correct:0,explanation:"Cloud Logging supports multiple log sinks per project, each with its own inclusion/exclusion filters and destination. You can create one sink with filter 'logName:cloudaudit.googleapis.com'  BigQuery, another with filter 'severity>=ERROR'  Pub/Sub, and a third with no filter (all logs)  Cloud Storage. Each log entry is independently evaluated against all sinks, allowing flexible multi-destination routing.",wrongExplanations:{1:"While this approach works, it's more complex and costly than using Cloud Logging's native multi-sink capabilities. You'd pay for Cloud Storage ingress, Cloud Function invocations, and egress to other destinations. Cloud Logging sinks handle this natively.",2:"Similar to option 1, using Dataflow adds unnecessary complexity and cost for a use case that Cloud Logging's multiple sinks handle natively. Reserve Dataflow for complex log transformations or enrichment that sinks don't provide.",3:"This is incorrect. Cloud Logging explicitly supports multiple sinks per project, each with independent filters and destinations. There is no one-sink limitation."}},{id:532,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Managing App Engine and Cloud Run resources - Cloud Logging",question:"Your logs contain sensitive customer PII (personally identifiable information) such as email addresses and phone numbers. You need to redact this data before storing logs for compliance reasons. What should you do?",options:["Use Cloud Data Loss Prevention (DLP) API to inspect and redact PII from logs before they're written to log sinks","Configure log exclusion filters in Cloud Logging to drop logs containing PII patterns","Use Cloud Logging's built-in PII redaction feature in sink configuration","Write a Cloud Function that intercepts logs via Pub/Sub sink, redacts PII, and writes to final destination"],correct:3,explanation:"Cloud Logging doesn't have built-in PII redaction. The recommended pattern is: create a log sink to Pub/Sub, deploy a Cloud Function (or Cloud Run service) that processes messages from Pub/Sub, use the DLP API to redact sensitive data, then write the sanitized logs to the final destination (Cloud Storage, BigQuery). This creates a processing pipeline for log transformation while maintaining log delivery guarantees.",wrongExplanations:{0:"While DLP API is the right tool for redaction, you can't intercept logs before they reach Cloud Logging sinks. Logs are already in Cloud Logging when sinks process them. You need a processing step between the sink and final storage, typically using Pub/Sub + Cloud Function/Run + DLP API.",1:"Exclusion filters drop entire log entries; they don't redact specific fields. Dropping logs with PII would result in losing valuable operational data. The goal is to redact sensitive fields while preserving the rest of the log entry.",2:"Cloud Logging sinks don't have built-in PII redaction capabilities. Sinks export logs as-is to destinations. You must implement redaction in a processing layer between the sink and final storage."}},{id:533,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Managing storage and database solutions - Database Center (2025)",question:"You manage multiple Cloud SQL instances and want a centralized view to monitor performance, identify slow queries, and get optimization recommendations. Which Google Cloud tool introduced in 2025 provides this unified database management experience?",options:["Database Center in the Google Cloud Console","Cloud Monitoring with custom dashboards for Cloud SQL","Query Insights for Cloud SQL","Database Migration Service (DMS)"],correct:0,explanation:"Database Center is a unified console introduced in 2025 that provides centralized management and monitoring for multiple database instances across Cloud SQL, AlloyDB, and Spanner. It offers a single pane of glass for performance metrics, recommendations, query insights, and configuration management across your database fleet. This is the go-to tool for holistic database operations.",wrongExplanations:{1:"While Cloud Monitoring can track Cloud SQL metrics and you can build custom dashboards, it doesn't provide database-specific optimization recommendations, query analysis, or the specialized database management features that Database Center offers.",2:"Query Insights is a feature for analyzing slow queries within individual Cloud SQL instances. While powerful, it focuses on query-level analysis for a single instance, not fleet-wide management and recommendations that Database Center provides.",3:"Database Migration Service (DMS) is for migrating databases to Google Cloud (from on-premises or other clouds). It's not a monitoring or management tool for running databases; it's specifically for the migration process."}},{id:534,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Managing storage and database solutions - Database Center (2025)",question:"In Database Center, you notice a recommendation to enable automatic storage increases for one of your Cloud SQL instances. The instance currently has 100GB of storage with 85GB used. What is the benefit of implementing this recommendation?",options:["Automatic storage increases prevent downtime from running out of disk space; Cloud SQL will automatically add storage before reaching capacity","It reduces storage costs by dynamically shrinking storage when usage decreases","It improves query performance by pre-allocating storage for indexes","It enables automatic failover to a standby instance if storage fails"],correct:0,explanation:"Automatic storage increases in Cloud SQL monitor disk usage and automatically increase storage capacity before it's exhausted. This prevents application downtime that would occur if the database ran out of disk space. The feature is especially important for production databases where usage patterns may be unpredictable. Database Center highlights this recommendation when current storage is approaching limits.",wrongExplanations:{1:"Automatic storage increases only add capacitythey never shrink storage. Cloud SQL persistent disk storage cannot be decreased once allocated. The feature is purely for preventing capacity issues, not for cost optimization through reduction.",2:"While having adequate storage is necessary for performance, automatic storage increases don't pre-allocate or optimize storage for specific database features like indexes. The feature is about availability (preventing out-of-space errors), not performance tuning.",3:"Automatic storage increases address disk capacity, not high availability or failover. High availability is configured separately through HA configuration with standby instances. Storage increases ensure you don't run out of disk space."}},{id:535,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Managing storage and database solutions - Database Center (2025)",question:"You're using Database Center to monitor your fleet of Cloud SQL instances across dev, staging, and production environments. You want to filter the view to show only production instances. How can you accomplish this?",options:["Use labels on Cloud SQL instances (e.g., environment=production) and filter by labels in Database Center","Create separate Google Cloud projects for each environment and use Database Center's project filter","Database Center doesn't support filtering; you must manually identify production instances","Use Cloud Monitoring metrics scopes to separate production from non-production monitoring"],correct:0,explanation:"Labels are key-value pairs that you can attach to Google Cloud resources for organization and filtering. Database Center supports filtering by labels, allowing you to tag instances with labels like 'environment=production', 'team=backend', or 'cost-center=engineering'. This enables easy filtering and grouping in Database Center and other Google Cloud tools. Labels are the recommended approach for resource organization.",wrongExplanations:{1:"While separate projects per environment is a valid organizational strategy and Database Center can filter by project, it's more rigid than labels. Projects affect IAM, billing, and quotas. Labels provide flexible filtering without the overhead of multiple projects. For simple filtering, labels are more appropriate.",2:"This is incorrect. Database Center supports robust filtering by various attributes including labels, project, instance type, and more. Filtering is a core feature for managing large fleets of databases.",3:"Cloud Monitoring metrics scopes are for aggregating monitoring data across projects. While they could separate environments in monitoring, Database Center provides native filtering capabilities that are simpler and more direct for this use case."}},{id:536,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Managing storage and database solutions - Query Insights (2025)",question:"Query Insights shows that a specific query on your Cloud SQL instance is consuming 40% of total database resources and has high execution times. The query is called frequently by your application. What should you do first to optimize this?",options:["Review the query execution plan in Query Insights to identify missing indexes or inefficient operations; add appropriate indexes or rewrite the query","Increase the Cloud SQL instance's CPU and memory to handle the expensive query","Enable Cloud SQL query cache to cache query results","Implement connection pooling in the application to reduce query overhead"],correct:0,explanation:"Query Insights provides detailed execution plans showing how queries are executed, which tables are scanned, and whether indexes are used. The first optimization step is to analyze the execution plan to identify inefficiencies like full table scans, missing indexes, or suboptimal joins. Adding indexes or rewriting queries often yields dramatic performance improvements without increasing costs. This is the standard database optimization workflow.",wrongExplanations:{1:"Scaling up the instance (vertical scaling) increases costs without addressing the root cause. An inefficient query with a full table scan will still be inefficient on a larger instancejust faster by a small margin. Fix the query first, then scale if necessary.",2:"Query cache in Cloud SQL (MySQL) only helps with identical repeated queries that return the same results. If the query has parameters that vary or returns different results, the cache won't help. More importantly, query cache doesn't address the root issue of an expensive query that should be optimized.",3:"Connection pooling reduces connection overhead and improves concurrency but doesn't optimize the query execution itself. If a query is slow due to missing indexes or poor design, connection pooling won't make the query itself faster."}},{id:537,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Managing storage and database solutions - Query Insights (2025)",question:"Your team wants to identify which queries are causing the most load on your Cloud SQL database to prioritize optimization efforts. Query Insights is enabled. What metric should you focus on to find the highest-impact queries?",options:["Total query execution time (cumulative time across all executions)","Query execution frequency (number of times each query is called)","Average query latency per execution","Query CPU utilization percentage"],correct:0,explanation:"Total query execution time (cumulative across all executions) identifies queries with the highest overall impact on database resources. A query that runs frequently with moderate latency or a query that runs infrequently but takes very long both show up as high total time. This metric balances frequency and individual execution cost, making it the best metric for prioritizing optimization efforts. Query Insights highlights top queries by this metric.",wrongExplanations:{1:"Execution frequency alone can be misleading. A query that executes millions of times but is extremely fast (e.g., a simple key lookup) might not be worth optimizing compared to a less frequent but much slower query. Total execution time accounts for both frequency and cost.",2:"Average latency per execution shows which queries are individually slow but doesn't account for frequency. A query with high average latency that rarely runs has less overall impact than a moderately slow query that runs constantly. Total time is more actionable.",3:"CPU utilization is important but doesn't capture I/O wait time, lock contention, or other factors that contribute to query cost. Total execution time is a more comprehensive metric for overall query impact on the database."}},{id:538,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.5 Monitoring and logging - Cloud Trace",question:"Your microservices application is experiencing high latency. You want to trace requests across multiple services (Cloud Run, Cloud Functions, GKE) to identify which service is causing the bottleneck. What should you do?",options:["Enable Cloud Trace in your application using OpenTelemetry instrumentation; Cloud Trace will automatically correlate requests across services using trace context","Check Cloud Monitoring metrics for each service individually to compare latencies","Use Cloud Logging to search for error messages across services","Enable profiling on each service using Cloud Profiler to identify bottlenecks"],correct:0,explanation:"Cloud Trace provides distributed tracing that follows individual requests across service boundaries. When you instrument your code with OpenTelemetry (or Cloud Trace SDKs), trace context (trace ID, span ID) is propagated in HTTP headers between services. Cloud Trace correlates these spans into a single trace, showing the end-to-end request flow with timing for each service hop. This is specifically designed for microservices latency analysis.",wrongExplanations:{1:"Comparing metrics per service shows which services are generally slow but doesn't correlate individual requests. You can't see that 'request A spent 2s in service X and 0.1s in service Y'. Cloud Trace provides request-level correlation that metrics can't.",2:"Cloud Logging shows individual log entries but doesn't automatically correlate logs across services for a single request unless you manually implement correlation IDs. Cloud Trace is purpose-built for request tracing and provides visualization of the request path.",3:"Cloud Profiler shows CPU and memory usage within a single application, helping identify inefficient code paths. But it doesn't trace requests across multiple services or show network latency between services. Use Cloud Trace for distributed tracing and Cloud Profiler for code-level optimization."}},{id:539,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.5 Monitoring and logging - Cloud Trace",question:"You've enabled Cloud Trace for your application, but you're only seeing traces for a small percentage of requests. Your application handles thousands of requests per second. Why is this happening, and how can you see more traces if needed?",options:["Cloud Trace uses sampling to reduce overhead and costs; the default sampling rate is low (e.g., 0.1 requests per second). You can increase the sampling rate in your OpenTelemetry configuration if needed","Cloud Trace only traces requests that result in errors; successful requests aren't traced","Cloud Trace is in a startup phase and will begin tracing all requests within 24 hours","You need to enable 'full trace mode' in Cloud Trace settings to capture all requests"],correct:0,explanation:"Cloud Trace uses sampling to avoid overwhelming overhead on high-traffic applications. Tracing every request in a system handling thousands of QPS would generate massive amounts of data and impact performance. The default sampling rate is conservative (e.g., 0.1 req/s or 1 in 1000 requests). You can configure higher sampling rates in your OpenTelemetry or Cloud Trace SDK configuration based on your needs, balancing observability with overhead and cost.",wrongExplanations:{1:"Cloud Trace traces requests based on sampling configuration, not based on whether they succeed or fail. Both successful and failed requests are sampled according to the sampling rate. You can configure sampling to capture more errors if desired.",2:"There is no 'startup phase' or delayed activation for Cloud Trace. If tracing is enabled and instrumented, traces appear immediately based on the configured sampling rate. The low number of traces is due to sampling, not a warmup period.",3:"There is no 'full trace mode' setting in Cloud Trace. You control sampling through your instrumentation configuration (OpenTelemetry sampler configuration or Cloud Trace SDK settings). Sampling is configured in code or via environment variables, not in Cloud Trace console settings."}},{id:540,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.6 Managing storage and database solutions - Cloud Storage lifecycle",question:"Your application stores user-uploaded images in Cloud Storage. Images are frequently accessed in the first 30 days but rarely accessed afterward. You want to optimize costs while maintaining availability. What should you do?",options:["Create an Object Lifecycle Management policy to transition objects to Nearline storage after 30 days and to Coldline after 90 days","Manually move old objects to Nearline storage monthly using a Cloud Function","Use Autoclass storage; it automatically transitions objects to appropriate storage classes based on access patterns","Delete objects after 30 days and recreate them from backups if needed"],correct:2,explanation:"Autoclass (introduced for Standard storage buckets) automatically moves objects between Standard, Nearline, Coldline, and Archive storage classes based on access patterns. It optimizes costs without manual configuration of lifecycle rules. For the use case described (frequent access initially, rare access later), Autoclass automatically handles transitions, providing the simplest solution. It also moves objects back to Standard if access patterns change.",wrongExplanations:{0:"Lifecycle Management policies work well for predictable, time-based transitions, and this is a valid approach. However, Autoclass is simpler because it adapts to actual access patterns rather than fixed time rules. If some images are accessed beyond 30 days, Autoclass won't prematurely downgrade them.",1:"Manual processes are error-prone, operationally expensive, and don't scale. Cloud Storage provides built-in solutions (Lifecycle policies or Autoclass) that are more reliable and require no ongoing manual intervention.",3:"Deleting data to save costs and relying on backups defeats the purpose of cloud storage. Users expect data to remain available. The question asks to 'maintain availability' while optimizing costsAutoclass or Lifecycle policies reduce storage costs while keeping data accessible."}},{id:541,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.6 Managing storage and database solutions - Cloud Storage versioning",question:"A user accidentally deleted an important file from a Cloud Storage bucket. The file was deleted 2 days ago. Object Versioning is enabled on the bucket. How can you recover the file?",options:["Use the Cloud Storage Console or gsutil to list object versions and restore the deleted version","Recover the file from the Cloud Storage trash/recycle bin within the Console","File cannot be recovered after 24 hours even with versioning enabled","Contact Google Cloud Support to restore the file from backups"],correct:0,explanation:"Object Versioning in Cloud Storage maintains multiple versions of objects, including deleted objects. When an object is deleted, it's actually replaced with a delete marker (a new version). Previous versions remain accessible. You can use the Console, gsutil (gsutil ls -a gs://bucket/object for all versions, gsutil cp gs://bucket/object#generation for restore), or the API to list versions and restore deleted objects. This is a self-service recovery process.",wrongExplanations:{1:"Cloud Storage doesn't have a 'trash' or 'recycle bin' concept like local filesystems. Versioning serves this purpose. With versioning disabled, deletions are permanent. With versioning enabled, you recover by accessing previous versions, not via a trash bin UI.",2:"This is incorrect. With Object Versioning enabled, non-current versions are retained according to the bucket's retention policy or lifecycle rules. By default, they're retained indefinitely until explicitly deleted or removed by lifecycle rules. The 24-hour limit doesn't apply to versioned objects.",3:"Google Cloud Support cannot restore objects from backupsthey don't maintain backups of customer data in Cloud Storage. Data durability and availability are your responsibility using features like Object Versioning, replication, and backups to other buckets. Versioning is the recovery mechanism."}},{id:542,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.6 Managing storage and database solutions - Cloud SQL maintenance",question:"Your Cloud SQL instance is scheduled for automatic maintenance during your business hours, which could cause a brief disruption. How can you minimize the impact on your application?",options:["Configure a custom maintenance window during off-peak hours; enable high availability (HA) configuration to minimize downtime during maintenance","Disable automatic maintenance to prevent disruptions","Maintenance cannot be rescheduled; design your application to handle brief database unavailability","Export the database before maintenance and restore it afterward to avoid disruption"],correct:0,explanation:"Cloud SQL allows you to configure a maintenance window to schedule automatic maintenance during low-traffic periods (e.g., 3 AM on Sundays). Enabling HA configuration (with standby instance) minimizes downtime during maintenancethe standby can serve traffic during updates. These configurations together reduce maintenance impact to typically less than 1 minute. This is the recommended operational approach for production databases.",wrongExplanations:{1:"Disabling automatic maintenance defers critical security patches and updates, leaving your database vulnerable. While you can defer maintenance, Google Cloud will eventually force required updates for security reasons. The correct approach is to schedule maintenance appropriately, not disable it.",2:"While applications should handle transient database unavailability (connection retries, circuit breakers), you can minimize disruption by scheduling maintenance windows. This is incorrect because Cloud SQL explicitly allows custom maintenance windows for this purpose.",3:"Exporting and restoring databases for every maintenance would cause hours of downtime (far worse than maintenance itself), data loss (changes during export), and operational complexity. HA configuration and maintenance windows provide much simpler solutions."}},{id:543,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Custom roles",question:"Your security team needs to grant developers the ability to view Compute Engine instances and start/stop them, but not delete them or modify configurations. The predefined roles are too broad. What should you do?",options:["Create a custom IAM role with permissions: compute.instances.get, compute.instances.list, compute.instances.start, compute.instances.stop; grant this role to developers","Grant the predefined Compute Viewer role plus compute.instances.start and compute.instances.stop permissions","Create a custom role that includes the Compute Instance Admin role minus delete permissions","Use IAM conditions on the Compute Instance Admin role to deny delete operations"],correct:0,explanation:"Custom IAM roles allow you to create roles with precisely the permissions needed, following the principle of least privilege. You select specific permissions from Google Cloud's permission catalog (e.g., compute.instances.start) and group them into a custom role. This is the standard approach when predefined roles are too broad or don't match your requirements. Custom roles can be defined at the organization or project level.",wrongExplanations:{1:"IAM roles cannot be combined or modified by adding individual permissions. You either grant a predefined role as-is, or you create a custom role with the exact permissions you need. You can't grant 'Compute Viewer plus extra permissions'that requires a custom role.",2:"Custom roles are created from individual permissions, not by modifying other roles. There's no concept of 'role inheritance' or 'role minus permissions'. You explicitly list the permissions your custom role includes, selected from the permission catalog.",3:"IAM conditions control when a role applies (based on resource attributes, date/time, IP address), not which specific permissions within a role are granted. You can't use conditions to selectively disable certain permissions from a role. Custom roles are the mechanism for permission selection."}},{id:544,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Role recommendations",question:"You granted a service account the Editor role on a project 6 months ago. You suspect the service account may have excessive permissions that it's not using. How can you identify unused permissions to improve security?",options:["Use IAM Recommender to analyze the service account's activity over the past 90 days; it will suggest removing unused permissions or switching to a less privileged role","Review Cloud Audit Logs manually to identify which APIs the service account has called","Remove the Editor role, wait for the application to break, then grant only the permissions that were needed","Use Policy Analyzer to see all permissions the service account has"],correct:0,explanation:"IAM Recommender uses machine learning to analyze service account and user activity over time (90 days by default). It identifies permissions that haven't been used and recommends removing them or switching to more restrictive predefined or custom roles. This automated approach is safer than manual analysis and follows Google's recommendation for continuous right-sizing of IAM permissions. Recommendations can be applied directly or used as guidance.",wrongExplanations:{1:"While reviewing Audit Logs can show what actions were performed, manually correlating API calls to IAM permissions across 6 months of data is extremely time-consuming and error-prone. IAM Recommender automates this analysis with ML-based insights.",2:"This 'break-then-fix' approach causes application downtime and production incidents. It's not a safe or professional way to adjust permissions. IAM Recommender provides insights without disrupting services.",3:"Policy Analyzer shows what permissions are granted (current state) but doesn't analyze whether those permissions are actually used. It's useful for understanding policy structure but doesn't provide usage-based recommendations like IAM Recommender does."}},{id:545,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - IAM conditions",question:"You want to grant a contractor temporary access to a Cloud Storage bucket, but only during business hours (9 AM - 5 PM) and only from your office IP range (203.0.113.0/24). How can you implement this in IAM?",options:["Grant the Storage Object Viewer role with IAM conditions specifying the time range (request.time) and source IP (origin.ip) restrictions using 'gcloud storage buckets add-iam-policy-binding gs://BUCKET --member=user:contractor@example.com --role=roles/storage.objectViewer --condition=EXPRESSION'","Create a custom role that includes time and IP restrictions in the role definition","Use VPC firewall rules to restrict access to the bucket by IP, and use a Cloud Scheduler job to grant/revoke the role on a schedule","IAM doesn't support time or IP-based restrictions; implement this logic in your application"],correct:0,explanation:`IAM conditions allow you to add conditional logic to role bindings using Common Expression Language (CEL). Apply conditions using 'gcloud storage buckets add-iam-policy-binding gs://BUCKET --member=user:contractor@example.com --role=roles/storage.objectViewer --condition="expression=request.time >= timestamp('2024-01-01T09:00:00Z') && request.time <= timestamp('2024-12-31T17:00:00Z') && origin.ip in ['203.0.113.0/24'],title=BusinessHoursOfficeIP"'. You can create conditions based on resource attributes, date/time (request.time), source IP (origin.ip), resource tags, and more. This allows fine-grained, context-aware access control. The condition is evaluated on every request, automatically enforcing the time and IP restrictions without manual intervention.`,wrongExplanations:{1:"Custom roles define sets of permissions, but they don't support conditional logic. Conditions are separate from role definitionsthey're applied to role bindings (who gets what role under what conditions). Custom roles answer 'what permissions', conditions answer 'when/where/under what circumstances'.",2:"VPC firewall rules control network traffic, not IAM authentication and authorization. Cloud Storage access is via HTTPS APIs, not network-level connections that firewalls control. Cloud Scheduler with role grant/revoke is complex, brittle, and has gaps (role remains active until next scheduled revocation).",3:"IAM conditions explicitly support time-based and IP-based restrictions. This is a core feature introduced to enable context-aware access control. Application-level logic would be redundant and wouldn't protect against direct API calls bypassing the application."}},{id:546,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Principle of least privilege",question:"A developer asks for the Owner role on a project to deploy a Cloud Function. They explain they need 'full control' to configure the function, set environment variables, and grant the function's service account appropriate permissions. What should you do?",options:["Grant the Cloud Functions Developer role for function deployment and Cloud Functions Service Agent for runtime permissions; avoid granting Owner role which includes billing and IAM administration","Grant the Owner role as requested; developers should have full control of their projects","Grant Editor role instead of Owner; it provides nearly the same permissions but without billing access","Create a custom role with exactly the permissions the developer mentioned"],correct:0,explanation:"The Owner role includes highly sensitive permissions like modifying IAM policies, changing billing accounts, and deleting projects. For deploying Cloud Functions, the Cloud Functions Developer role provides sufficient permissions (deploy, configure, set environment variables). If the developer needs to manage service account IAM, grant specific service account roles. Always follow least privilegegrant the minimum permissions necessary for the task. Owner should be reserved for project administrators.",wrongExplanations:{1:"Owner role is extremely broad and should be restricted to a small number of trusted administrators. It includes billing management, IAM policy administration, and the ability to grant themselves any permission. Most operational tasks don't require Owner.",2:"Editor role is still very broadit includes permissions to create, modify, and delete almost all resources. For specific tasks like deploying Cloud Functions, more targeted roles like Cloud Functions Developer are more appropriate and safer.",3:"While a custom role is an option, Google's predefined roles for specific services (like Cloud Functions Developer) are well-designed for common use cases and maintained by Google. Creating custom roles should be reserved for scenarios where predefined roles don't fit. Start with predefined roles."}},{id:547,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Resource hierarchy",question:"You have three projects (dev, staging, prod) in your organization. You want to grant a user read-only access to Cloud Storage buckets in all three projects without granting access individually per project. What is the most efficient approach?",options:["Create a folder containing the three projects; grant the Storage Object Viewer role at the folder level; the user inherits access to all buckets in child projects","Grant the Storage Object Viewer role at the organization level","Write a script to grant the role in each of the three projects","Use gcloud to grant the role with --all-projects flag"],correct:0,explanation:"IAM policies inherit down the resource hierarchy (Organization  Folder  Project  Resource). By organizing related projects into a folder and granting IAM roles at the folder level, you simplify permission management. The user automatically gets access to resources in all current and future projects in that folder. This is the recommended pattern for managing permissions across multiple related projects. Changes at the folder level propagate automatically.",wrongExplanations:{1:"Granting at the organization level would give access to Storage buckets in every project in the entire organization, which violates least privilege if there are other projects the user shouldn't access. Folder-level grants provide the right scope for groups of related projects.",2:"While a script works, it doesn't scale and requires re-execution whenever new projects are added to the group. Folder-level IAM grants are declarative, automatically apply to new projects, and are easier to audit and maintain.",3:"There is no --all-projects flag in gcloud for IAM operations. IAM bindings are scoped to specific resources (organization, folder, project, or individual resource). The folder hierarchy is the mechanism for applying permissions across multiple projects."}},{id:548,domain:"Configuring access and security",subdomain:"4.2 Managing service accounts - Workload Identity",question:"Your application running in GKE needs to access Cloud Storage. You want to follow best practices for authentication without managing service account keys. What should you do?",options:["Enable Workload Identity on the GKE cluster and namespace with 'gcloud container clusters update CLUSTER --workload-pool=PROJECT_ID.svc.id.goog'; bind the Kubernetes service account to a Google Cloud service account using 'gcloud iam service-accounts add-iam-policy-binding'","Create a service account key, store it in a Kubernetes Secret, and mount it as a volume in your pods","Use the Compute Engine default service account and grant it Cloud Storage permissions","Create a Cloud Storage HMAC key and use it for authentication"],correct:0,explanation:`Workload Identity is Google's recommended way to authenticate GKE workloads to Google Cloud APIs. Enable it with 'gcloud container clusters update CLUSTER --workload-pool=PROJECT_ID.svc.id.goog --zone=ZONE', then bind Kubernetes service accounts to Google service accounts using 'gcloud iam service-accounts add-iam-policy-binding GSA_NAME@PROJECT_ID.iam.gserviceaccount.com --role=roles/iam.workloadIdentityUser --member="serviceAccount:PROJECT_ID.svc.id.goog[NAMESPACE/KSA_NAME]"'. Pods using that K8s service account automatically get credentials. This eliminates key management, rotation, and the security risks of stored keys.`,wrongExplanations:{1:"Service account keys stored in Kubernetes Secrets are a security anti-pattern. Keys can be extracted, leaked, or stolen. They require manual rotation and management. Workload Identity eliminates the need for keys entirely by using Google's infrastructure to securely provide credentials.",2:"The Compute Engine default service account has Editor permissions on the project by default, which violates least privilege. Additionally, using node-level service accounts means all pods on a node share the same permissions, preventing fine-grained access control per application. Workload Identity provides pod-level identity.",3:"HMAC keys are for S3-compatible API access to Cloud Storage, not the recommended JSON API. They have the same key management problems as service account keys. Workload Identity is the modern, secure, keyless authentication approach for GKE."}},{id:549,domain:"Configuring access and security",subdomain:"4.2 Managing service accounts - Service account keys",question:"Your on-premises application needs to call Google Cloud APIs. A team member suggests creating a service account key and downloading the JSON file to use for authentication. What security considerations should you address?",options:["Service account keys are long-lived credentials that pose security risks if compromised; implement key rotation (rotate every 90 days), store keys securely (never in code repositories), and monitor key usage via Cloud Audit Logs","Service account keys are safe to use; Google manages their security automatically","Avoid service account keys entirely; use Workload Identity Federation to authenticate on-premises workloads without keys","Service account keys expire automatically after 30 days, so they're safe for temporary use"],correct:2,explanation:"Workload Identity Federation allows on-premises and multi-cloud workloads to authenticate to Google Cloud without service account keys by establishing trust with external identity providers (AWS, Azure AD, OIDC providers). This keyless authentication approach is more secure and the recommended pattern. If keys are unavoidable, option A describes necessary security practices, but avoiding keys entirely (option C) is the best practice. Both A and C are valid, but C represents the ideal security posture.",wrongExplanations:{0:"While this answer describes correct security practices for service account keys (rotation, secure storage, monitoring), it's not the best answer because Workload Identity Federation eliminates the need for keys entirely. However, if keys are unavoidable (legacy systems), these practices are mandatory.",1:"Service account keys don't expire automatically and aren't managed by Google after download. They remain valid until explicitly deleted, which can be years. If a key is leaked, it poses a security risk until revoked. Keys require active management and monitoring.",3:"Service account keys do not expire automatically. They remain valid indefinitely until manually deleted or rotated. This is part of why they're riskylost or leaked keys can be used by attackers for extended periods if not actively managed."}},{id:550,domain:"Configuring access and security",subdomain:"4.2 Managing service accounts - Impersonation",question:"You're developing an application locally and need to test it with the same permissions it will have in production. The production application uses a specific service account. You don't want to download service account keys. How can you test with production permissions?",options:["Use service account impersonation: grant yourself the Service Account Token Creator role on the production service account, then use gcloud --impersonate-service-account flag or client library impersonation","Create a service account key for the production service account and use it locally; delete it after testing","Create a separate development service account with identical permissions","Run gcloud auth application-default login and use your user credentials; your permissions should match the service account"],correct:0,explanation:"Service account impersonation allows authorized users to obtain short-lived tokens for a service account without keys. With the Service Account Token Creator role (or iam.serviceAccounts.getAccessToken permission), you can impersonate the service account for testing. Use gcloud --impersonate-service-account or client libraries' impersonation features. This provides production-equivalent permissions without key management and is audited in Cloud Audit Logs.",wrongExplanations:{1:"Creating and downloading keys creates security risk and operational overhead (remembering to delete the key, ensuring it's not committed to version control). Impersonation provides temporary credentials without keys and is the recommended approach for development and testing.",2:"Creating a duplicate service account is extra work and doesn't guarantee identical permissions over time as production permissions evolve. Impersonation tests against the actual production service account identity, ensuring permission parity.",3:"Your user credentials have your user's IAM permissions, which are likely different from the service account's permissions. Using user credentials doesn't accurately test how the application will behave in production with the service account's specific permissions."}},{id:551,domain:"Configuring access and security",subdomain:"4.2 Managing service accounts - Service account best practices",question:"You're designing a microservices architecture on GKE where different services need different Google Cloud API permissions. How should you structure service accounts to follow security best practices?",options:["Create a unique service account for each microservice with only the permissions that specific service needs; use Workload Identity to bind each Kubernetes service account to its corresponding Google service account","Use a single service account with all necessary permissions shared across all microservices for simplicity","Use the GKE default service account for all services; it has sufficient permissions","Create one service account per namespace with all permissions needed by services in that namespace"],correct:0,explanation:"One service account per microservice follows the principle of least privilege and provides strong isolation. If one service is compromised, the blast radius is limited to that service's permissions. Combined with Workload Identity (binding K8s service accounts to Google service accounts), this provides fine-grained, pod-level access control. This is the recommended architecture for secure microservices on GKE.",wrongExplanations:{1:"Shared service accounts across multiple services violate least privilege and increase blast radius. If any service is compromised, an attacker gets all the permissions of the shared account, potentially affecting other services. Fine-grained service accounts limit damage from compromises.",2:"The GKE default service account (or Compute Engine default service account) typically has overly broad permissions and is shared across the cluster. Using default accounts prevents fine-grained access control and makes it impossible to enforce least privilege per service.",3:"While namespace-level service accounts are better than cluster-wide sharing, they're still too broad. Different microservices in the same namespace likely have different permission needs. Service-level granularity provides the best security posture and isolation."}},{id:552,domain:"Configuring access and security",subdomain:"4.3 Viewing audit logs - Security best practices",question:"Your security team wants to be alerted whenever IAM policy changes are made in your production project (role grants, revocations, or modifications). How should you implement this?",options:["Create a log sink for Admin Activity audit logs with filter logName:'cloudaudit.googleapis.com/activity' AND protoPayload.serviceName='iam.googleapis.com'; export to Pub/Sub; trigger Cloud Function for alerting","Enable Data Access audit logs for IAM and create an alerting policy in Cloud Monitoring","Use Cloud Logging's built-in IAM change alerting feature","Create a log-based metric for IAM changes and set up a Cloud Monitoring alert on the metric"],correct:0,explanation:"Admin Activity audit logs capture IAM policy changes automatically (always enabled, no configuration needed). Creating a log sink with appropriate filters allows you to route these logs to Pub/Sub, where a Cloud Function can process them in real-time and send alerts (email, Slack, PagerDuty). This is the standard pattern for custom alerting on specific audit log events. Alternatively, option D (log-based metrics) is also valid for numeric thresholds.",wrongExplanations:{1:"Data Access audit logs are for data read/write operations (e.g., reading Cloud Storage objects, querying BigQuery), not administrative operations like IAM changes. IAM changes are captured in Admin Activity logs, which are always enabled. Data Access logs wouldn't capture IAM policy modifications.",2:"Cloud Logging doesn't have a built-in 'IAM change alerting feature'. You must create log sinks, log-based metrics, or use other mechanisms to build alerting on top of audit logs. Logging provides the log data; you build alerting workflows.",3:"Log-based metrics extract numeric values from logs and can trigger Cloud Monitoring alerts. This works well for counting occurrences ('alert if more than 5 IAM changes in 1 hour'). However, for real-time detailed alerting with full context, Pub/Sub + Cloud Function provides more flexibility. Both approaches are valid depending on requirements."}},{id:553,domain:"Configuring access and security",subdomain:"4.4 Managing network security - VPC firewall rules",question:"Your application running on Compute Engine instances with tag 'web-server' needs to accept HTTPS traffic from the internet and SSH traffic only from your office IP (203.0.113.0/24). How should you configure VPC firewall rules?",options:["Create two ingress allow rules: 1) priority 1000, source 0.0.0.0/0, target tag 'web-server', tcp:443; 2) priority 1000, source 203.0.113.0/24, target tag 'web-server', tcp:22","Create one ingress allow rule with source 0.0.0.0/0, target tag 'web-server', ports tcp:443,22; security is handled by the application","Create egress rules to allow outbound HTTPS and SSH traffic","Create ingress allow rules at the subnet level rather than using tags"],correct:0,explanation:"VPC firewall rules control traffic based on source/destination IP ranges, ports, protocols, and target tags (or service accounts). Two separate rules provide precise control: one allows HTTPS from anywhere (0.0.0.0/0) for public web traffic, another restricts SSH to your office IP range. Using target tags ('web-server') ensures rules apply only to relevant instances. This follows the principle of least privilege for network access.",wrongExplanations:{1:"Allowing SSH from 0.0.0.0/0 (the entire internet) is a major security vulnerability. SSH should be restricted to known, trusted IP ranges (office, VPN, bastion hosts). One rule cannot have different source IP restrictions for different ports; you need separate rules.",2:"Firewall rules control ingress (inbound) traffic to instances. Egress rules control outbound traffic from instances. For accepting HTTPS and SSH connections from external sources, you need ingress rules. Egress rules wouldn't allow external clients to initiate connections to your instances.",3:"VPC firewall rules can target all instances in a subnet, but using network tags provides more flexibility. You can apply tags to specific instances regardless of subnet, allowing mixed-purpose subnets. Tags are the recommended approach for organizing firewall rules by application function."}},{id:554,domain:"Configuring access and security",subdomain:"4.4 Managing network security - Private Google Access",question:"Your Compute Engine instances don't have external IP addresses for security reasons, but they need to access Google Cloud APIs (Cloud Storage, BigQuery). Instances can't currently reach these services. What should you do?",options:["Enable Private Google Access on the subnet; instances without external IPs can then reach Google APIs via internal IP routes","Grant instances external IPs to access Google APIs","Configure Cloud NAT to allow instances to reach Google APIs","Create VPN tunnels to Google's API endpoints"],correct:0,explanation:"Private Google Access allows instances without external IPs to reach Google APIs and services using internal routing through Google's network. You enable it per subnet in VPC settings. Instances route API requests to internal IP addresses (199.36.153.8/30 or restricted.googleapis.com) that resolve to Google services, keeping traffic within Google's network. This is the standard solution for private instances accessing Google APIs.",wrongExplanations:{1:"Adding external IPs defeats the security requirement of keeping instances private. While it would enable API access, it exposes instances to the internet. Private Google Access provides API access while maintaining private instance posture.",2:"Cloud NAT allows instances without external IPs to initiate outbound connections to the internet (non-Google destinations). While Cloud NAT can route traffic to Google APIs, Private Google Access is more efficient (stays on Google's network, no NAT overhead) and is the recommended approach specifically for Google API access.",3:"VPN tunnels are for connecting on-premises networks or other clouds to GCP. They're not needed for Compute Engine instances to access Google APIs. Private Google Access provides direct, internal routing without VPN complexity."}},{id:555,domain:"Configuring access and security",subdomain:"4.4 Managing network security - VPC firewall rules priority",question:"You have a deny all ingress firewall rule (priority 65534, implied deny rule). You need to allow HTTP traffic to instances with tag 'web-server' while keeping everything else blocked. What should you do?",options:["Create an ingress allow rule with priority lower than 65534 (e.g., 1000), source 0.0.0.0/0, target tag 'web-server', tcp:80; lower priority numbers take precedence over higher priority numbers","Create an ingress allow rule with priority higher than 65534 to override the deny","Delete the deny all rule, then create the allow rule","Deny rules always take precedence over allow rules regardless of priority"],correct:0,explanation:"VPC firewall rules are evaluated in priority order, with lower numbers evaluated first. Rules with priority 0-999 are evaluated before the implied deny (65534). When you create an allow rule with priority 1000, it's evaluated before the default deny, allowing HTTP traffic to tagged instances while keeping the default deny for all other traffic. This is the standard pattern for allowing specific traffic through a default-deny firewall posture.",wrongExplanations:{1:"Priority numbers in VPC firewall rules work opposite to intuition: lower numbers = higher priority (evaluated first). Priority 65535 is the lowest priority. You cannot create rules with priority higher than 65534 to override the default deny; you must use lower numbers (0-65533).",2:"The implied deny rule (priority 65534) is automatic and cannot be deleted. It ensures that any traffic not explicitly allowed by higher-priority rules (lower priority numbers) is denied. You don't delete it; you create allow rules with lower priority numbers that are evaluated first.",3:"This is incorrect. VPC firewall rules follow priority order regardless of allow/deny action. If an allow rule has lower priority number (evaluated first) than a deny rule, the allow rule takes precedence. Deny doesn't automatically override allow."}},{id:556,domain:"Configuring access and security",subdomain:"4.5 Cloud NGFW Enterprise (2025) - Advanced firewall",question:"Your security team wants centralized, advanced firewall capabilities including intrusion detection, URL filtering, and threat intelligence for your GKE clusters and Compute Engine instances. What Google Cloud service introduced in 2025 should you use?",options:["Cloud NGFW (Next Generation Firewall) Enterprise, which provides advanced Layer 7 security features integrated with VPC","VPC firewall rules with custom IDS/IPS software on each instance","Cloud Armor for DDoS protection and WAF capabilities","Third-party firewall appliances deployed in the VPC"],correct:0,explanation:"Cloud NGFW Enterprise, launched in 2025, provides next-generation firewall capabilities including intrusion detection/prevention (IDS/IPS), URL filtering, threat intelligence, and advanced threat protection. It integrates natively with VPC and is managed centrally, providing consistent security across Compute Engine, GKE, and other workloads. This is Google's strategic solution for advanced network security beyond basic VPC firewall rules.",wrongExplanations:{1:"Deploying IDS/IPS software per instance doesn't scale, creates management overhead, lacks centralized visibility, and introduces performance overhead on each instance. Cloud NGFW provides centralized, managed capabilities without per-instance software.",2:"Cloud Armor is for DDoS protection and web application firewall (WAF) for HTTP(S) applications behind load balancers. It operates at Layer 7 for HTTP traffic. Cloud NGFW provides broader network-level security (IDS/IPS, URL filtering) across all protocols and workloads, not just HTTP(S) applications.",3:"While third-party appliances can provide NGFW features, they require manual deployment, licensing, scaling, and maintenance. Cloud NGFW Enterprise is Google-managed, integrates natively with VPC and other GCP services, and scales automatically. It's the preferred solution for native GCP environments."}},{id:557,domain:"Configuring access and security",subdomain:"4.5 Cloud NGFW Enterprise (2025) - Threat prevention",question:"After enabling Cloud NGFW Enterprise, you want to block traffic from known malicious IPs and URLs identified by Google's threat intelligence. How should you configure this?",options:["Enable threat prevention profiles in Cloud NGFW that use Google's threat intelligence feeds; configure policies to block traffic matching threat signatures","Manually import threat intelligence lists and create VPC firewall rules for each IP","Use Cloud Armor security policies with IP blocklists","Configure Cloud IDS (Intrusion Detection System) separately from Cloud NGFW"],correct:0,explanation:"Cloud NGFW Enterprise includes threat prevention capabilities powered by Google's threat intelligence. You configure threat prevention profiles that define actions for traffic matching threat signatures (malicious IPs, URLs, domains, known attack patterns). The firewall automatically blocks or alerts on threats without manual rule creation. Threat intelligence feeds are continuously updated by Google, providing up-to-date protection against emerging threats.",wrongExplanations:{1:"Manually maintaining threat intelligence lists doesn't scale and can't keep up with rapidly evolving threats. Threat intelligence includes millions of indicators updated constantly. Cloud NGFW's integrated threat prevention automates this with Google's continuously updated feeds.",2:"Cloud Armor is for HTTP(S) application-level protection behind load balancers, not general network-level threat prevention. While Cloud Armor has IP blocklists, Cloud NGFW provides broader threat detection across protocols and network layers with automated threat intelligence integration.",3:"Cloud IDS was a separate service before Cloud NGFW Enterprise. Cloud NGFW Enterprise integrates IDS/IPS functionality along with threat prevention, URL filtering, and other NGFW features in a unified service. You don't need to configure Cloud IDS separately when using Cloud NGFW Enterprise."}},{id:558,domain:"Configuring access and security",subdomain:"4.5 Cloud NGFW Enterprise (2025) - Policy management",question:"You want to apply different Cloud NGFW security policies to different types of workloads: strict policies for production, more permissive policies for development environments. How can you implement this?",options:["Create multiple firewall endpoint associations, each with different security profiles and policies; associate endpoints with VPC networks or subnets based on environment","Cloud NGFW only supports one global policy that applies to all traffic","Use VPC firewall rules in combination with Cloud NGFW; VPC rules for development, Cloud NGFW for production","Create separate VPCs for each environment; Cloud NGFW policies are per-VPC"],correct:0,explanation:"Cloud NGFW Enterprise uses firewall endpoints that can be associated with VPC networks or specific subnets. You can create multiple endpoints with different security profiles (threat prevention, IDS/IPS sensitivity, URL filtering rules) and associate them based on workload type. For example, production subnets use strict endpoints, development subnets use more permissive endpoints. This provides flexible, environment-specific security policies while using a single NGFW service.",wrongExplanations:{1:"Cloud NGFW Enterprise supports multiple endpoints with different policies, not just one global policy. This flexibility is key to the serviceallowing customized security postures for different workloads, environments, and security requirements within the same organization.",2:"Mixing VPC firewall rules and Cloud NGFW adds complexity and potential conflicts. Cloud NGFW is designed to provide comprehensive security, and you can configure it with different policies per environment. Using inconsistent security tools makes management harder.",3:"While separate VPCs per environment is a valid architecture pattern, Cloud NGFW doesn't require it. You can apply different NGFW policies within a single VPC by associating different endpoints with different subnets. This is more flexible than requiring complete VPC separation."}},{id:559,domain:"Configuring access and security",subdomain:"4.6 Managing encryption - CMEK",question:"Your compliance requirements mandate that you control the encryption keys used to encrypt data in Cloud Storage and BigQuery. You need the ability to revoke access to data by disabling keys. What should you do?",options:["Use Customer-Managed Encryption Keys (CMEK) with Cloud KMS; create keys in Cloud KMS and specify them when creating Cloud Storage buckets and BigQuery datasets; you can disable keys to revoke access","Use Customer-Supplied Encryption Keys (CSEK) where you provide keys with each API request","Use Google-managed encryption keys; they provide the same level of control","Enable encryption at rest in Cloud Storage and BigQuery settings"],correct:0,explanation:"CMEK allows you to create and manage encryption keys in Cloud KMS while Google handles the encryption operations. You control key lifecycle (rotation, disabling, destruction) and can revoke access to data by disabling the key. When you create Cloud Storage buckets or BigQuery datasets, you specify a Cloud KMS key, and Google uses it for encryption. This meets compliance requirements for key control while leveraging Google's encryption infrastructure. Disabling a CMEK key immediately prevents decryption of data.",wrongExplanations:{1:"CSEK requires you to provide the encryption key with every API request for encryption/decryption operations. This is operationally complex, requires secure key distribution to all clients, and doesn't integrate with Cloud IAM for access control. CMEK provides better operational simplicity while meeting key control requirements.",2:"Google-managed encryption keys (default encryption) are managed entirely by Google. You have no control over key lifecycle, rotation policies, or the ability to disable keys. For compliance requiring key control, you must use CMEK or CSEK.",3:"Encryption at rest is enabled by default in Google Cloud using Google-managed keys. This provides encryption but not key control. 'Enabling encryption' isn't a setting you changeit's always on. CMEK is how you gain control over the keys used for that encryption."}},{id:561,domain:"Setting up a cloud solution environment",subdomain:"1.1 Gemini Cloud Assist + 1.2 Billing",question:"You are a new engineer. Your manager asks you to write a gcloud command to create five Spot VMs with the n2-standard-4 machine type in 'us-central1-a' using the latest Debian 11 image, and to find out how this will affect your project's costs. What is the most efficient way to accomplish this using new Google Cloud features?",options:["Ask Gemini Cloud Assist to 'generate a gcloud command to create 5 Spot n2-standard-4 VMs with debian-11' and then separately use the Pricing Calculator.","Write the command from memory, run it, and then check the Billing export to BigQuery the next day to see the cost.","Ask Gemini Cloud Assist: 'What's the gcloud command to create 5 Spot n2-standard-4 VMs and how much will they cost?' Gemini can provide the command and a cost estimate.","Use the gcloud cheat sheet to find the command, then create a budget alert for the cost, and then run the command."],correct:2,explanation:"A key feature of Gemini Cloud Assist is its ability to answer multi-part questions, including generating CLI commands and providing cost estimations in one conversational flow, based on the resources described. ",wrongExplanations:{0:"This is a valid, but less efficient, two-step process. Gemini can often handle both parts in a single query.",1:"Running the command without knowing the cost is dangerous and does not provide an estimate. Billing data is also not real-time.",3:"This is a good manual process, but Gemini is designed to accelerate this by providing the command and estimate directly."}},{id:562,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Cloud NGFW + 4.1 Secure Tags",question:"You are using Cloud NGFW (Next-Generation Firewall). You need to allow VMs tagged with the secure tag `role=payment-processor` to make egress connections on TCP port 443 to any destination. All other traffic should be denied. What is the correct way to configure this?",options:["Create a VPC firewall rule with priority 1000, source tag `role=payment-processor`, and allow tcp:443.","Create a network firewall policy with a rule of priority 1000, source secure tag `role=payment-processor`, action 'allow', and protocol tcp:443.","VPC firewall rules and Cloud NGFW policies cannot be combined. You must use one or the other.","Create a Cloud Armor policy to allow egress traffic based on the tag."],correct:1,explanation:"Cloud NGFW uses 'network firewall policies' which are more advanced than legacy VPC firewall rules. A key feature is the ability to use IAM-controlled 'secure tags' (not network tags) as the source or destination for a rule. [cite: 910, 911]",wrongExplanations:{0:"Legacy VPC firewall rules use 'network tags', not 'secure tags'. Cloud NGFW uses 'network firewall policies' which are a different resource.",2:"While you should migrate to one, they can coexist, but the question is about the *correct* way to use the new NGFW features.",3:"Cloud Armor is a WAF for external load balancers and does not control egress traffic from VMs."}},{id:563,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Database Center + 3.4 Query Insights",question:"You are a fleet manager using the **Database Center** dashboard and notice that a specific Cloud SQL instance has a 'High CPU' warning. What is the most logical *next step* to diagnose the root cause of this high CPU?",options:["Increase the vCPU count for the instance immediately to resolve the performance issue.","Use the links in Database Center to navigate directly to the **Query Insights** dashboard for that instance to identify the specific queries causing the load.","Check Cloud Logging to see if the database instance has any error messages.","Use `gcloud sql instances describe` to check the instance's configuration."],correct:1,explanation:"Database Center  provides the high-level 'what' (High CPU). Query Insights  provides the 'why' (which query is at fault). The intended workflow is to use Database Center to monitor the fleet and then drill down into Query Insights for per-instance diagnostics.",wrongExplanations:{0:"Scaling up is a reactive fix that doesn't solve the root cause. An inefficient query will just consume more CPU on a larger instance. You must diagnose first.",2:"Logs are useful, but high CPU is a performance issue, not necessarily an error. Query Insights is the specific tool for diagnosing query performance.",3:"Describing the instance shows its configuration, not its real-time performance or the specific queries that are running."}},{id:564,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Database selection (AlloyDB vs. Spanner vs. SQL)",question:"A financial services company is migrating a 10TB mission-critical PostgreSQL database. They require 99.99% availability, full PostgreSQL compatibility, and transaction performance that is 3-4x faster than standard PostgreSQL. What is the best-fit managed service?",options:["Cloud SQL for PostgreSQL with a high-availability configuration.","AlloyDB for PostgreSQL.","Cloud Spanner with the PostgreSQL interface.","Manually managing a PostgreSQL cluster on Compute Engine."],correct:1,explanation:"AlloyDB  is specifically designed for this. It offers full PostgreSQL compatibility, a 99.99% SLA, and significantly higher performance for transactional workloads (advertised as 4x faster) than standard PostgreSQL.",wrongExplanations:{0:"Cloud SQL's HA SLA is 99.95% [cite: 24] and it offers *standard* PostgreSQL performance, not the 3-4x required.",2:"Cloud Spanner is for global scale and horizontal sharding. While it has a PostgreSQL interface, it is not 100% wire-compatible and is designed for a different architectural pattern than a traditional PostgreSQL lift-and-shift.",3:"Manual management on Compute Engine creates massive operational overhead and does not come with a 99.99% SLA by default."}},{id:565,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Compute (GKE Autopilot vs. Cloud Run)",question:"You have a complex, multi-container application defined by Kubernetes YAML files (Deployments, Services, etc.). You want a serverless, managed environment that can run these manifests directly with minimal changes, but you do not want to manage nodes. What should you use?",options:["Cloud Run, by deploying each container as a separate service.","GKE Autopilot cluster.","Cloud Functions, by rewriting the containers as functions.","App Engine Flexible."],correct:1,explanation:"GKE Autopilot [cite: 900] is the correct choice because it consumes standard Kubernetes manifests directly. It provides a serverless *Kubernetes* experience, abstracting nodes while keeping the full Kubernetes API. Cloud Run can't run complex, multi-container YAMLs directly.",wrongExplanations:{0:"Cloud Run is container-based but does not use the Kubernetes resource model (Deployments, Services) directly. It would require a significant re-architecture of the application's deployment.",2:"Rewriting to Cloud Functions is a major change and is not suitable for a complex, existing Kubernetes application.",3:"App Engine Flexible is a PaaS platform, not a Kubernetes environment. It cannot consume Kubernetes YAML files."}},{id:566,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC (Fabric FAST vs. Terraform)",question:"Your enterprise is starting its Google Cloud adoption. You need to deploy a secure, multi-project foundation with networking, IAM, and billing already configured according to Google's best practices. You want to use Terraform but want to avoid writing all the modules from scratch. What should you use?",options:["Google's Fabric FAST framework.","Cloud Deployment Manager.","Config Connector.","gcloud init."],correct:0,explanation:"Fabric FAST (part of the Cloud Foundation Toolkit)  is a set of pre-built, best-practice Terraform modules specifically designed to deploy a secure, enterprise-ready Google Cloud foundation. It's the 'fast start' for organizations that want to use Terraform without reinventing the wheel.",wrongExplanations:{1:"Cloud Deployment Manager is Google's native IaC tool, but it is not as widely adopted as Terraform. Fabric FAST is specifically the Terraform-based solution.",2:"Config Connector is for managing GCP resources *from* Kubernetes, not for setting up the initial foundation.",3:"`gcloud init` [cite: 52] configures your local CLI, it does not deploy an entire enterprise-scale cloud foundation."}},{id:567,domain:"Configuring access and security",subdomain:"4.1 IAM Conditions",question:"A contractor needs to access a production VM to perform a one-time fix. You want to grant them `roles/compute.osAdminLogin` for *only* the next 4 hours. What is the most secure and automated way to do this?",options:["Grant them the role and set a calendar reminder to revoke it in 4 hours.","Grant them the role with an IAM Condition that specifies a 4-hour expiration timestamp.","Give them the credentials to a shared admin service account for 4 hours.","Use `gcloud compute ssh` [cite: 122] to SSH in for them and share your terminal."],correct:1,explanation:"IAM Conditions allow for time-limited access. You can add a condition to the role binding that automatically expires at a specific timestamp. This is the most secure and auditable method, as it requires no manual cleanup and enforces the time limit programmatically.",wrongExplanations:{0:"Manual revocation is error-prone. If you forget, the contractor has permanent admin access.",2:"Sharing service account credentials is a security anti-pattern and should be avoided.",3:"Sharing your terminal is insecure and not scalable."}},{id:568,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Ops Agent vs. Legacy Agents",question:"You are setting up monitoring for a new fleet of Compute Engine VMs. You need to collect system metrics (like memory and disk usage), application logs (from `/var/log/myapp.log`), and metrics from a third-party app (like Nginx). What is the single, unified agent you should install?",options:["The legacy Cloud Logging agent and the legacy Cloud Monitoring agent.","The Cloud Trace agent.","The Ops Agent.","The GKE metrics agent."],correct:2,explanation:"The Ops Agent is Google's unified agent [cite: 958] that replaces the older, separate Logging and Monitoring agents. It is built on OpenTelemetry and can collect logs (from files, syslog) and metrics (system, third-party apps like Nginx/Redis) in a single, configurable agent.",wrongExplanations:{0:"Using the two legacy agents is no longer the best practice. The Ops Agent is the modern, recommended solution that unifies both functions.",1:"The Cloud Trace agent is for collecting distributed tracing spans, not for logs or system metrics.",3:"There is no 'GKE metrics agent'. GKE monitoring is integrated into the node image."}},{id:569,domain:"Configuring access and security",subdomain:"4.2 GKE Workload Identity",question:"Your application, running as a pod in a GKE cluster, needs to write objects to a Cloud Storage bucket. Following the principle of least privilege and avoiding static keys, what is the correct sequence of steps?",options:["Create a GCS bucket. Grant the GKE node's service account `roles/storage.objectCreator`. All pods on the node will inherit this permission.","Create a Google Service Account (GSA) with `roles/storage.objectCreator`. Create a service account key, store it as a Kubernetes secret, and mount it into the pod.","Enable Workload Identity on the cluster. Create a Google Service Account (GSA) with `roles/storage.objectCreator`. Bind the pod's Kubernetes Service Account (KSA) to the GSA.","Make the GCS bucket public, and configure the pod to write to the public URL."],correct:2,explanation:"This is the modern, secure, and recommended pattern. Workload Identity [cite: 974] allows a Kubernetes Service Account (KSA) to impersonate a Google Service Account (GSA). This gives pod-level, fine-grained permissions without using the node's service account or managing static JSON keys.",wrongExplanations:{0:"Using the node's service account is bad practice. It grants all pods on that node the same broad permissions, violating least privilege.",1:"Storing static keys in Kubernetes secrets is the old method and is a security risk. Workload Identity is the keyless, more secure replacement.",3:"Making the bucket public is a massive security vulnerability."}},{id:570,domain:"Setting up a cloud solution environment",subdomain:"1.2 Billing Automation",question:"Your company policy requires that any project exceeding its $1,000 monthly budget must be automatically shut down. What is the most reliable way to implement this?",options:["Set a billing budget alert for 100%. The alert will automatically stop all resources in the project.","Set a billing budget alert for 100% that sends a message to a Pub/Sub topic. Create a Cloud Function triggered by that topic that programmatically disables billing for the project.","Set a hard quota on the project for $1,000.","Grant the finance team `roles/billing.projectManager` so they can manually disable billing when they receive the 100% alert email."],correct:1,explanation:"This is a key automation pattern. Billing alerts *do not* stop spending[cite: 100, 1301, 111]; they only send notifications. To automate action, you send the alert to a Pub/Sub topic, which triggers a Cloud Function. That function can then call the Cloud Billing API to disable the project's billing, effectively shutting down all resources.",wrongExplanations:{0:"This is incorrect. Billing alerts *only* notify; they never take action or stop spending.",2:'Quotas are based on resource counts (e.g., "10 VMs"), not on dollar amounts. You cannot set a quota for $1,000.',3:"This relies on manual intervention, which is not reliable or fast enough to prevent cost overruns from a runaway script."}}],Lp=Et.reduce((M,Z)=>(M[Z.domain]||(M[Z.domain]=[]),M[Z.domain].push(Z),M),{}),Pl=Et.length,Up=Et.reduce((M,Z)=>Math.max(M,Z.id),0),Qi=()=>Array.from({length:Up+1},()=>null),Hi=()=>Array.from({length:Up+1},()=>!1),Yp=M=>{const Z=Math.min(Math.max(0,M),Pl),te=Object.entries(Lp).map(([L,D])=>{const R=D.length/Pl*Z,I=Math.floor(R);return{domain:L,total:D.length,allocated:Math.min(I,D.length),remainder:R-I}});let m=te.reduce((L,D)=>L+D.allocated,0);const K=Math.min(Z,te.reduce((L,D)=>L+D.total,0));if(m<K){const L=[...te].sort((R,I)=>I.remainder-R.remainder);let D=0;for(;m<K&&L.length>0;){const R=L[D%L.length];if(R.allocated<R.total&&(R.allocated+=1,m+=1),D+=1,D>L.length*4)break}}if(m>K){const L=[...te].sort((R,I)=>R.remainder-I.remainder);let D=0;for(;m>K&&L.length>0;){const R=L[D%L.length];if(R.allocated>0&&(R.allocated-=1,m-=1),D+=1,D>L.length*4)break}}return te.map(({domain:L,allocated:D,total:R})=>({domain:L,count:D,total:R}))},cm=()=>{const[M,Z]=he.useState(!0),[te,m]=he.useState(!1),[K,L]=he.useState(!1),[D,R]=he.useState("landing"),[I,k]=he.useState(null),[re,N]=he.useState(10),[ke,st]=he.useState(""),[Ze,ze]=he.useState(null),[Se,De]=he.useState(Qi),[Fe,Ie]=he.useState(Hi),[Qe,ct]=he.useState("all"),[nt,J]=he.useState(!1),[Pe,Je]=he.useState(null),[Ne,Ve]=he.useState(null),[be,ut]=he.useState(null),[Ke,Ae]=he.useState(0),[b,E]=he.useState({}),[V,ue]=he.useState(null),[me,d]=he.useState(!1),[S,T]=he.useState(null),[P,H]=he.useState([]),[F,le]=he.useState(null),W=he.useMemo(()=>I?I.map(x=>Et.find(A=>A.id===x)).filter(Boolean):Et,[I]),Ee=he.useMemo(()=>Qe==="all"?W:W.filter(x=>{const A=Se[x.id]===x.correct;return Qe==="correct"?A:!A}),[W,Qe,Se]),_=W[Ke],Bt=_?Se[_.id]:null,oi=he.useMemo(()=>{const x=W.length;let A=0,j=0;W.forEach(O=>{Se[O.id]!==null&&(A+=1),Fe[O.id]&&(j+=1)});const U=Math.max(0,x-A);return{total:x,answered:A,unanswered:U,flagged:j}},[W,Se,Fe]),Fi=oi.answered;he.useEffect(()=>{Ke>=W.length&&Ae(Math.max(0,W.length-1))},[W,Ke]);const bt=x=>{const A=[...x];for(let j=A.length-1;j>0;j--){const U=Math.floor(Math.random()*(j+1));[A[j],A[U]]=[A[U],A[j]]}return A},Ki=x=>{const A=Math.min(Math.max(0,x),Pl),j=Yp(A),U=[];if(j.forEach(({domain:O,count:q})=>{if(q<=0)return;const Q=[...Lp[O]||[]],oe=bt(Q).slice(0,Math.min(q,Q.length));U.push(...oe.map(Ye=>Ye.id))}),U.length<A){const O=Et.filter(q=>!U.includes(q.id));U.push(...bt(O).slice(0,A-U.length).map(q=>q.id))}return bt(U).slice(0,A)},ii=x=>{const A={};return x.forEach(j=>{const U=Et.find(O=>O.id===j);if(U){const O=Array.from({length:U.options.length},(q,Q)=>Q);A[j]=bt(O)}}),A};he.useEffect(()=>{if(D!=="practice"||!_){D!=="practice"&&Ve(null);return}if(Bt===null){Ve(null);return}const x=Bt===_.correct;Ve(A=>A&&A.questionId===_.id&&A.selectedOption===Bt&&A.isCorrect===x?A:{questionId:_.id,isCorrect:x,selectedOption:Bt})},[D,_,Bt]);const _i=()=>{const x=bt(Et.map(U=>U.id)).slice(0,Math.min(50,Et.length));ut(null),ze(null),Ve(null),J(!1),Je(null),ct("all"),k(x),De(Qi()),Ie(Hi());const A=ii(x);E(A),Ae(0);const j=Date.now();T(j),ue(7200),d(!1),localStorage.setItem("examStartTime",j.toString()),localStorage.setItem("examSessionIds",JSON.stringify(x)),localStorage.setItem("shuffledOptions",JSON.stringify(A)),R("exam"),typeof window<"u"&&Z(window.innerWidth>900)},Xi=()=>{k(null),Ae(0),ue(null),T(null),E({}),De(Qi()),Ie(Hi()),ut(null),Ve(null),ze(null),J(!1),Je(null),localStorage.removeItem("examStartTime"),localStorage.removeItem("examSessionIds"),localStorage.removeItem("shuffledOptions")},ni=()=>{const x=W.length,A=W.filter(Q=>Se[Q.id]===Q.correct).length,j=x===0?0:Math.round(A/x*100),U=S?Math.floor((Date.now()-S)/1e3):0,q=[{id:Date.now().toString(),date:Date.now(),score:A,total:x,percentage:j,timeSpent:U,questionIds:I||[],userAnswers:[...Se],shuffledOptions:{...b}},...P].slice(0,20);H(q),localStorage.setItem("examHistory",JSON.stringify(q)),ue(null),T(null),d(!1),localStorage.removeItem("examStartTime"),localStorage.removeItem("examSessionIds"),localStorage.removeItem("shuffledOptions"),J(!1),Je(null),R("results")},ai=x=>{const{answered:A,unanswered:j,flagged:U,total:O}=oi;if(A===0){alert("You haven't answered any questions yet!");return}if(!(x!=null&&x.bypassFlagged)&&U>0){Je({flagged:U,unanswered:j,answered:A,total:O}),J(!0);return}const q=j>0?`You have answered ${A} of ${O} questions (${j} unanswered).

Are you sure you want to submit your exam?`:`You have answered all ${O} questions.

Are you sure you want to submit your exam?`;window.confirm(q)&&ni()},Zi=()=>{const x=W.findIndex(A=>Fe[A.id]);x>=0&&Ae(x),J(!1),Je(null)},Ji=x=>{const A=Math.floor(x/3600),j=Math.floor(x%3600/60),U=x%60;return`${A}:${j.toString().padStart(2,"0")}:${U.toString().padStart(2,"0")}`},cs=x=>{const A=Math.min(Math.max(1,x),Et.length),j=Ki(A);if(j.length===0){alert("Unable to start practice session  no questions available.");return}ut(null),k(j),De(Qi()),Ie(Hi()),ct("all"),J(!1),Je(null);const U=ii(j);E(U),Ae(0),Ve(null),ze(Date.now()),R("practice"),typeof window<"u"&&Z(window.innerWidth>900)},rt=()=>{if(!W.length)return;const x=W.filter(q=>Se[q.id]===null).length;if(x>0&&!window.confirm(`You still have ${x} unanswered question${x===1?"":"s"}.

Do you want to finish the practice session anyway?`))return;const A={};W.forEach(q=>{A[q.domain]||(A[q.domain]={correct:0,total:0,answered:0}),A[q.domain].total+=1;const Q=Se[q.id];Q!==null&&(A[q.domain].answered+=1,Q===q.correct&&(A[q.domain].correct+=1))});const j=Object.values(A).reduce((q,Q)=>q+Q.correct,0),U=Object.values(A).reduce((q,Q)=>q+Q.answered,0),O=Ze?Math.floor((Date.now()-Ze)/1e3):0;ut({correct:j,total:W.length,answered:U,timeSpent:O,breakdown:A}),ze(null),Ve(null),R("practiceResults")};he.useEffect(()=>{const x=()=>{const A=window.innerWidth<=900;m(A),Z(!A)};return x(),window.addEventListener("resize",x),()=>window.removeEventListener("resize",x)},[]),he.useEffect(()=>{if(D!=="exam"||V===null)return;const x=setInterval(()=>{ue(A=>A===null||A<=0?(clearInterval(x),alert("Time is up! Your exam will now be submitted."),ai({bypassFlagged:!0}),0):(A===900&&!me&&(d(!0),alert("Warning: Only 15 minutes remaining!")),A-1))},1e3);return()=>clearInterval(x)},[D,V,me]),he.useEffect(()=>{const x=localStorage.getItem("examHistory");if(x)try{const A=JSON.parse(x);H(A)}catch(A){console.error("Failed to load exam history",A)}},[]),he.useEffect(()=>{const x=localStorage.getItem("examStartTime"),A=localStorage.getItem("examSessionIds"),j=localStorage.getItem("shuffledOptions");if(x&&A&&D==="exam"){const U=parseInt(x),O=Math.floor((Date.now()-U)/1e3),q=Math.max(0,7200-O);if(q>0){if(ue(q),T(U),j)try{E(JSON.parse(j))}catch(Q){console.error("Failed to restore shuffled options",Q)}}else localStorage.removeItem("examStartTime"),localStorage.removeItem("examSessionIds"),localStorage.removeItem("shuffledOptions")}},[]);const Kn=x=>{if(!_)return;const A=b[_.id],j=A?A[x]:x,U=[...Se];U[_.id]=j,De(U),D==="practice"&&Ve({questionId:_.id,isCorrect:j===_.correct,selectedOption:j})},_n=()=>{if(!_)return;const x=[...Fe];x[_.id]=!x[_.id],Ie(x)},$i=()=>{const x=["id","question","selected","correct","flagged"],A=W.map(Q=>{const oe=Se[Q.id],Ye=Q.correct;return[Q.id,'"'+Q.question.replace(/"/g,'""')+'"',oe===null?"":oe.toString(),Ye.toString(),Fe[Q.id]?"1":"0"].join(",")}),j=[x.join(","),...A].join(`
`),U=new Blob([j],{type:"text/csv"}),O=URL.createObjectURL(U),q=document.createElement("a");q.href=O,q.download="results.csv",q.click(),URL.revokeObjectURL(O)};if(D==="landing")return c.jsx("div",{className:"landing-hero",style:{minHeight:"100vh",display:"flex",alignItems:"center",justifyContent:"center",background:"linear-gradient(180deg, #0b4bd8 0%, #0b2e63 100%)",color:"#fff",padding:0,margin:0},children:c.jsxs("div",{style:{width:"100%",textAlign:"center",padding:"80px 24px"},children:[c.jsx("div",{style:{display:"flex",justifyContent:"center",marginBottom:18},"aria-hidden":!0,children:c.jsxs("svg",{width:"72",height:"48",viewBox:"0 0 64 48",fill:"none",xmlns:"http://www.w3.org/2000/svg",children:[c.jsx("defs",{children:c.jsxs("linearGradient",{id:"g1",x1:"0",x2:"1",children:[c.jsx("stop",{offset:"0",stopColor:"#fff",stopOpacity:"0.95"}),c.jsx("stop",{offset:"1",stopColor:"#e6f0ff",stopOpacity:"0.9"})]})}),c.jsx("path",{d:"M20 30c-6 0-10-5-10-10 0-5 4-10 10-10 2 0 4 0 6 1 3-5 9-6 15-3 5 3 7 9 5 14-1 4-5 8-11 8H20z",fill:"url(#g1)"})]})}),c.jsxs("h1",{style:{fontSize:56,margin:"0 0 8px",fontWeight:800,lineHeight:1.05},children:["Google Cloud",c.jsx("br",{}),"Associate Engineer"]}),c.jsx("p",{style:{marginTop:8,fontSize:18,opacity:.95},children:"Practice Exam - 2025 Edition  50 Questions"}),c.jsx("div",{style:{marginTop:36,display:"flex",justifyContent:"center"},children:c.jsxs("div",{style:{width:"min(820px, 92%)",textAlign:"center",padding:"26px 30px",borderRadius:14,background:"rgba(0,0,0,0.12)"},children:[c.jsx("h2",{style:{color:"#fff",marginTop:0,marginBottom:6},children:"Start New Exam"}),c.jsx("p",{style:{color:"rgba(255,255,255,0.9)",marginTop:0},children:"Full-length practice exam simulating the real GCE certification test. All questions include explanations."}),c.jsxs("div",{style:{display:"flex",gap:12,justifyContent:"center",marginTop:18,flexWrap:"wrap"},children:[c.jsx("button",{onClick:()=>{_i(),Z(!0)},style:{background:"#1a73e8",color:"#fff",border:"none",padding:"12px 22px",borderRadius:8,fontSize:16},children:"Start New Exam "}),c.jsx("button",{onClick:()=>{R("practiceConfig")},style:{background:"transparent",color:"#fff",border:"2px solid rgba(255,255,255,0.16)",padding:"10px 18px",borderRadius:8,fontSize:16},children:"Practice"}),c.jsxs("button",{onClick:()=>R("history"),style:{background:"transparent",color:"#fff",border:"2px solid rgba(255,255,255,0.16)",padding:"10px 18px",borderRadius:8,fontSize:16},children:[" Exam History (",P.length,")"]})]})]})})]})});if(D==="practiceConfig"){const x=[5,10,20,50],A=re,j=Math.min(A,Et.length),U=Yp(A);return c.jsx("div",{className:"config-container",style:{minHeight:"100vh",padding:"40px 20px",background:"linear-gradient(to bottom right, #1e293b, #1e40af, #1e293b)"},children:c.jsx("div",{style:{maxWidth:800,margin:"0 auto"},children:c.jsxs("div",{style:{background:"rgba(51,65,85,0.95)",borderRadius:16,padding:40,backdropFilter:"blur(12px)",border:"1px solid rgba(255,255,255,0.1)"},children:[c.jsx("h1",{style:{margin:"0 0 12px",fontSize:32,color:"#f9fafb"},children:"Configure Practice Session"}),c.jsx("p",{style:{color:"#9ca3af",margin:"0 0 32px"},children:"Select the number of questions for your practice set. Questions will be proportionally distributed across all domains."}),c.jsxs("div",{style:{marginBottom:32},children:[c.jsx("label",{style:{color:"#f9fafb",fontSize:16,fontWeight:600,display:"block",marginBottom:12},children:"Number of Questions"}),c.jsx("div",{style:{display:"flex",gap:12,flexWrap:"wrap",marginBottom:16},children:x.map(O=>c.jsx("button",{onClick:()=>N(O),style:{padding:"12px 24px",borderRadius:8,border:re===O?"2px solid #1a73e8":"2px solid rgba(255,255,255,0.2)",background:re===O?"rgba(26,115,232,0.2)":"rgba(255,255,255,0.05)",color:"#fff",fontSize:16,fontWeight:600,cursor:"pointer"},children:O},O))}),c.jsxs("div",{style:{display:"flex",gap:12,alignItems:"center"},children:[c.jsx("input",{type:"number",min:"1",max:Et.length,value:ke,onChange:O=>{st(O.target.value);const q=parseInt(O.target.value);q>0&&q<=Et.length&&N(q)},placeholder:"Custom (1-500)",style:{padding:"12px",borderRadius:8,border:"2px solid rgba(255,255,255,0.2)",background:"rgba(255,255,255,0.05)",color:"#fff",fontSize:16,width:180}}),c.jsx("span",{style:{color:"#9ca3af"},children:"or enter a custom number"})]})]}),c.jsxs("div",{style:{background:"rgba(26,115,232,0.1)",border:"1px solid rgba(26,115,232,0.3)",borderRadius:12,padding:24,marginBottom:32},children:[c.jsxs("h3",{style:{margin:"0 0 16px",fontSize:18,color:"#f9fafb"},children:["Distribution Preview (",j," questions)"]}),c.jsx("div",{style:{display:"grid",gap:12},children:U.map(({domain:O,total:q,count:Q})=>c.jsxs("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",padding:"8px 0",borderBottom:"1px solid rgba(255,255,255,0.1)",gap:"12px"},children:[c.jsx("span",{style:{color:"#e5e7eb",fontSize:14,flex:1},children:O}),c.jsxs("span",{style:{color:"#60a5fa",fontWeight:600,whiteSpace:"nowrap"},children:[Q," / ",q]})]},O))})]}),c.jsxs("div",{style:{display:"flex",gap:12,justifyContent:"flex-end"},children:[c.jsx("button",{onClick:()=>R("landing"),style:{padding:"12px 24px",borderRadius:8,border:"2px solid rgba(255,255,255,0.2)",background:"transparent",color:"#fff",fontSize:16,cursor:"pointer"},children:"Cancel"}),c.jsx("button",{onClick:()=>cs(re),style:{padding:"12px 24px",borderRadius:8,border:"none",background:"#1a73e8",color:"#fff",fontSize:16,fontWeight:600,cursor:"pointer"},children:"Start Practice Session "})]})]})})})}if(D==="practiceResults"){if(!be)return c.jsx("div",{className:"practice-results-container",style:{minHeight:"100vh",padding:"40px 20px",background:"linear-gradient(to bottom right, #1e293b, #1e40af, #1e293b)"},children:c.jsx("div",{style:{maxWidth:800,margin:"0 auto"},children:c.jsxs("div",{style:{background:"rgba(51,65,85,0.95)",borderRadius:16,padding:40,textAlign:"center",color:"#f9fafb"},children:[c.jsx("h1",{style:{marginTop:0},children:"Practice Summary Unavailable"}),c.jsx("p",{style:{color:"#9ca3af"},children:"Start a new practice session to view results."}),c.jsx("button",{onClick:()=>R("landing"),style:{marginTop:24,padding:"12px 24px",borderRadius:8,border:"none",background:"#1a73e8",color:"#fff",fontSize:16,fontWeight:600,cursor:"pointer"},children:"Back to Home"})]})})});const x=be.total>0?Math.round(be.correct/be.total*100):0,A=be.total>0?Math.round(be.answered/be.total*100):0,j=Ji(be.timeSpent||0),U=Object.entries(be.breakdown).sort((O,q)=>O[0].localeCompare(q[0]));return c.jsx("div",{className:"practice-results-container",children:c.jsxs("div",{className:"practice-results-inner",children:[c.jsxs("section",{className:"practice-results-header",children:[c.jsx("h1",{className:"practice-results-title",children:"Practice Session Summary"}),c.jsxs("div",{className:"practice-results-stats",children:[c.jsxs("div",{className:"practice-stat-card",children:[c.jsxs("div",{className:"practice-stat-value primary",children:[x,"%"]}),c.jsx("div",{className:"practice-stat-label",children:"Overall Accuracy"})]}),c.jsxs("div",{className:"practice-stat-card",children:[c.jsxs("div",{className:"practice-stat-value",children:[be.correct," / ",be.total]}),c.jsx("div",{className:"practice-stat-label",children:"Correct Answers"})]}),c.jsxs("div",{className:"practice-stat-card",children:[c.jsxs("div",{className:"practice-stat-value",children:[be.answered," / ",be.total]}),c.jsxs("div",{className:"practice-stat-label",children:["Answered (",A,"%)"]})]}),c.jsxs("div",{className:"practice-stat-card",children:[c.jsx("div",{className:"practice-stat-value",children:j}),c.jsx("div",{className:"practice-stat-label",children:"Time Spent"})]})]}),c.jsxs("div",{className:"practice-results-actions",children:[c.jsx("button",{onClick:()=>{R("practice"),Ae(0),typeof window<"u"&&Z(window.innerWidth>900)},className:"practice-action-button secondary",type:"button",children:"Review Questions"}),c.jsx("button",{onClick:()=>{ut(null),k(null),De(Qi()),Ie(Hi()),E({}),Ve(null),ze(null),Ae(0),R("practiceConfig")},className:"practice-action-button primary",type:"button",children:"Start New Practice Set "}),c.jsx("button",{onClick:()=>{ut(null),k(null),De(Qi()),Ie(Hi()),Ve(null),ze(null),R("landing"),Z(!1)},className:"practice-action-button tertiary",type:"button",children:"Back to Home"})]})]}),c.jsxs("section",{className:"practice-results-breakdown",children:[c.jsx("h2",{className:"practice-breakdown-title",children:"Performance by Domain"}),c.jsx("div",{className:"practice-table-wrapper",children:c.jsxs("table",{className:"practice-table",children:[c.jsx("thead",{children:c.jsxs("tr",{children:[c.jsx("th",{className:"practice-table-header left",children:"Domain"}),c.jsx("th",{className:"practice-table-header",children:"Correct"}),c.jsx("th",{className:"practice-table-header",children:"Answered"}),c.jsx("th",{className:"practice-table-header",children:"Total"}),c.jsx("th",{className:"practice-table-header",children:"Accuracy"})]})}),c.jsx("tbody",{children:U.map(([O,q])=>{const Q=q.total>0?Math.round(q.correct/q.total*100):0,oe=q.correct===q.total;return c.jsxs("tr",{className:oe?"full-correct":"",children:[c.jsx("td",{className:"practice-table-cell left",children:O}),c.jsx("td",{className:"practice-table-cell",children:q.correct}),c.jsx("td",{className:"practice-table-cell",children:q.answered}),c.jsx("td",{className:"practice-table-cell",children:q.total}),c.jsxs("td",{className:`practice-table-cell ${Q>=70?"high-accuracy":"low-accuracy"}`,children:[Q,"%"]})]},O)})})]})})]})]})})}if(D==="history")return c.jsx("div",{className:"history-container",style:{minHeight:"100vh",padding:"40px 20px",background:"linear-gradient(to bottom right, #1e293b, #1e40af, #1e293b)"},children:c.jsxs("div",{style:{maxWidth:1200,margin:"0 auto"},children:[c.jsxs("div",{style:{background:"rgba(51,65,85,0.95)",borderRadius:16,padding:32,marginBottom:32,backdropFilter:"blur(12px)",border:"1px solid rgba(255,255,255,0.1)"},children:[c.jsxs("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",marginBottom:24},children:[c.jsx("h1",{style:{margin:0,fontSize:32,color:"#f9fafb"},children:" Exam History"}),c.jsx("button",{onClick:()=>R("landing"),className:"nav-btn",children:"Back to Home"})]}),c.jsxs("p",{style:{color:"#9ca3af",margin:0},children:["Review your past ",P.length," exam attempt",P.length!==1?"s":""," (showing last 20)"]})]}),P.length===0?c.jsxs("div",{style:{background:"rgba(51,65,85,0.95)",borderRadius:16,padding:64,textAlign:"center",backdropFilter:"blur(12px)",border:"1px solid rgba(255,255,255,0.1)"},children:[c.jsx("div",{style:{fontSize:48,marginBottom:16},children:""}),c.jsx("h2",{style:{color:"#f9fafb",marginBottom:8},children:"No Exam History Yet"}),c.jsx("p",{style:{color:"#9ca3af",marginBottom:24},children:"Complete an exam to see your results here"}),c.jsx("button",{onClick:()=>{_i(),Z(!0)},style:{background:"linear-gradient(135deg, rgba(59,130,246,0.9), rgba(96,165,250,0.9))",color:"#fff",border:"none",padding:"12px 24px",borderRadius:8,fontSize:16,fontWeight:600,cursor:"pointer"},children:"Start Your First Exam"})]}):c.jsx("div",{style:{display:"grid",gap:16},children:P.map((x,A)=>{const j=x.percentage>=70,U=new Date(x.date),O=Ji(x.timeSpent);return c.jsx("div",{style:{background:"rgba(51,65,85,0.95)",borderRadius:12,padding:24,backdropFilter:"blur(12px)",border:`2px solid ${j?"rgba(52,211,153,0.3)":"rgba(248,113,113,0.3)"}`,boxShadow:"0 8px 24px rgba(0,0,0,0.2)",cursor:"pointer",transition:"transform 0.2s"},onMouseEnter:q=>q.currentTarget.style.transform="translateY(-2px)",onMouseLeave:q=>q.currentTarget.style.transform="translateY(0)",onClick:()=>{le(x),k(x.questionIds),De(x.userAnswers),E(x.shuffledOptions),Ae(0),R("results")},children:c.jsxs("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"flex-start",gap:16,flexWrap:"wrap"},children:[c.jsxs("div",{style:{flex:1},children:[c.jsxs("div",{style:{display:"flex",alignItems:"center",gap:12,marginBottom:8},children:[c.jsxs("div",{style:{fontSize:32,fontWeight:800,color:j?"#34d399":"#f87171"},children:[x.percentage,"%"]}),c.jsx("div",{style:{padding:"4px 12px",borderRadius:6,fontSize:14,fontWeight:600,background:j?"rgba(52,211,153,0.2)":"rgba(248,113,113,0.2)",color:j?"#34d399":"#f87171"},children:j?"PASSED":"FAILED"})]}),c.jsxs("div",{style:{color:"#9ca3af",fontSize:14,marginBottom:4},children:["Score: ",x.score,"/",x.total," questions correct"]}),c.jsxs("div",{style:{color:"#9ca3af",fontSize:14,marginBottom:4},children:["Time: ",O]}),c.jsxs("div",{style:{color:"#9ca3af",fontSize:14},children:["Date: ",U.toLocaleDateString()," at ",U.toLocaleTimeString()]})]}),c.jsx("div",{children:c.jsx("button",{onClick:q=>{q.stopPropagation(),le(x),k(x.questionIds),De(x.userAnswers),E(x.shuffledOptions),Ae(0),R("results")},style:{background:"linear-gradient(135deg, rgba(59,130,246,0.9), rgba(96,165,250,0.9))",color:"#fff",border:"none",padding:"10px 20px",borderRadius:8,fontSize:14,fontWeight:600,cursor:"pointer"},children:"Review "})})]})},x.id)})})]})});if(D==="results"){const x=F?F.score:W.filter(oe=>Se[oe.id]===oe.correct).length,A=F?F.total:W.length,j=A-x,U=F?F.percentage:Math.round(x/A*100),O=U>=70,q=Ee.length,Q=[{key:"all",label:"All",count:A},{key:"correct",label:"Correct",count:x},{key:"wrong",label:"Wrong",count:j}];return c.jsx("div",{className:"results-container",children:c.jsxs("div",{className:"results-inner",children:[c.jsxs("section",{className:"results-header",children:[c.jsx("h1",{className:"results-title",children:"Exam Results"}),c.jsxs("div",{className:`results-score ${O?"passed":"failed"}`,children:[U,"%"]}),c.jsx("div",{className:`results-status ${O?"passed":"failed"}`,children:O?" PASSED":" FAILED"}),c.jsxs("p",{className:"results-summary",children:["You answered ",x," out of ",A," questions correctly"]}),c.jsxs("div",{className:"results-actions",children:[F&&c.jsx("button",{onClick:()=>{le(null),R("history")},className:"results-action secondary",type:"button",children:" Back to History"}),c.jsx("button",{onClick:()=>{Xi(),le(null),R("landing")},className:"results-action primary",type:"button",children:"Back to Home"})]})]}),c.jsxs("section",{className:"results-breakdown",children:[c.jsxs("div",{className:"results-breakdown-header",children:[c.jsxs("div",{children:[c.jsx("h2",{className:"results-subtitle",children:"Detailed Breakdown"}),Qe!=="all"&&c.jsxs("p",{className:"results-filter-summary",children:["Showing ",q," of ",A," questions"]})]}),c.jsx("div",{className:"results-filter-group",children:Q.map(oe=>{const Ye=Qe===oe.key;return c.jsxs("button",{onClick:()=>ct(oe.key),className:`results-filter-button${Ye?" active":""}`,type:"button",children:[oe.label," (",oe.count,")"]},oe.key)})})]}),Ee.length===0?c.jsx("div",{className:"results-empty-state",children:"No questions match this filter. Try a different filter to continue reviewing your results."}):Ee.map((oe,Ye)=>{const Yo=Se[oe.id],si=Yo===oe.correct,us=W.findIndex(uo=>uo.id===oe.id)+1;return c.jsxs("article",{className:`results-card ${si?"correct":"wrong"}`,children:[c.jsxs("header",{className:"results-card-header",children:[c.jsx("div",{className:`results-card-status ${si?"correct":"wrong"}`,children:si?"":""}),c.jsxs("div",{className:"results-card-meta",children:[c.jsxs("span",{children:["Question ",us," of ",A]}),Qe!=="all"&&c.jsxs("span",{className:"results-card-filter-index",children:["Filter ",Ye+1," of ",q]})]})]}),c.jsx("div",{className:"results-card-domain",children:c.jsx("span",{className:"domain-badge",children:oe.domain})}),c.jsx("h3",{className:"results-card-title",children:oe.question}),c.jsx("div",{className:"results-card-options",children:oe.options.map((uo,po)=>{const Lo=Yo===po,Tt=oe.correct===po;return c.jsx("div",{className:`results-card-option${Tt?" correct":""}${Lo&&!Tt?" incorrect":""}`,children:c.jsxs("div",{className:"results-card-option-label",children:[Tt&&c.jsx("span",{className:"results-card-chip correct",children:" Correct"}),Lo&&!Tt&&c.jsx("span",{className:"results-card-chip incorrect",children:" Your Answer"}),c.jsx("span",{children:uo})]})},po)})}),c.jsxs("div",{className:"results-card-explanation primary",children:[c.jsx("div",{className:"results-card-explanation-title",children:" Explanation:"}),c.jsx("div",{children:oe.explanation})]}),!si&&Yo!==null&&oe.wrongExplanations&&oe.wrongExplanations[Yo]&&c.jsxs("div",{className:"results-card-explanation warning",children:[c.jsx("div",{className:"results-card-explanation-title",children:" Why your answer was wrong:"}),c.jsx("div",{children:oe.wrongExplanations[Yo]})]})]},oe.id)})]})]})})}return c.jsxs("div",{className:"app-layout",children:[nt&&Pe&&c.jsx("div",{role:"dialog","aria-modal":"true","aria-labelledby":"submit-warning-title",className:"modal-overlay",children:c.jsxs("div",{className:"modal-content",role:"document",children:[c.jsx("h3",{id:"submit-warning-title",className:"modal-title",children:"Review Flagged Questions?"}),c.jsxs("p",{className:"modal-text",children:["You have ",Pe.flagged," flagged question",Pe.flagged===1?"":"s","."]}),c.jsxs("p",{className:"modal-text",children:["You have answered ",Pe.answered," of ",Pe.total," questions so far."]}),Pe.unanswered>0&&c.jsxs("p",{className:"modal-text",children:["There ",Pe.unanswered===1?"is":"are"," also ",Pe.unanswered," unanswered question",Pe.unanswered===1?"":"s"," remaining."]}),c.jsx("p",{className:"modal-text muted",children:"You can jump back to the first flagged question to review or submit now and view the full breakdown in the results screen."}),c.jsxs("div",{className:"modal-actions",children:[c.jsx("button",{onClick:Zi,className:"modal-button secondary",type:"button",children:"Review Flagged Questions"}),c.jsx("button",{onClick:ni,className:"modal-button primary",type:"button",children:"Submit Anyway"})]})]})}),c.jsxs("div",{className:"exam-banner fixed-banner",children:[c.jsxs("div",{className:"exam-banner__row",style:{display:"flex",justifyContent:"space-between",alignItems:"center",flexWrap:"wrap",gap:12},children:[c.jsxs("div",{style:{display:"flex",alignItems:"center",gap:16},children:[c.jsx("div",{style:{fontWeight:700},children:D==="exam"?`Exam  ${W.length} questions`:D==="practice"?`Practice  ${W.length} questions`:"Practice"}),D==="exam"&&V!==null&&c.jsxs("div",{style:{fontWeight:600,fontSize:18,color:V<900?"#f87171":"#60a5fa",padding:"4px 12px",borderRadius:8,background:V<900?"rgba(248,113,113,0.1)":"rgba(96,165,250,0.1)"},children:[" ",Ji(V)]}),D==="practice"&&c.jsxs("div",{style:{fontWeight:600,fontSize:16,color:"#bfdbfe",padding:"4px 10px",borderRadius:8,background:"rgba(96,165,250,0.12)"},children:["Progress: ",Fi,"/",W.length," answered"]})]}),c.jsxs("div",{style:{display:"flex",gap:8,alignItems:"center",flexWrap:"wrap"},children:[D==="exam"&&c.jsxs("button",{className:"submit-btn",onClick:()=>ai(),style:{background:"linear-gradient(135deg, rgba(34,197,94,0.9), rgba(74,222,128,0.9))",fontWeight:600},children:["Submit Exam (",W.filter(x=>Se[x.id]!==null).length,"/",W.length,")"]}),D==="practice"&&c.jsxs("button",{className:"csv-btn",onClick:rt,style:{fontWeight:600},children:["Finish Practice (",Fi,"/",W.length,")"]}),D!=="exam"&&c.jsx("button",{className:"csv-btn",onClick:()=>{_i(),Z(typeof window<"u"?window.innerWidth>900:!0)},children:"Start 50-Question Exam"}),c.jsx("button",{className:"csv-btn",onClick:()=>{Xi(),R("landing"),Z(!1)},children:"Back to Home"}),!te&&c.jsx("button",{className:"csv-btn",onClick:()=>Z(x=>!x),children:M?"Hide Sidebar":"Show Sidebar"})]})]}),te&&_&&c.jsxs("div",{className:"mobile-domain-row",children:[c.jsxs("div",{className:"mobile-domain-row__top",children:[c.jsx("span",{className:"domain-badge mobile",children:_.domain}),c.jsxs("span",{className:"mobile-question-pill",children:["Question ",Ke+1," / ",W.length]})]}),_.subdomain&&c.jsx("span",{className:"mobile-domain-row__subdomain",children:_.subdomain})]})]}),c.jsx("div",{className:"sidebar-wrapper",children:M&&!te&&c.jsxs("aside",{className:"sidebar fixed-sidebar",children:[c.jsx("h3",{style:{marginTop:0},children:"Navigator"}),c.jsx("div",{className:"navigator-grid",children:W.map((x,A)=>c.jsx("button",{className:`nav-item ${Se[x.id]!==null?"answered":""} ${Fe[x.id]?"flagged":""} ${A===Ke?"current":""}`,onClick:()=>Ae(A),"aria-pressed":A===Ke,title:`Question ${A+1}`,children:A+1},x.id))}),c.jsx("div",{style:{marginTop:12},children:c.jsx("button",{className:"submit-all-btn",onClick:$i,children:"Export CSV"})})]})}),te&&D!=="landing"&&c.jsxs(c.Fragment,{children:[c.jsx("button",{className:"mobile-nav-fab",onClick:()=>L(!0),"aria-label":"Open navigator",children:""}),K&&c.jsx("div",{className:"mobile-navigator-sheet",children:c.jsxs("div",{className:"sheet-panel",children:[c.jsxs("div",{className:"sheet-header",children:[c.jsx("strong",{children:"Navigator"}),c.jsx("button",{className:"sheet-close",onClick:()=>L(!1),"aria-label":"Close",children:""})]}),c.jsx("div",{className:"nav-grid",children:W.map((x,A)=>c.jsx("button",{className:`nav-item ${A===Ke?"current":""} ${Se[x.id]!==null?"answered":""} ${Fe[x.id]?"flagged":""}`,onClick:()=>{Ae(A),L(!1)},children:A+1},x.id))}),c.jsx("div",{className:"sheet-footer",children:c.jsx("button",{onClick:()=>{$i(),L(!1)},children:"Export CSV"})})]})})]}),c.jsx("main",{className:"main-area",style:{marginLeft:!te&&M?"calc(var(--sidebar-width) + 32px)":0,marginTop:0},children:c.jsx("div",{style:{padding:20},children:_?c.jsxs("div",{children:[c.jsxs("div",{className:"question-domain-block",children:[c.jsx("span",{className:"domain-badge",children:_.domain}),_.subdomain&&c.jsx("div",{className:"subdomain-label",children:_.subdomain})]}),c.jsxs("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",marginBottom:24},children:[c.jsxs("div",{style:{fontSize:14,color:"#9ca3af"},children:["Question ",Ke+1," of ",W.length]}),c.jsxs("button",{onClick:_n,className:`flag-button ${Fe[_.id]?"flagged":""}`,children:[" ",Fe[_.id]?"Flagged":"Flag"]})]}),c.jsx("h2",{style:{marginTop:0,fontSize:22,lineHeight:1.5,color:"#f9fafb",fontWeight:500},children:_.question}),c.jsx("div",{style:{display:"grid",gap:8,marginTop:12},children:(()=>{const x=b[_.id];return(x?x.map(j=>_.options[j]):_.options).map((j,U)=>{const O=x?x[U]:U,q=Se[_.id]===O;return c.jsx("div",{className:`option ${q?"selected":""}`,onClick:()=>Kn(U),role:"button",tabIndex:0,onKeyDown:Q=>{(Q.key==="Enter"||Q.key===" ")&&Kn(U)},children:j},U)})})()}),D==="practice"&&Ne&&Ne.questionId===_.id&&c.jsxs("div",{style:{marginTop:20,padding:"16px 18px",borderRadius:10,border:`1px solid ${Ne.isCorrect?"rgba(52,211,153,0.45)":"rgba(248,113,113,0.45)"}`,background:Ne.isCorrect?"rgba(52,211,153,0.12)":"rgba(248,113,113,0.12)"},children:[c.jsx("div",{style:{fontWeight:700,color:Ne.isCorrect?"#34d399":"#f87171",marginBottom:8},children:Ne.isCorrect?" Correct!":" Incorrect"}),c.jsxs("div",{style:{color:"#e5e7eb",marginBottom:6},children:["Your answer: ",c.jsx("strong",{children:Ne.selectedOption!==null?_.options[Ne.selectedOption]:""})]}),c.jsxs("div",{style:{color:"#bfdbfe",marginBottom:10},children:["Correct answer: ",c.jsx("strong",{children:_.options[_.correct]})]}),c.jsx("div",{style:{color:"#e5e7eb",lineHeight:1.6,marginBottom:Ne.isCorrect?0:10},children:_.explanation}),!Ne.isCorrect&&Ne.selectedOption!==null&&_.wrongExplanations&&_.wrongExplanations[Ne.selectedOption]&&c.jsx("div",{style:{marginTop:10,padding:"12px 14px",borderRadius:8,border:"1px solid rgba(248,113,113,0.35)",background:"rgba(248,113,113,0.1)",color:"#fecaca"},children:_.wrongExplanations[Ne.selectedOption]})]}),c.jsxs("div",{style:{marginTop:18,display:"flex",gap:8},children:[c.jsx("button",{onClick:()=>Ae(x=>Math.max(0,x-1)),className:"nav-btn",children:"Previous"}),c.jsx("button",{onClick:()=>Ae(x=>Math.min(W.length-1,x+1)),className:"nav-btn",children:"Next"})]})]}):c.jsx("div",{children:"No questions available"})})})]})},um=document.getElementById("root"),dm=lm.createRoot(um);dm.render(c.jsx(cm,{}));
